{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization and Optimization of Neural Networks - Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Recall from the last lab that we had a training accuracy close to 90% and a test set accuracy close to 76%.\n",
    "\n",
    "As with our previous machine learning work, we should be asking a couple of questions:\n",
    "- Is there a high bias? yes/no\n",
    "- Is there a high variance? yes/no\n",
    "\n",
    "Also recall that \"high bias\" is a relative concept. Knowing we have 7 classes and the topics are related, we'll assume that a 90% accuracy is pretty good and the bias on the training set is low. (We've also discussed concepts like precision, recall as well as AUC and ROC curves.)   \n",
    "\n",
    "In this lab, we'll use the notion of training/validation/test set to get better insights of how we can mitigate our variance, and we'll look at a few regularization techniques. You'll start by repeating the process from the last section: importing the data and performing preprocessing including one-hot encoding. Then, just before you go on to train the model, we'll introduce how to include a validation set. You'll then define and compile the model as before. This time, when you are presented with the `history` dictionary of the model, you will have additional data entries for not only the train and test, but the train, test and validation  and then defigning, compiling and training the model. \n",
    "\n",
    "\n",
    "## Objectives\n",
    "\n",
    "You will be able to:\n",
    "\n",
    "* Construct and run a basic model in Keras\n",
    "* Construct a validation set and explain potential benefits\n",
    "* Apply L1 and L2 regularization\n",
    "* Aplly dropout regularization\n",
    "* Observe and comment on the effect of using more data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the libraries\n",
    "\n",
    "As usual, start by importing some of the packages and modules that you intend to use. The first thing we'll be doing is importing the data and taking a random sample, so that should clue you in to what tools to import. If you need more tools down the line, you can always import additional packages later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here; import some packages/modules you plan to use\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "\n",
    "As with the previous lab, the data is stored in a file **Bank_complaints.csv**. Load and preview the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>In XX/XX/XXXX I filled out the Fedlaon applica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>I am being contacted by a debt collector for p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>I cosigned XXXX student loans at SallieMae for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>Navient has sytematically and illegally failed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>My wife became eligible for XXXX Loan Forgiven...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Product                       Consumer complaint narrative\n",
       "0  Student loan  In XX/XX/XXXX I filled out the Fedlaon applica...\n",
       "1  Student loan  I am being contacted by a debt collector for p...\n",
       "2  Student loan  I cosigned XXXX student loans at SallieMae for...\n",
       "3  Student loan  Navient has sytematically and illegally failed...\n",
       "4  Student loan  My wife became eligible for XXXX Loan Forgiven..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Your code here; load and preview the dataset\n",
    "\n",
    "df = pd.read_csv('Bank_complaints.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Overview\n",
    "\n",
    "Before we begin to practice some of our new tools regarding regularization and optimization, let's practice munging some data as we did in the previous section with bank complaints. Recall some techniques:\n",
    "\n",
    "* Sampling in order to reduce training time (investigate model accuracy vs data size later on)\n",
    "* One-hot encoding our complaint text\n",
    "* Transforming our category labels\n",
    "* Train - test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Generate a Random Sample\n",
    "\n",
    "Since we have quite a bit of data and training networks takes a substantial amount of time and resources, we will downsample in order to test our initial pipeline. Going forward, these can be interesting areas of investigation: how does our models performance change as we increase (or decrease) the size of our dataset?  \n",
    "\n",
    "Generate the random sample using seed 123 for consistency of results. Make your new sample have 10,000 observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here\n",
    "sample = df.sample(n=10000, random_state=123)\n",
    "products = sample.Product\n",
    "complaints = sample['Consumer complaint narrative']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: One-hot Encoding of the Complaints\n",
    "\n",
    "As before, we need to do some preprocessing and data manipulationg before building the neural network. Last time, we guided you through the process, and now its time for you to practice that pipeline independently.  \n",
    "\n",
    "Only keep 2,000 most common words and use one-hot encoding to reformat the complaints into a matrix of vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 2000)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Your code here; use one-hot encoding to reformat the complaints into a matrix of vectors.\n",
    "#Only keep the 2000 most common words.\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "\n",
    "tokenizer.fit_on_texts(complaints)\n",
    "\n",
    "ohencoded = tokenizer.texts_to_matrix(complaints, mode='binary')\n",
    "np.shape(ohencoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Encoding the Products\n",
    "\n",
    "Similarly, now transform the descriptive product labels to integers labels. After transforming them to integer labels, retransform them into a matrix of binary flags, one for each of the various product labels.  \n",
    "  \n",
    "  (Note: this is similar to our previous work with dummy variables: each of the various product categories will be its own column, and each observation will be a row. Each of these observation rows will have a 1 in the column associated with it's label, and all other entries for the row will be zero.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here; transform the product labels to numerical values\n",
    "#Then transform these integer values into a matrix of binary flags\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils.np_utils import to_categorical\n",
    "le = LabelEncoder()\n",
    "le.fit(products)\n",
    "product_enc = le.transform(products)\n",
    "product_ohe = to_categorical(product_enc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test Split\n",
    "\n",
    "Now onto the ever familiar train-test split! Be sure to split both the complaint data (now transformed into word vectors) as well as their associated labels. Perform an appropriate train test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(ohencoded, product_ohe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the model using a validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Validation Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture we mentioned that in deep learning, we generally keep aside a validation set, which is used during hyperparameter tuning. Then when we have made the final model decision, the test set is used to define the final model perforance. \n",
    "\n",
    "In this example, let's take the first 1000 cases out of the training set to become the validation set. You should do this for both `train` and `label_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just run this block of code \n",
    "import random\n",
    "random.seed(123)\n",
    "val = X_train[:1000]\n",
    "train_final = X_train[1000:]\n",
    "label_val = y_train[:1000]\n",
    "label_train_final = y_train[1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rebuild a fully connected (Dense) layer network with relu activations in Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we used 2 hidden with 50 units in the first layer and 25 in the second, both with a `relu` activation function. Because we are dealing with a multiclass problem (classifying the complaints into 7 classes), we use a use a softmax classifyer in order to output 7 class probabilities per case.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here; build a neural network using Keras as described above.\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(50,activation='relu', input_shape=(2000,)))\n",
    "model.add(Dense(25, activation='relu'))\n",
    "model.add(Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the Model\n",
    "In the compiler, you'll be passing the optimizer, loss function, and metrics. Train the model for 120 epochs in mini-batches of 256 samples. This time, let's include the argument `validation_data` and assign it `(val, label_val)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here\n",
    "adam = Adam()\n",
    "model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Code Along\n",
    "\n",
    "The remaining portion of this lab will introduce you to code snippets for a myriad of different methods discussed in the lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "Ok, now for the resource intensive part: time to train our model! Note that this is where we also introduce the validation data to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "6500/6500 [==============================] - 0s 59us/step - loss: 1.7138 - acc: 0.3478 - val_loss: 1.3966 - val_acc: 0.5350\n",
      "Epoch 2/120\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 1.0810 - acc: 0.6792 - val_loss: 0.9664 - val_acc: 0.6790\n",
      "Epoch 3/120\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.7223 - acc: 0.7794 - val_loss: 0.8011 - val_acc: 0.7170\n",
      "Epoch 4/120\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.5459 - acc: 0.8405 - val_loss: 0.7412 - val_acc: 0.7320\n",
      "Epoch 5/120\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.4392 - acc: 0.8725 - val_loss: 0.6936 - val_acc: 0.7510\n",
      "Epoch 6/120\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.3578 - acc: 0.9040 - val_loss: 0.6879 - val_acc: 0.7470\n",
      "Epoch 7/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.2979 - acc: 0.9225 - val_loss: 0.6928 - val_acc: 0.7520\n",
      "Epoch 8/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.2490 - acc: 0.9395 - val_loss: 0.6909 - val_acc: 0.7530\n",
      "Epoch 9/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.2081 - acc: 0.9531 - val_loss: 0.7055 - val_acc: 0.7470\n",
      "Epoch 10/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.1751 - acc: 0.9657 - val_loss: 0.7366 - val_acc: 0.7420\n",
      "Epoch 11/120\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.1490 - acc: 0.9734 - val_loss: 0.7536 - val_acc: 0.7460\n",
      "Epoch 12/120\n",
      "6500/6500 [==============================] - 0s 29us/step - loss: 0.1258 - acc: 0.9802 - val_loss: 0.7683 - val_acc: 0.7490\n",
      "Epoch 13/120\n",
      "6500/6500 [==============================] - 0s 49us/step - loss: 0.1054 - acc: 0.9848 - val_loss: 0.8068 - val_acc: 0.7400\n",
      "Epoch 14/120\n",
      "6500/6500 [==============================] - 0s 46us/step - loss: 0.0894 - acc: 0.9885 - val_loss: 0.8381 - val_acc: 0.7310\n",
      "Epoch 15/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.0776 - acc: 0.9914 - val_loss: 0.8599 - val_acc: 0.7310\n",
      "Epoch 16/120\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.0664 - acc: 0.9945 - val_loss: 0.8974 - val_acc: 0.7350\n",
      "Epoch 17/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.0579 - acc: 0.9955 - val_loss: 0.9263 - val_acc: 0.7330\n",
      "Epoch 18/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.0510 - acc: 0.9966 - val_loss: 0.9530 - val_acc: 0.7290\n",
      "Epoch 19/120\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.0432 - acc: 0.9971 - val_loss: 0.9735 - val_acc: 0.7320\n",
      "Epoch 20/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.0373 - acc: 0.9978 - val_loss: 1.0063 - val_acc: 0.7340\n",
      "Epoch 21/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.0331 - acc: 0.9978 - val_loss: 1.0297 - val_acc: 0.7310\n",
      "Epoch 22/120\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.0295 - acc: 0.9982 - val_loss: 1.0671 - val_acc: 0.7330\n",
      "Epoch 23/120\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.0261 - acc: 0.9988 - val_loss: 1.0920 - val_acc: 0.7320\n",
      "Epoch 24/120\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.0225 - acc: 0.9992 - val_loss: 1.1064 - val_acc: 0.7360\n",
      "Epoch 25/120\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.0208 - acc: 0.9988 - val_loss: 1.1300 - val_acc: 0.7340\n",
      "Epoch 26/120\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.0191 - acc: 0.9988 - val_loss: 1.1552 - val_acc: 0.7300\n",
      "Epoch 27/120\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.0172 - acc: 0.9992 - val_loss: 1.1757 - val_acc: 0.7330\n",
      "Epoch 28/120\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.0159 - acc: 0.9991 - val_loss: 1.2002 - val_acc: 0.7330\n",
      "Epoch 29/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.0146 - acc: 0.9989 - val_loss: 1.2079 - val_acc: 0.7340\n",
      "Epoch 30/120\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.0131 - acc: 0.9992 - val_loss: 1.2359 - val_acc: 0.7300\n",
      "Epoch 31/120\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.0128 - acc: 0.9994 - val_loss: 1.2593 - val_acc: 0.7330\n",
      "Epoch 32/120\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.0122 - acc: 0.9989 - val_loss: 1.2814 - val_acc: 0.7250\n",
      "Epoch 33/120\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 0.0112 - acc: 0.9997 - val_loss: 1.2836 - val_acc: 0.7300\n",
      "Epoch 34/120\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 0.0106 - acc: 0.9995 - val_loss: 1.2988 - val_acc: 0.7340\n",
      "Epoch 35/120\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.0090 - acc: 0.9997 - val_loss: 1.3088 - val_acc: 0.7320\n",
      "Epoch 36/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.0086 - acc: 0.9995 - val_loss: 1.3227 - val_acc: 0.7270\n",
      "Epoch 37/120\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.0091 - acc: 0.9995 - val_loss: 1.3442 - val_acc: 0.7300\n",
      "Epoch 38/120\n",
      "6500/6500 [==============================] - 0s 49us/step - loss: 0.0073 - acc: 0.9997 - val_loss: 1.3576 - val_acc: 0.7260\n",
      "Epoch 39/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.0075 - acc: 0.9995 - val_loss: 1.3706 - val_acc: 0.7290\n",
      "Epoch 40/120\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.0060 - acc: 0.9997 - val_loss: 1.3859 - val_acc: 0.7250\n",
      "Epoch 41/120\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.0062 - acc: 0.9995 - val_loss: 1.3992 - val_acc: 0.7290\n",
      "Epoch 42/120\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 0.0068 - acc: 0.9997 - val_loss: 1.4125 - val_acc: 0.7300\n",
      "Epoch 43/120\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 0.0058 - acc: 0.9995 - val_loss: 1.4133 - val_acc: 0.7290\n",
      "Epoch 44/120\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.0056 - acc: 0.9997 - val_loss: 1.4367 - val_acc: 0.7300\n",
      "Epoch 45/120\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.0056 - acc: 0.9997 - val_loss: 1.4479 - val_acc: 0.7310\n",
      "Epoch 46/120\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.0051 - acc: 0.9995 - val_loss: 1.4530 - val_acc: 0.7300\n",
      "Epoch 47/120\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.0055 - acc: 0.9992 - val_loss: 1.4625 - val_acc: 0.7280\n",
      "Epoch 48/120\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.0043 - acc: 0.9997 - val_loss: 1.4736 - val_acc: 0.7280\n",
      "Epoch 49/120\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.0058 - acc: 0.9994 - val_loss: 1.4929 - val_acc: 0.7240\n",
      "Epoch 50/120\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.0041 - acc: 0.9995 - val_loss: 1.4913 - val_acc: 0.7300\n",
      "Epoch 51/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.0047 - acc: 0.9995 - val_loss: 1.5031 - val_acc: 0.7240\n",
      "Epoch 52/120\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.0040 - acc: 0.9995 - val_loss: 1.5126 - val_acc: 0.7250\n",
      "Epoch 53/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.0034 - acc: 0.9997 - val_loss: 1.5219 - val_acc: 0.7220\n",
      "Epoch 54/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.0038 - acc: 0.9997 - val_loss: 1.5295 - val_acc: 0.7290\n",
      "Epoch 55/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.0036 - acc: 0.9997 - val_loss: 1.5552 - val_acc: 0.7270\n",
      "Epoch 56/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.0042 - acc: 0.9997 - val_loss: 1.5649 - val_acc: 0.7240\n",
      "Epoch 57/120\n",
      "6500/6500 [==============================] - 0s 57us/step - loss: 0.0035 - acc: 0.9997 - val_loss: 1.5626 - val_acc: 0.7270\n",
      "Epoch 58/120\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 0.0040 - acc: 0.9995 - val_loss: 1.5659 - val_acc: 0.7250\n",
      "Epoch 59/120\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.0049 - acc: 0.9995 - val_loss: 1.5980 - val_acc: 0.7240\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.0035 - acc: 0.9995 - val_loss: 1.5993 - val_acc: 0.7260\n",
      "Epoch 61/120\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.0036 - acc: 0.9995 - val_loss: 1.6215 - val_acc: 0.7200\n",
      "Epoch 62/120\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.0032 - acc: 0.9995 - val_loss: 1.5975 - val_acc: 0.7240\n",
      "Epoch 63/120\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.0042 - acc: 0.9995 - val_loss: 1.6142 - val_acc: 0.7250\n",
      "Epoch 64/120\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.0037 - acc: 0.9997 - val_loss: 1.6445 - val_acc: 0.7230\n",
      "Epoch 65/120\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.0047 - acc: 0.9992 - val_loss: 1.6254 - val_acc: 0.7260\n",
      "Epoch 66/120\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.0057 - acc: 0.9995 - val_loss: 1.6192 - val_acc: 0.7290\n",
      "Epoch 67/120\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.0046 - acc: 0.9995 - val_loss: 1.6363 - val_acc: 0.7280\n",
      "Epoch 68/120\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.0043 - acc: 0.9995 - val_loss: 1.6348 - val_acc: 0.7290\n",
      "Epoch 69/120\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.0042 - acc: 0.9995 - val_loss: 1.6414 - val_acc: 0.7280\n",
      "Epoch 70/120\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.0041 - acc: 0.9997 - val_loss: 1.6492 - val_acc: 0.7280\n",
      "Epoch 71/120\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.0040 - acc: 0.9995 - val_loss: 1.6554 - val_acc: 0.7300\n",
      "Epoch 72/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.0040 - acc: 0.9997 - val_loss: 1.6603 - val_acc: 0.7280\n",
      "Epoch 73/120\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.0039 - acc: 0.9995 - val_loss: 1.6671 - val_acc: 0.7290\n",
      "Epoch 74/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.0039 - acc: 0.9997 - val_loss: 1.6731 - val_acc: 0.7270\n",
      "Epoch 75/120\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.0038 - acc: 0.9995 - val_loss: 1.6777 - val_acc: 0.7280\n",
      "Epoch 76/120\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 0.0038 - acc: 0.9995 - val_loss: 1.6873 - val_acc: 0.7260\n",
      "Epoch 77/120\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 0.0038 - acc: 0.9997 - val_loss: 1.6910 - val_acc: 0.7270\n",
      "Epoch 78/120\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.0037 - acc: 0.9995 - val_loss: 1.6970 - val_acc: 0.7300\n",
      "Epoch 79/120\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.0037 - acc: 0.9995 - val_loss: 1.7017 - val_acc: 0.7270\n",
      "Epoch 80/120\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 0.0037 - acc: 0.9995 - val_loss: 1.7057 - val_acc: 0.7290\n",
      "Epoch 81/120\n",
      "6500/6500 [==============================] - 0s 44us/step - loss: 0.0036 - acc: 0.9997 - val_loss: 1.7125 - val_acc: 0.7300\n",
      "Epoch 82/120\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.0036 - acc: 0.9997 - val_loss: 1.7180 - val_acc: 0.7290\n",
      "Epoch 83/120\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.0036 - acc: 0.9995 - val_loss: 1.7222 - val_acc: 0.7310\n",
      "Epoch 84/120\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 0.0035 - acc: 0.9995 - val_loss: 1.7283 - val_acc: 0.7270\n",
      "Epoch 85/120\n",
      "6500/6500 [==============================] - 0s 44us/step - loss: 0.0035 - acc: 0.9997 - val_loss: 1.7326 - val_acc: 0.7310\n",
      "Epoch 86/120\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.0035 - acc: 0.9997 - val_loss: 1.7378 - val_acc: 0.7310\n",
      "Epoch 87/120\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.0035 - acc: 0.9995 - val_loss: 1.7429 - val_acc: 0.7310\n",
      "Epoch 88/120\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.0034 - acc: 0.9995 - val_loss: 1.7485 - val_acc: 0.7300\n",
      "Epoch 89/120\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.0034 - acc: 0.9997 - val_loss: 1.7520 - val_acc: 0.7280\n",
      "Epoch 90/120\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.0034 - acc: 0.9997 - val_loss: 1.7578 - val_acc: 0.7300\n",
      "Epoch 91/120\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.0034 - acc: 0.9997 - val_loss: 1.7621 - val_acc: 0.7300\n",
      "Epoch 92/120\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.0034 - acc: 0.9995 - val_loss: 1.7669 - val_acc: 0.7280\n",
      "Epoch 93/120\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.0034 - acc: 0.9997 - val_loss: 1.7710 - val_acc: 0.7300\n",
      "Epoch 94/120\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.0033 - acc: 0.9997 - val_loss: 1.7738 - val_acc: 0.7300\n",
      "Epoch 95/120\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.0033 - acc: 0.9997 - val_loss: 1.7809 - val_acc: 0.7290\n",
      "Epoch 96/120\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.0033 - acc: 0.9997 - val_loss: 1.7848 - val_acc: 0.7290\n",
      "Epoch 97/120\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.0033 - acc: 0.9997 - val_loss: 1.7891 - val_acc: 0.7280\n",
      "Epoch 98/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.0033 - acc: 0.9995 - val_loss: 1.7924 - val_acc: 0.7280\n",
      "Epoch 99/120\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.0033 - acc: 0.9997 - val_loss: 1.7988 - val_acc: 0.7270\n",
      "Epoch 100/120\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 0.0032 - acc: 0.9995 - val_loss: 1.8014 - val_acc: 0.7300\n",
      "Epoch 101/120\n",
      "6500/6500 [==============================] - 0s 50us/step - loss: 0.0032 - acc: 0.9997 - val_loss: 1.8047 - val_acc: 0.7300\n",
      "Epoch 102/120\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.0032 - acc: 0.9997 - val_loss: 1.8091 - val_acc: 0.7300\n",
      "Epoch 103/120\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.0032 - acc: 0.9995 - val_loss: 1.8131 - val_acc: 0.7300\n",
      "Epoch 104/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.0032 - acc: 0.9997 - val_loss: 1.8171 - val_acc: 0.7310\n",
      "Epoch 105/120\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.0032 - acc: 0.9997 - val_loss: 1.8205 - val_acc: 0.7300\n",
      "Epoch 106/120\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.0032 - acc: 0.9997 - val_loss: 1.8262 - val_acc: 0.7280\n",
      "Epoch 107/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.0031 - acc: 0.9997 - val_loss: 1.8294 - val_acc: 0.7310\n",
      "Epoch 108/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.0032 - acc: 0.9997 - val_loss: 1.8325 - val_acc: 0.7290\n",
      "Epoch 109/120\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.0031 - acc: 0.9997 - val_loss: 1.8379 - val_acc: 0.7290\n",
      "Epoch 110/120\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.0031 - acc: 0.9997 - val_loss: 1.8402 - val_acc: 0.7310\n",
      "Epoch 111/120\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.0031 - acc: 0.9997 - val_loss: 1.8451 - val_acc: 0.7280\n",
      "Epoch 112/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.0031 - acc: 0.9995 - val_loss: 1.8477 - val_acc: 0.7310\n",
      "Epoch 113/120\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.0031 - acc: 0.9997 - val_loss: 1.8502 - val_acc: 0.7320\n",
      "Epoch 114/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.0031 - acc: 0.9997 - val_loss: 1.8548 - val_acc: 0.7310\n",
      "Epoch 115/120\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.0031 - acc: 0.9997 - val_loss: 1.8581 - val_acc: 0.7300\n",
      "Epoch 116/120\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.0031 - acc: 0.9995 - val_loss: 1.8622 - val_acc: 0.7340\n",
      "Epoch 117/120\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.0031 - acc: 0.9997 - val_loss: 1.8665 - val_acc: 0.7300\n",
      "Epoch 118/120\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.0030 - acc: 0.9997 - val_loss: 1.8700 - val_acc: 0.7310\n",
      "Epoch 119/120\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.0030 - acc: 0.9995 - val_loss: 1.8721 - val_acc: 0.7310\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120\n",
      "6500/6500 [==============================] - 0s 49us/step - loss: 0.0030 - acc: 0.9997 - val_loss: 1.8769 - val_acc: 0.7310\n"
     ]
    }
   ],
   "source": [
    "#Code provided; note the extra validation parameter passed.\n",
    "model_val = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving Performance Results: the `history` dictionary\n",
    "\n",
    "The dictionary `history` contains four entries this time: one per metric that was being monitored during training and during validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_val_dict = model_val.history\n",
    "model_val_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6500/6500 [==============================] - 0s 42us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500/2500 [==============================] - 0s 58us/step\n"
     ]
    }
   ],
   "source": [
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0030033339782155684, 0.9996923076923077]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.8392724798202515, 0.7492]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the result isn't exactly the same as before. Note that this because the training set is slightly different! We remove 1000 instances for validation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the result similarly to what we have done in the previous lab. This time though, let's include the training and the validation loss in the same plot. We'll do the same thing for the training and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.clf()\n",
    "\n",
    "\n",
    "loss_values = model_val_dict['loss']\n",
    "val_loss_values = model_val_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "plt.plot(epochs, loss_values, 'g', label='Training loss')\n",
    "plt.plot(epochs, val_loss_values, 'blue', label='Validation loss')\n",
    "\n",
    "plt.title('Training & validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "acc_values = model_val_dict['acc'] \n",
    "val_acc_values = model_val_dict['val_acc']\n",
    "\n",
    "plt.plot(epochs, acc_values, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc_values, 'blue', label='Validation acc')\n",
    "plt.title('Training & validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe an interesting pattern here: although the training accuracy keeps increasing when going through more epochs, and the training loss keeps decreasing, the validation accuracy and loss seem to be reaching a status quo around the 60th epoch. This means that we're actually **overfitting** to the train data when we do as many epochs as we were doing. Luckily, you learned how to tackle overfitting in the previous lecture! For starters, it does seem clear that we are training too long. So let's stop training at the 60th epoch first (so-called \"early stopping\") before we move to more advanced regularization techniques!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6500 samples, validate on 1000 samples\n",
      "Epoch 1/60\n",
      "6500/6500 [==============================] - 0s 50us/step - loss: 1.3273 - acc: 0.5237 - val_loss: 0.7764 - val_acc: 0.7200\n",
      "Epoch 2/60\n",
      "6500/6500 [==============================] - 0s 28us/step - loss: 0.5184 - acc: 0.8277 - val_loss: 0.6733 - val_acc: 0.7500\n",
      "Epoch 3/60\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.3137 - acc: 0.8994 - val_loss: 0.6649 - val_acc: 0.7540\n",
      "Epoch 4/60\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.2036 - acc: 0.9471 - val_loss: 0.7118 - val_acc: 0.7470\n",
      "Epoch 5/60\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.1386 - acc: 0.9695 - val_loss: 0.7611 - val_acc: 0.7510\n",
      "Epoch 6/60\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.0960 - acc: 0.9814 - val_loss: 0.8294 - val_acc: 0.7570\n",
      "Epoch 7/60\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.0682 - acc: 0.9895 - val_loss: 0.8777 - val_acc: 0.7450\n",
      "Epoch 8/60\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.0502 - acc: 0.9937 - val_loss: 0.9365 - val_acc: 0.7410\n",
      "Epoch 9/60\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 0.0359 - acc: 0.9963 - val_loss: 0.9856 - val_acc: 0.7380\n",
      "Epoch 10/60\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.0274 - acc: 0.9980 - val_loss: 1.0199 - val_acc: 0.7470\n",
      "Epoch 11/60\n",
      "6500/6500 [==============================] - 0s 29us/step - loss: 0.0210 - acc: 0.9985 - val_loss: 1.0610 - val_acc: 0.7400\n",
      "Epoch 12/60\n",
      "6500/6500 [==============================] - 0s 28us/step - loss: 0.0179 - acc: 0.9988 - val_loss: 1.0978 - val_acc: 0.7390\n",
      "Epoch 13/60\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.0138 - acc: 0.9994 - val_loss: 1.1188 - val_acc: 0.7440\n",
      "Epoch 14/60\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.0113 - acc: 0.9994 - val_loss: 1.1408 - val_acc: 0.7440\n",
      "Epoch 15/60\n",
      "6500/6500 [==============================] - 0s 44us/step - loss: 0.0106 - acc: 0.9992 - val_loss: 1.1690 - val_acc: 0.7410\n",
      "Epoch 16/60\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.0088 - acc: 0.9997 - val_loss: 1.2056 - val_acc: 0.7450\n",
      "Epoch 17/60\n",
      "6500/6500 [==============================] - 0s 28us/step - loss: 0.0090 - acc: 0.9992 - val_loss: 1.2146 - val_acc: 0.7430\n",
      "Epoch 18/60\n",
      "6500/6500 [==============================] - 0s 27us/step - loss: 0.0072 - acc: 0.9997 - val_loss: 1.2363 - val_acc: 0.7410\n",
      "Epoch 19/60\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.0069 - acc: 0.9994 - val_loss: 1.2416 - val_acc: 0.7390\n",
      "Epoch 20/60\n",
      "6500/6500 [==============================] - 0s 45us/step - loss: 0.0067 - acc: 0.9995 - val_loss: 1.2611 - val_acc: 0.7400\n",
      "Epoch 21/60\n",
      "6500/6500 [==============================] - 0s 28us/step - loss: 0.0055 - acc: 0.9994 - val_loss: 1.2677 - val_acc: 0.7370\n",
      "Epoch 22/60\n",
      "6500/6500 [==============================] - 0s 27us/step - loss: 0.0051 - acc: 0.9995 - val_loss: 1.2958 - val_acc: 0.7360\n",
      "Epoch 23/60\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.0063 - acc: 0.9994 - val_loss: 1.2975 - val_acc: 0.7350\n",
      "Epoch 24/60\n",
      "6500/6500 [==============================] - 0s 28us/step - loss: 0.0043 - acc: 0.9997 - val_loss: 1.3089 - val_acc: 0.7370\n",
      "Epoch 25/60\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.0053 - acc: 0.9994 - val_loss: 1.3363 - val_acc: 0.7350\n",
      "Epoch 26/60\n",
      "6500/6500 [==============================] - 0s 29us/step - loss: 0.0042 - acc: 0.9995 - val_loss: 1.3300 - val_acc: 0.7390\n",
      "Epoch 27/60\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.0044 - acc: 0.9992 - val_loss: 1.3419 - val_acc: 0.7360\n",
      "Epoch 28/60\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.0042 - acc: 0.9994 - val_loss: 1.3586 - val_acc: 0.7340\n",
      "Epoch 29/60\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.0039 - acc: 0.9997 - val_loss: 1.3773 - val_acc: 0.7380\n",
      "Epoch 30/60\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.0038 - acc: 0.9994 - val_loss: 1.3730 - val_acc: 0.7330\n",
      "Epoch 31/60\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.0042 - acc: 0.9995 - val_loss: 1.3853 - val_acc: 0.7310\n",
      "Epoch 32/60\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.0034 - acc: 0.9994 - val_loss: 1.3953 - val_acc: 0.7340\n",
      "Epoch 33/60\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.0030 - acc: 0.9995 - val_loss: 1.4040 - val_acc: 0.7320\n",
      "Epoch 34/60\n",
      "6500/6500 [==============================] - 0s 28us/step - loss: 0.0032 - acc: 0.9997 - val_loss: 1.4155 - val_acc: 0.7370\n",
      "Epoch 35/60\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.0029 - acc: 0.9995 - val_loss: 1.4347 - val_acc: 0.7340\n",
      "Epoch 36/60\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.0030 - acc: 0.9997 - val_loss: 1.4363 - val_acc: 0.7370\n",
      "Epoch 37/60\n",
      "6500/6500 [==============================] - 0s 50us/step - loss: 0.0039 - acc: 0.9995 - val_loss: 1.4277 - val_acc: 0.7320\n",
      "Epoch 38/60\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 0.0017 - acc: 0.9998 - val_loss: 1.4424 - val_acc: 0.7340\n",
      "Epoch 39/60\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.0026 - acc: 0.9997 - val_loss: 1.4613 - val_acc: 0.7370\n",
      "Epoch 40/60\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.0026 - acc: 0.9995 - val_loss: 1.4707 - val_acc: 0.7370\n",
      "Epoch 41/60\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.0027 - acc: 0.9997 - val_loss: 1.4704 - val_acc: 0.7320\n",
      "Epoch 42/60\n",
      "6500/6500 [==============================] - 0s 29us/step - loss: 0.0025 - acc: 0.9997 - val_loss: 1.4571 - val_acc: 0.7290\n",
      "Epoch 43/60\n",
      "6500/6500 [==============================] - 0s 29us/step - loss: 0.0027 - acc: 0.9997 - val_loss: 1.4881 - val_acc: 0.7340\n",
      "Epoch 44/60\n",
      "6500/6500 [==============================] - 0s 26us/step - loss: 0.0026 - acc: 0.9997 - val_loss: 1.4917 - val_acc: 0.7350\n",
      "Epoch 45/60\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.0034 - acc: 0.9995 - val_loss: 1.4806 - val_acc: 0.7300\n",
      "Epoch 46/60\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.0023 - acc: 0.9997 - val_loss: 1.5090 - val_acc: 0.7320\n",
      "Epoch 47/60\n",
      "6500/6500 [==============================] - 0s 29us/step - loss: 0.0023 - acc: 0.9997 - val_loss: 1.5098 - val_acc: 0.7350\n",
      "Epoch 48/60\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.0023 - acc: 0.9997 - val_loss: 1.5167 - val_acc: 0.7310\n",
      "Epoch 49/60\n",
      "6500/6500 [==============================] - 0s 27us/step - loss: 0.0022 - acc: 0.9994 - val_loss: 1.5176 - val_acc: 0.7340\n",
      "Epoch 50/60\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.0023 - acc: 0.9997 - val_loss: 1.5153 - val_acc: 0.7380\n",
      "Epoch 51/60\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.0022 - acc: 0.9997 - val_loss: 1.5348 - val_acc: 0.7280\n",
      "Epoch 52/60\n",
      "6500/6500 [==============================] - 0s 29us/step - loss: 0.0023 - acc: 0.9997 - val_loss: 1.5152 - val_acc: 0.7310\n",
      "Epoch 53/60\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.0039 - acc: 0.9995 - val_loss: 1.5508 - val_acc: 0.7310\n",
      "Epoch 54/60\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.0036 - acc: 0.9995 - val_loss: 1.5446 - val_acc: 0.7350\n",
      "Epoch 55/60\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.0017 - acc: 0.9997 - val_loss: 1.5576 - val_acc: 0.7370\n",
      "Epoch 56/60\n",
      "6500/6500 [==============================] - 0s 23us/step - loss: 0.0024 - acc: 0.9995 - val_loss: 1.5399 - val_acc: 0.7350\n",
      "Epoch 57/60\n",
      "6500/6500 [==============================] - 0s 25us/step - loss: 0.0016 - acc: 0.9997 - val_loss: 1.5540 - val_acc: 0.7300\n",
      "Epoch 58/60\n",
      "6500/6500 [==============================] - 0s 29us/step - loss: 0.0022 - acc: 0.9997 - val_loss: 1.5572 - val_acc: 0.7360\n",
      "Epoch 59/60\n",
      "6500/6500 [==============================] - 0s 28us/step - loss: 0.0016 - acc: 0.9995 - val_loss: 1.5750 - val_acc: 0.7340\n",
      "Epoch 60/60\n",
      "6500/6500 [==============================] - 0s 28us/step - loss: 0.0022 - acc: 0.9997 - val_loss: 1.5674 - val_acc: 0.7350\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = Sequential()\n",
    "model.add(Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(Dense(25, activation='relu'))\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer=adam,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "final_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=60,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can use the test set to make label predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6500/6500 [==============================] - 0s 53us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500/2500 [==============================] - 0s 75us/step\n"
     ]
    }
   ],
   "source": [
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0009191768320922095, 0.9998461538461538]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.559409718132019, 0.7604]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've significantly reduced the variance, so this is already pretty good! Our test set accuracy is slightly worse, but this model will definitely be more robust than the 120 epochs one we fitted before.\n",
    "\n",
    "Now, let's see what else we can do to improve the result!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's include L2 regularization. You can easily do this in keras adding the argument kernel_regulizers.l2 and adding a value for the regularization parameter lambda between parentheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "6500/6500 [==============================] - 0s 60us/step - loss: 1.6915 - acc: 0.5489 - val_loss: 1.2287 - val_acc: 0.7120\n",
      "Epoch 2/120\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.9663 - acc: 0.8163 - val_loss: 1.0616 - val_acc: 0.7360\n",
      "Epoch 3/120\n",
      "6500/6500 [==============================] - 0s 49us/step - loss: 0.8045 - acc: 0.8572 - val_loss: 0.9863 - val_acc: 0.7530\n",
      "Epoch 4/120\n",
      "6500/6500 [==============================] - 0s 44us/step - loss: 0.7314 - acc: 0.8769 - val_loss: 0.9581 - val_acc: 0.7470\n",
      "Epoch 5/120\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6956 - acc: 0.8837 - val_loss: 0.9459 - val_acc: 0.7510\n",
      "Epoch 6/120\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.6591 - acc: 0.8952 - val_loss: 0.9647 - val_acc: 0.7520\n",
      "Epoch 7/120\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 0.6331 - acc: 0.9100 - val_loss: 0.9458 - val_acc: 0.7580\n",
      "Epoch 8/120\n",
      "6500/6500 [==============================] - 0s 46us/step - loss: 0.6198 - acc: 0.9145 - val_loss: 0.9554 - val_acc: 0.7490\n",
      "Epoch 9/120\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.5990 - acc: 0.9203 - val_loss: 0.9499 - val_acc: 0.7450\n",
      "Epoch 10/120\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.5833 - acc: 0.9237 - val_loss: 0.9558 - val_acc: 0.7520\n",
      "Epoch 11/120\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.5714 - acc: 0.9252 - val_loss: 0.9697 - val_acc: 0.7330\n",
      "Epoch 12/120\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.5603 - acc: 0.9314 - val_loss: 0.9746 - val_acc: 0.7430\n",
      "Epoch 13/120\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.5502 - acc: 0.9374 - val_loss: 0.9542 - val_acc: 0.7440\n",
      "Epoch 14/120\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.5451 - acc: 0.9369 - val_loss: 0.9747 - val_acc: 0.7390\n",
      "Epoch 15/120\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.5300 - acc: 0.9403 - val_loss: 0.9684 - val_acc: 0.7510\n",
      "Epoch 16/120\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.5218 - acc: 0.9477 - val_loss: 0.9689 - val_acc: 0.7500\n",
      "Epoch 17/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.5103 - acc: 0.9455 - val_loss: 0.9538 - val_acc: 0.7480\n",
      "Epoch 18/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.5021 - acc: 0.9514 - val_loss: 0.9760 - val_acc: 0.7360\n",
      "Epoch 19/120\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.4956 - acc: 0.9529 - val_loss: 0.9664 - val_acc: 0.7460\n",
      "Epoch 20/120\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.4880 - acc: 0.9531 - val_loss: 0.9692 - val_acc: 0.7370\n",
      "Epoch 21/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.4829 - acc: 0.9554 - val_loss: 0.9611 - val_acc: 0.7450\n",
      "Epoch 22/120\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.4824 - acc: 0.9549 - val_loss: 0.9656 - val_acc: 0.7450\n",
      "Epoch 23/120\n",
      "6500/6500 [==============================] - 0s 46us/step - loss: 0.4714 - acc: 0.9598 - val_loss: 0.9555 - val_acc: 0.7480\n",
      "Epoch 24/120\n",
      "6500/6500 [==============================] - 0s 54us/step - loss: 0.4672 - acc: 0.9600 - val_loss: 0.9844 - val_acc: 0.7430\n",
      "Epoch 25/120\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.4636 - acc: 0.9611 - val_loss: 0.9644 - val_acc: 0.7460\n",
      "Epoch 26/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.4607 - acc: 0.9580 - val_loss: 0.9720 - val_acc: 0.7490\n",
      "Epoch 27/120\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.4755 - acc: 0.9518 - val_loss: 0.9887 - val_acc: 0.7360\n",
      "Epoch 28/120\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.4576 - acc: 0.9635 - val_loss: 0.9681 - val_acc: 0.7460\n",
      "Epoch 29/120\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.4471 - acc: 0.9658 - val_loss: 0.9608 - val_acc: 0.7520\n",
      "Epoch 30/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.4414 - acc: 0.9686 - val_loss: 0.9659 - val_acc: 0.7450\n",
      "Epoch 31/120\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.4394 - acc: 0.9688 - val_loss: 0.9814 - val_acc: 0.7410\n",
      "Epoch 32/120\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.4436 - acc: 0.9637 - val_loss: 0.9810 - val_acc: 0.7400\n",
      "Epoch 33/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.4339 - acc: 0.9671 - val_loss: 0.9720 - val_acc: 0.7430\n",
      "Epoch 34/120\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.4269 - acc: 0.9703 - val_loss: 0.9756 - val_acc: 0.7430\n",
      "Epoch 35/120\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.4243 - acc: 0.9720 - val_loss: 0.9700 - val_acc: 0.7450\n",
      "Epoch 36/120\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.4228 - acc: 0.9697 - val_loss: 0.9875 - val_acc: 0.7390\n",
      "Epoch 37/120\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.4234 - acc: 0.9703 - val_loss: 0.9820 - val_acc: 0.7410\n",
      "Epoch 38/120\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.4179 - acc: 0.9718 - val_loss: 0.9804 - val_acc: 0.7450\n",
      "Epoch 39/120\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.4097 - acc: 0.9743 - val_loss: 0.9961 - val_acc: 0.7330\n",
      "Epoch 40/120\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.4082 - acc: 0.9718 - val_loss: 0.9751 - val_acc: 0.7430\n",
      "Epoch 41/120\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.4062 - acc: 0.9757 - val_loss: 0.9768 - val_acc: 0.7490\n",
      "Epoch 42/120\n",
      "6500/6500 [==============================] - 0s 49us/step - loss: 0.4034 - acc: 0.9763 - val_loss: 0.9904 - val_acc: 0.7450\n",
      "Epoch 43/120\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 0.3995 - acc: 0.9742 - val_loss: 0.9879 - val_acc: 0.7420\n",
      "Epoch 44/120\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.3964 - acc: 0.9755 - val_loss: 0.9801 - val_acc: 0.7420\n",
      "Epoch 45/120\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.3908 - acc: 0.9775 - val_loss: 0.9778 - val_acc: 0.7380\n",
      "Epoch 46/120\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 0.3878 - acc: 0.9817 - val_loss: 0.9782 - val_acc: 0.7400\n",
      "Epoch 47/120\n",
      "6500/6500 [==============================] - 0s 48us/step - loss: 0.3895 - acc: 0.9771 - val_loss: 0.9868 - val_acc: 0.7390\n",
      "Epoch 48/120\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.3911 - acc: 0.9752 - val_loss: 0.9755 - val_acc: 0.7370\n",
      "Epoch 49/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.4049 - acc: 0.9655 - val_loss: 1.0014 - val_acc: 0.7480\n",
      "Epoch 50/120\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.3897 - acc: 0.9777 - val_loss: 0.9924 - val_acc: 0.7380\n",
      "Epoch 51/120\n",
      "6500/6500 [==============================] - 0s 49us/step - loss: 0.3852 - acc: 0.9786 - val_loss: 1.0099 - val_acc: 0.7370\n",
      "Epoch 52/120\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 0.3780 - acc: 0.9797 - val_loss: 0.9837 - val_acc: 0.7480\n",
      "Epoch 53/120\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.3793 - acc: 0.9778 - val_loss: 0.9987 - val_acc: 0.7400\n",
      "Epoch 54/120\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.3850 - acc: 0.9762 - val_loss: 0.9977 - val_acc: 0.7350\n",
      "Epoch 55/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.3738 - acc: 0.9792 - val_loss: 1.0098 - val_acc: 0.7340\n",
      "Epoch 56/120\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.3722 - acc: 0.9815 - val_loss: 0.9899 - val_acc: 0.7370\n",
      "Epoch 57/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.3645 - acc: 0.9817 - val_loss: 0.9946 - val_acc: 0.7450\n",
      "Epoch 58/120\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.3620 - acc: 0.9812 - val_loss: 0.9851 - val_acc: 0.7350\n",
      "Epoch 59/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.3649 - acc: 0.9808 - val_loss: 1.0072 - val_acc: 0.7400\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.3643 - acc: 0.9806 - val_loss: 0.9938 - val_acc: 0.7340\n",
      "Epoch 61/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.3668 - acc: 0.9803 - val_loss: 1.0005 - val_acc: 0.7370\n",
      "Epoch 62/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.3667 - acc: 0.9805 - val_loss: 1.0208 - val_acc: 0.7400\n",
      "Epoch 63/120\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.3652 - acc: 0.9795 - val_loss: 1.0151 - val_acc: 0.7430\n",
      "Epoch 64/120\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.3542 - acc: 0.9832 - val_loss: 0.9920 - val_acc: 0.7360\n",
      "Epoch 65/120\n",
      "6500/6500 [==============================] - 0s 48us/step - loss: 0.3532 - acc: 0.9843 - val_loss: 0.9978 - val_acc: 0.7390\n",
      "Epoch 66/120\n",
      "6500/6500 [==============================] - 0s 51us/step - loss: 0.3491 - acc: 0.9846 - val_loss: 1.0039 - val_acc: 0.7420\n",
      "Epoch 67/120\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.3500 - acc: 0.9857 - val_loss: 0.9971 - val_acc: 0.7400\n",
      "Epoch 68/120\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.3483 - acc: 0.9835 - val_loss: 1.0023 - val_acc: 0.7390\n",
      "Epoch 69/120\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.3440 - acc: 0.9852 - val_loss: 0.9936 - val_acc: 0.7430\n",
      "Epoch 70/120\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.3447 - acc: 0.9842 - val_loss: 1.0084 - val_acc: 0.7460\n",
      "Epoch 71/120\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.3434 - acc: 0.9863 - val_loss: 0.9981 - val_acc: 0.7420\n",
      "Epoch 72/120\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.3440 - acc: 0.9863 - val_loss: 1.0061 - val_acc: 0.7410\n",
      "Epoch 73/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.3535 - acc: 0.9792 - val_loss: 1.0446 - val_acc: 0.7350\n",
      "Epoch 74/120\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.3506 - acc: 0.9825 - val_loss: 1.0202 - val_acc: 0.7410\n",
      "Epoch 75/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.3400 - acc: 0.9863 - val_loss: 1.0298 - val_acc: 0.7300\n",
      "Epoch 76/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.3432 - acc: 0.9848 - val_loss: 1.0202 - val_acc: 0.7400\n",
      "Epoch 77/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.3368 - acc: 0.9871 - val_loss: 0.9947 - val_acc: 0.7390\n",
      "Epoch 78/120\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.3329 - acc: 0.9878 - val_loss: 1.0191 - val_acc: 0.7440\n",
      "Epoch 79/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.3294 - acc: 0.9872 - val_loss: 1.0039 - val_acc: 0.7450\n",
      "Epoch 80/120\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.3264 - acc: 0.9905 - val_loss: 1.0259 - val_acc: 0.7320\n",
      "Epoch 81/120\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.3312 - acc: 0.9852 - val_loss: 0.9995 - val_acc: 0.7410\n",
      "Epoch 82/120\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.3257 - acc: 0.9882 - val_loss: 1.0179 - val_acc: 0.7370\n",
      "Epoch 83/120\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 0.3249 - acc: 0.9883 - val_loss: 1.0302 - val_acc: 0.7300\n",
      "Epoch 84/120\n",
      "6500/6500 [==============================] - 0s 49us/step - loss: 0.3276 - acc: 0.9875 - val_loss: 1.0073 - val_acc: 0.7390\n",
      "Epoch 85/120\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.3263 - acc: 0.9854 - val_loss: 1.0121 - val_acc: 0.7420\n",
      "Epoch 86/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.3258 - acc: 0.9871 - val_loss: 1.0135 - val_acc: 0.7380\n",
      "Epoch 87/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.3208 - acc: 0.9894 - val_loss: 1.0240 - val_acc: 0.7360\n",
      "Epoch 88/120\n",
      "6500/6500 [==============================] - 0s 51us/step - loss: 0.3168 - acc: 0.9886 - val_loss: 1.0152 - val_acc: 0.7430\n",
      "Epoch 89/120\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 0.3148 - acc: 0.9892 - val_loss: 1.0097 - val_acc: 0.7330\n",
      "Epoch 90/120\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.3196 - acc: 0.9871 - val_loss: 1.0173 - val_acc: 0.7390\n",
      "Epoch 91/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.3181 - acc: 0.9886 - val_loss: 1.0236 - val_acc: 0.7360\n",
      "Epoch 92/120\n",
      "6500/6500 [==============================] - 0s 51us/step - loss: 0.3139 - acc: 0.9902 - val_loss: 1.0108 - val_acc: 0.7330\n",
      "Epoch 93/120\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.3101 - acc: 0.9912 - val_loss: 1.0074 - val_acc: 0.7380\n",
      "Epoch 94/120\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.3093 - acc: 0.9911 - val_loss: 1.0323 - val_acc: 0.7320\n",
      "Epoch 95/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.3109 - acc: 0.9897 - val_loss: 1.0066 - val_acc: 0.7370\n",
      "Epoch 96/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.3114 - acc: 0.9885 - val_loss: 1.0164 - val_acc: 0.7360\n",
      "Epoch 97/120\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.3130 - acc: 0.9895 - val_loss: 1.0183 - val_acc: 0.7400\n",
      "Epoch 98/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.3068 - acc: 0.9906 - val_loss: 1.0273 - val_acc: 0.7390\n",
      "Epoch 99/120\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.3020 - acc: 0.9928 - val_loss: 1.0056 - val_acc: 0.7400\n",
      "Epoch 100/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.2979 - acc: 0.9923 - val_loss: 1.0104 - val_acc: 0.7420\n",
      "Epoch 101/120\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.2997 - acc: 0.9914 - val_loss: 1.0030 - val_acc: 0.7450\n",
      "Epoch 102/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.3046 - acc: 0.9891 - val_loss: 1.0220 - val_acc: 0.7460\n",
      "Epoch 103/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.3123 - acc: 0.9869 - val_loss: 1.0252 - val_acc: 0.7330\n",
      "Epoch 104/120\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.3065 - acc: 0.9892 - val_loss: 1.0226 - val_acc: 0.7340\n",
      "Epoch 105/120\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.3073 - acc: 0.9878 - val_loss: 1.0253 - val_acc: 0.7420\n",
      "Epoch 106/120\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.3035 - acc: 0.9874 - val_loss: 1.0148 - val_acc: 0.7370\n",
      "Epoch 107/120\n",
      "6500/6500 [==============================] - 0s 57us/step - loss: 0.2993 - acc: 0.9909 - val_loss: 1.0238 - val_acc: 0.7440\n",
      "Epoch 108/120\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 0.2955 - acc: 0.9898 - val_loss: 1.0221 - val_acc: 0.7400\n",
      "Epoch 109/120\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.2970 - acc: 0.9928 - val_loss: 1.0136 - val_acc: 0.7370\n",
      "Epoch 110/120\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.2939 - acc: 0.9922 - val_loss: 1.0442 - val_acc: 0.7250\n",
      "Epoch 111/120\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.3097 - acc: 0.9886 - val_loss: 1.0439 - val_acc: 0.7390\n",
      "Epoch 112/120\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.3028 - acc: 0.9894 - val_loss: 1.0171 - val_acc: 0.7430\n",
      "Epoch 113/120\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.2905 - acc: 0.9925 - val_loss: 1.0202 - val_acc: 0.7440\n",
      "Epoch 114/120\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.2852 - acc: 0.9931 - val_loss: 1.0292 - val_acc: 0.7430\n",
      "Epoch 115/120\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.2857 - acc: 0.9935 - val_loss: 1.0132 - val_acc: 0.7440\n",
      "Epoch 116/120\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.2863 - acc: 0.9935 - val_loss: 1.0257 - val_acc: 0.7360\n",
      "Epoch 117/120\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.2845 - acc: 0.9932 - val_loss: 1.0196 - val_acc: 0.7450\n",
      "Epoch 118/120\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.2826 - acc: 0.9937 - val_loss: 1.0112 - val_acc: 0.7400\n",
      "Epoch 119/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.2786 - acc: 0.9948 - val_loss: 1.0140 - val_acc: 0.7360\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.2818 - acc: 0.9948 - val_loss: 1.0173 - val_acc: 0.7440\n"
     ]
    }
   ],
   "source": [
    "from keras import regularizers\n",
    "random.seed(123)\n",
    "model = Sequential()\n",
    "model.add(Dense(50, activation='relu',kernel_regularizer=regularizers.l2(0.005),\n",
    "                input_shape=(2000,))) #2 hidden layers\n",
    "model.add(Dense(25, kernel_regularizer=regularizers.l2(0.005), activation='relu'))\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer=adam,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L2_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L2_model_dict = L2_model.history\n",
    "L2_model_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the training accuracy as well as the validation accuracy for both the L2 and the model without regularization (for 120 epochs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "acc_values = L2_model_dict['acc'] \n",
    "val_acc_values = L2_model_dict['val_acc']\n",
    "model_acc = model_val_dict['acc']\n",
    "model_val_acc = model_val_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L2')\n",
    "plt.plot(epochs, val_acc_values, 'g', label='Validation acc L2')\n",
    "plt.plot(epochs, model_acc, 'r', label='Training acc')\n",
    "plt.plot(epochs, model_val_acc, 'r', label='Validation acc')\n",
    "plt.title('Training & validation accuracy L2 vs regular')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of L2 regularization are quite disappointing here. We notice the discrepancy between validation and training accuracy seems to have decreased slightly, but the end result is definitely not getting better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at L1 regularization. Will this work better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "6500/6500 [==============================] - 0s 68us/step - loss: 6.4432 - acc: 0.4228 - val_loss: 3.1322 - val_acc: 0.5870\n",
      "Epoch 2/120\n",
      "6500/6500 [==============================] - 0s 29us/step - loss: 2.3891 - acc: 0.6320 - val_loss: 1.9125 - val_acc: 0.6810\n",
      "Epoch 3/120\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 1.7636 - acc: 0.6875 - val_loss: 1.6678 - val_acc: 0.6970\n",
      "Epoch 4/120\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.5811 - acc: 0.7095 - val_loss: 1.5450 - val_acc: 0.6980\n",
      "Epoch 5/120\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 1.4914 - acc: 0.7134 - val_loss: 1.4874 - val_acc: 0.6920\n",
      "Epoch 6/120\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 1.4336 - acc: 0.7180 - val_loss: 1.4254 - val_acc: 0.7180\n",
      "Epoch 7/120\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 1.3835 - acc: 0.7226 - val_loss: 1.4119 - val_acc: 0.7040\n",
      "Epoch 8/120\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 1.3499 - acc: 0.7274 - val_loss: 1.3615 - val_acc: 0.7150\n",
      "Epoch 9/120\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 1.3078 - acc: 0.7323 - val_loss: 1.3255 - val_acc: 0.7170\n",
      "Epoch 10/120\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 1.2911 - acc: 0.7340 - val_loss: 1.3022 - val_acc: 0.7250\n",
      "Epoch 11/120\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 1.2635 - acc: 0.7420 - val_loss: 1.2909 - val_acc: 0.7150\n",
      "Epoch 12/120\n",
      "6500/6500 [==============================] - 0s 48us/step - loss: 1.2471 - acc: 0.7422 - val_loss: 1.2772 - val_acc: 0.7280\n",
      "Epoch 13/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.2287 - acc: 0.7411 - val_loss: 1.2551 - val_acc: 0.7330\n",
      "Epoch 14/120\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 1.2128 - acc: 0.7432 - val_loss: 1.2381 - val_acc: 0.7330\n",
      "Epoch 15/120\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 1.1991 - acc: 0.7463 - val_loss: 1.2334 - val_acc: 0.7230\n",
      "Epoch 16/120\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 1.1956 - acc: 0.7482 - val_loss: 1.2256 - val_acc: 0.7220\n",
      "Epoch 17/120\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 1.1877 - acc: 0.7503 - val_loss: 1.2114 - val_acc: 0.7340\n",
      "Epoch 18/120\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 1.1679 - acc: 0.7505 - val_loss: 1.2080 - val_acc: 0.7310\n",
      "Epoch 19/120\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 1.1627 - acc: 0.7517 - val_loss: 1.1939 - val_acc: 0.7290\n",
      "Epoch 20/120\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 1.1509 - acc: 0.7545 - val_loss: 1.1884 - val_acc: 0.7370\n",
      "Epoch 21/120\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 1.1433 - acc: 0.7551 - val_loss: 1.1838 - val_acc: 0.7400\n",
      "Epoch 22/120\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 1.1400 - acc: 0.7549 - val_loss: 1.1777 - val_acc: 0.7350\n",
      "Epoch 23/120\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 1.1332 - acc: 0.7605 - val_loss: 1.1660 - val_acc: 0.7440\n",
      "Epoch 24/120\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 1.1301 - acc: 0.7597 - val_loss: 1.1657 - val_acc: 0.7380\n",
      "Epoch 25/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.1190 - acc: 0.7598 - val_loss: 1.1550 - val_acc: 0.7420\n",
      "Epoch 26/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.1106 - acc: 0.7612 - val_loss: 1.1617 - val_acc: 0.7330\n",
      "Epoch 27/120\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 1.1064 - acc: 0.7628 - val_loss: 1.1509 - val_acc: 0.7410\n",
      "Epoch 28/120\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 1.1021 - acc: 0.7597 - val_loss: 1.1474 - val_acc: 0.7440\n",
      "Epoch 29/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.0955 - acc: 0.7718 - val_loss: 1.1379 - val_acc: 0.7520\n",
      "Epoch 30/120\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 1.0947 - acc: 0.7645 - val_loss: 1.1412 - val_acc: 0.7430\n",
      "Epoch 31/120\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 1.0907 - acc: 0.7648 - val_loss: 1.1280 - val_acc: 0.7520\n",
      "Epoch 32/120\n",
      "6500/6500 [==============================] - 0s 46us/step - loss: 1.0790 - acc: 0.7674 - val_loss: 1.1287 - val_acc: 0.7490\n",
      "Epoch 33/120\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 1.0731 - acc: 0.7692 - val_loss: 1.1185 - val_acc: 0.7530\n",
      "Epoch 34/120\n",
      "6500/6500 [==============================] - 0s 29us/step - loss: 1.0693 - acc: 0.7706 - val_loss: 1.1212 - val_acc: 0.7400\n",
      "Epoch 35/120\n",
      "6500/6500 [==============================] - 0s 28us/step - loss: 1.0682 - acc: 0.7678 - val_loss: 1.1138 - val_acc: 0.7540\n",
      "Epoch 36/120\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 1.0653 - acc: 0.7683 - val_loss: 1.1220 - val_acc: 0.7480\n",
      "Epoch 37/120\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.0609 - acc: 0.7702 - val_loss: 1.1196 - val_acc: 0.7450\n",
      "Epoch 38/120\n",
      "6500/6500 [==============================] - 0s 45us/step - loss: 1.0537 - acc: 0.7712 - val_loss: 1.1055 - val_acc: 0.7490\n",
      "Epoch 39/120\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 1.0544 - acc: 0.7732 - val_loss: 1.0989 - val_acc: 0.7630\n",
      "Epoch 40/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.0436 - acc: 0.7742 - val_loss: 1.0968 - val_acc: 0.7590\n",
      "Epoch 41/120\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.0457 - acc: 0.7748 - val_loss: 1.0938 - val_acc: 0.7570\n",
      "Epoch 42/120\n",
      "6500/6500 [==============================] - 0s 46us/step - loss: 1.0400 - acc: 0.7754 - val_loss: 1.0920 - val_acc: 0.7550\n",
      "Epoch 43/120\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 1.0329 - acc: 0.7758 - val_loss: 1.0926 - val_acc: 0.7470\n",
      "Epoch 44/120\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 1.0373 - acc: 0.7737 - val_loss: 1.0986 - val_acc: 0.7440\n",
      "Epoch 45/120\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 1.0295 - acc: 0.7748 - val_loss: 1.0866 - val_acc: 0.7510\n",
      "Epoch 46/120\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 1.0242 - acc: 0.7800 - val_loss: 1.0828 - val_acc: 0.7480\n",
      "Epoch 47/120\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.0202 - acc: 0.7765 - val_loss: 1.0944 - val_acc: 0.7500\n",
      "Epoch 48/120\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 1.0187 - acc: 0.7760 - val_loss: 1.0802 - val_acc: 0.7550\n",
      "Epoch 49/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.0150 - acc: 0.7783 - val_loss: 1.0813 - val_acc: 0.7410\n",
      "Epoch 50/120\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 1.0182 - acc: 0.7835 - val_loss: 1.0782 - val_acc: 0.7470\n",
      "Epoch 51/120\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 1.0138 - acc: 0.7817 - val_loss: 1.0738 - val_acc: 0.7560\n",
      "Epoch 52/120\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 1.0062 - acc: 0.7831 - val_loss: 1.0691 - val_acc: 0.7440\n",
      "Epoch 53/120\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 1.0038 - acc: 0.7820 - val_loss: 1.0726 - val_acc: 0.7630\n",
      "Epoch 54/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9989 - acc: 0.7848 - val_loss: 1.0735 - val_acc: 0.7560\n",
      "Epoch 55/120\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 1.0033 - acc: 0.7842 - val_loss: 1.0618 - val_acc: 0.7500\n",
      "Epoch 56/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.0001 - acc: 0.7843 - val_loss: 1.0694 - val_acc: 0.7460\n",
      "Epoch 57/120\n",
      "6500/6500 [==============================] - 0s 49us/step - loss: 0.9937 - acc: 0.7866 - val_loss: 1.0665 - val_acc: 0.7450\n",
      "Epoch 58/120\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.9937 - acc: 0.7838 - val_loss: 1.0601 - val_acc: 0.7520\n",
      "Epoch 59/120\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.9893 - acc: 0.7868 - val_loss: 1.0667 - val_acc: 0.7470\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.9839 - acc: 0.7908 - val_loss: 1.0637 - val_acc: 0.7450\n",
      "Epoch 61/120\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.9874 - acc: 0.7862 - val_loss: 1.0632 - val_acc: 0.7520\n",
      "Epoch 62/120\n",
      "6500/6500 [==============================] - 0s 29us/step - loss: 0.9858 - acc: 0.7872 - val_loss: 1.0621 - val_acc: 0.7500\n",
      "Epoch 63/120\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.9803 - acc: 0.7945 - val_loss: 1.0683 - val_acc: 0.7510\n",
      "Epoch 64/120\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.9761 - acc: 0.7897 - val_loss: 1.0576 - val_acc: 0.7490\n",
      "Epoch 65/120\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.9793 - acc: 0.7889 - val_loss: 1.0625 - val_acc: 0.7510\n",
      "Epoch 66/120\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.9758 - acc: 0.7942 - val_loss: 1.0718 - val_acc: 0.7410\n",
      "Epoch 67/120\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.9736 - acc: 0.7932 - val_loss: 1.0519 - val_acc: 0.7510\n",
      "Epoch 68/120\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.9712 - acc: 0.7920 - val_loss: 1.0490 - val_acc: 0.7610\n",
      "Epoch 69/120\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.9656 - acc: 0.7965 - val_loss: 1.0490 - val_acc: 0.7590\n",
      "Epoch 70/120\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.9661 - acc: 0.7918 - val_loss: 1.0534 - val_acc: 0.7520\n",
      "Epoch 71/120\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.9600 - acc: 0.7955 - val_loss: 1.0460 - val_acc: 0.7560\n",
      "Epoch 72/120\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.9610 - acc: 0.7983 - val_loss: 1.0498 - val_acc: 0.7590\n",
      "Epoch 73/120\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.9585 - acc: 0.7971 - val_loss: 1.0499 - val_acc: 0.7510\n",
      "Epoch 74/120\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.9602 - acc: 0.7972 - val_loss: 1.0562 - val_acc: 0.7490\n",
      "Epoch 75/120\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.9582 - acc: 0.8012 - val_loss: 1.0449 - val_acc: 0.7520\n",
      "Epoch 76/120\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.9551 - acc: 0.8009 - val_loss: 1.0621 - val_acc: 0.7420\n",
      "Epoch 77/120\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 0.9609 - acc: 0.7917 - val_loss: 1.0458 - val_acc: 0.7560\n",
      "Epoch 78/120\n",
      "6500/6500 [==============================] - 0s 45us/step - loss: 0.9542 - acc: 0.8018 - val_loss: 1.0572 - val_acc: 0.7460\n",
      "Epoch 79/120\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.9529 - acc: 0.8000 - val_loss: 1.0439 - val_acc: 0.7570\n",
      "Epoch 80/120\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.9460 - acc: 0.7988 - val_loss: 1.0497 - val_acc: 0.7520\n",
      "Epoch 81/120\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.9478 - acc: 0.8017 - val_loss: 1.0475 - val_acc: 0.7560\n",
      "Epoch 82/120\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.9441 - acc: 0.8000 - val_loss: 1.0410 - val_acc: 0.7620\n",
      "Epoch 83/120\n",
      "6500/6500 [==============================] - 0s 46us/step - loss: 0.9436 - acc: 0.8018 - val_loss: 1.0414 - val_acc: 0.7590\n",
      "Epoch 84/120\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.9420 - acc: 0.8066 - val_loss: 1.0384 - val_acc: 0.7550\n",
      "Epoch 85/120\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.9426 - acc: 0.8063 - val_loss: 1.0485 - val_acc: 0.7580\n",
      "Epoch 86/120\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.9377 - acc: 0.8057 - val_loss: 1.0348 - val_acc: 0.7540\n",
      "Epoch 87/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9335 - acc: 0.8069 - val_loss: 1.0407 - val_acc: 0.7510\n",
      "Epoch 88/120\n",
      "6500/6500 [==============================] - 0s 44us/step - loss: 0.9373 - acc: 0.8078 - val_loss: 1.0404 - val_acc: 0.7550\n",
      "Epoch 89/120\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.9288 - acc: 0.8126 - val_loss: 1.0332 - val_acc: 0.7630\n",
      "Epoch 90/120\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.9256 - acc: 0.8128 - val_loss: 1.0340 - val_acc: 0.7540\n",
      "Epoch 91/120\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.9236 - acc: 0.8125 - val_loss: 1.0399 - val_acc: 0.7520\n",
      "Epoch 92/120\n",
      "6500/6500 [==============================] - 0s 29us/step - loss: 0.9233 - acc: 0.8149 - val_loss: 1.0373 - val_acc: 0.7540\n",
      "Epoch 93/120\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.9229 - acc: 0.8155 - val_loss: 1.0315 - val_acc: 0.7590\n",
      "Epoch 94/120\n",
      "6500/6500 [==============================] - 0s 29us/step - loss: 0.9165 - acc: 0.8178 - val_loss: 1.0312 - val_acc: 0.7520\n",
      "Epoch 95/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9196 - acc: 0.8106 - val_loss: 1.0368 - val_acc: 0.7520\n",
      "Epoch 96/120\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.9163 - acc: 0.8162 - val_loss: 1.0356 - val_acc: 0.7620\n",
      "Epoch 97/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9118 - acc: 0.8158 - val_loss: 1.0313 - val_acc: 0.7540\n",
      "Epoch 98/120\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.9090 - acc: 0.8169 - val_loss: 1.0286 - val_acc: 0.7570\n",
      "Epoch 99/120\n",
      "6500/6500 [==============================] - 0s 29us/step - loss: 0.9112 - acc: 0.8186 - val_loss: 1.0365 - val_acc: 0.7550\n",
      "Epoch 100/120\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.9118 - acc: 0.8235 - val_loss: 1.0293 - val_acc: 0.7530\n",
      "Epoch 101/120\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.9101 - acc: 0.8174 - val_loss: 1.0308 - val_acc: 0.7590\n",
      "Epoch 102/120\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.9058 - acc: 0.8206 - val_loss: 1.0236 - val_acc: 0.7550\n",
      "Epoch 103/120\n",
      "6500/6500 [==============================] - 0s 56us/step - loss: 0.9048 - acc: 0.8242 - val_loss: 1.0223 - val_acc: 0.7610\n",
      "Epoch 104/120\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 0.8980 - acc: 0.8237 - val_loss: 1.0364 - val_acc: 0.7600\n",
      "Epoch 105/120\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.9043 - acc: 0.8206 - val_loss: 1.0268 - val_acc: 0.7580\n",
      "Epoch 106/120\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.9042 - acc: 0.8238 - val_loss: 1.0392 - val_acc: 0.7600\n",
      "Epoch 107/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9001 - acc: 0.8257 - val_loss: 1.0244 - val_acc: 0.7620\n",
      "Epoch 108/120\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8939 - acc: 0.8282 - val_loss: 1.0260 - val_acc: 0.7520\n",
      "Epoch 109/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8918 - acc: 0.8306 - val_loss: 1.0288 - val_acc: 0.7540\n",
      "Epoch 110/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8924 - acc: 0.8268 - val_loss: 1.0268 - val_acc: 0.7580\n",
      "Epoch 111/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8854 - acc: 0.8331 - val_loss: 1.0167 - val_acc: 0.7660\n",
      "Epoch 112/120\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8834 - acc: 0.8328 - val_loss: 1.0170 - val_acc: 0.7680\n",
      "Epoch 113/120\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.8879 - acc: 0.8285 - val_loss: 1.0382 - val_acc: 0.7580\n",
      "Epoch 114/120\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8864 - acc: 0.8317 - val_loss: 1.0228 - val_acc: 0.7570\n",
      "Epoch 115/120\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.8852 - acc: 0.8326 - val_loss: 1.0226 - val_acc: 0.7600\n",
      "Epoch 116/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8823 - acc: 0.8342 - val_loss: 1.0223 - val_acc: 0.7550\n",
      "Epoch 117/120\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.8821 - acc: 0.8391 - val_loss: 1.0366 - val_acc: 0.7560\n",
      "Epoch 118/120\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.8799 - acc: 0.8371 - val_loss: 1.0307 - val_acc: 0.7590\n",
      "Epoch 119/120\n",
      "6500/6500 [==============================] - 0s 29us/step - loss: 0.8796 - acc: 0.8375 - val_loss: 1.0243 - val_acc: 0.7660\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.8749 - acc: 0.8380 - val_loss: 1.0371 - val_acc: 0.7550\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = Sequential()\n",
    "model.add(Dense(50, activation='relu',kernel_regularizer=regularizers.l1(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(Dense(25, kernel_regularizer=regularizers.l1(0.005), activation='relu'))\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer=adam,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L1_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "L1_model_dict = L1_model.history\n",
    "plt.clf()\n",
    "\n",
    "acc_values = L1_model_dict['acc'] \n",
    "val_acc_values = L1_model_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L1')\n",
    "plt.plot(epochs, val_acc_values, 'g.', label='Validation acc L1')\n",
    "plt.title('Training & validation accuracy with L1 regularization')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how The training and validation accuracy don't diverge as much as before! Unfortunately, the validation accuracy doesn't reach rates much higher than 70%. It does seem like we can still improve the model by training much longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6500 samples, validate on 1000 samples\n",
      "Epoch 1/1000\n",
      "6500/6500 [==============================] - 0s 71us/step - loss: 6.4706 - acc: 0.3805 - val_loss: 3.1572 - val_acc: 0.5660\n",
      "Epoch 2/1000\n",
      "6500/6500 [==============================] - 0s 29us/step - loss: 2.3879 - acc: 0.6134 - val_loss: 1.9242 - val_acc: 0.6720\n",
      "Epoch 3/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 1.7896 - acc: 0.6786 - val_loss: 1.6762 - val_acc: 0.6950\n",
      "Epoch 4/1000\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 1.6057 - acc: 0.7026 - val_loss: 1.5646 - val_acc: 0.7000\n",
      "Epoch 5/1000\n",
      "6500/6500 [==============================] - 0s 46us/step - loss: 1.4973 - acc: 0.7168 - val_loss: 1.4862 - val_acc: 0.7050\n",
      "Epoch 6/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.4358 - acc: 0.7194 - val_loss: 1.4214 - val_acc: 0.7150\n",
      "Epoch 7/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 1.3762 - acc: 0.7285 - val_loss: 1.3808 - val_acc: 0.7090\n",
      "Epoch 8/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.3356 - acc: 0.7329 - val_loss: 1.3512 - val_acc: 0.7150\n",
      "Epoch 9/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 1.3018 - acc: 0.7386 - val_loss: 1.3145 - val_acc: 0.7250\n",
      "Epoch 10/1000\n",
      "6500/6500 [==============================] - 0s 28us/step - loss: 1.2726 - acc: 0.7408 - val_loss: 1.2914 - val_acc: 0.7200\n",
      "Epoch 11/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 1.2422 - acc: 0.7435 - val_loss: 1.2650 - val_acc: 0.7260\n",
      "Epoch 12/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 1.2180 - acc: 0.7423 - val_loss: 1.2469 - val_acc: 0.7220\n",
      "Epoch 13/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 1.2000 - acc: 0.7452 - val_loss: 1.2306 - val_acc: 0.7200\n",
      "Epoch 14/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 1.1799 - acc: 0.7474 - val_loss: 1.2256 - val_acc: 0.7270\n",
      "Epoch 15/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 1.1648 - acc: 0.7537 - val_loss: 1.2226 - val_acc: 0.7160\n",
      "Epoch 16/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 1.1610 - acc: 0.7494 - val_loss: 1.1919 - val_acc: 0.7290\n",
      "Epoch 17/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 1.1525 - acc: 0.7486 - val_loss: 1.1907 - val_acc: 0.7360\n",
      "Epoch 18/1000\n",
      "6500/6500 [==============================] - 0s 28us/step - loss: 1.1352 - acc: 0.7526 - val_loss: 1.1792 - val_acc: 0.7320\n",
      "Epoch 19/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 1.1269 - acc: 0.7560 - val_loss: 1.1661 - val_acc: 0.7330\n",
      "Epoch 20/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.1139 - acc: 0.7562 - val_loss: 1.1547 - val_acc: 0.7340\n",
      "Epoch 21/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 1.1008 - acc: 0.7538 - val_loss: 1.1517 - val_acc: 0.7280\n",
      "Epoch 22/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 1.0962 - acc: 0.7582 - val_loss: 1.1429 - val_acc: 0.7380\n",
      "Epoch 23/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 1.0840 - acc: 0.7595 - val_loss: 1.1401 - val_acc: 0.7270\n",
      "Epoch 24/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 1.0796 - acc: 0.7605 - val_loss: 1.1289 - val_acc: 0.7360\n",
      "Epoch 25/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 1.0727 - acc: 0.7602 - val_loss: 1.1222 - val_acc: 0.7370\n",
      "Epoch 26/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 1.0694 - acc: 0.7629 - val_loss: 1.1452 - val_acc: 0.7200\n",
      "Epoch 27/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 1.0645 - acc: 0.7642 - val_loss: 1.1197 - val_acc: 0.7380\n",
      "Epoch 28/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 1.0609 - acc: 0.7609 - val_loss: 1.1138 - val_acc: 0.7370\n",
      "Epoch 29/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 1.0548 - acc: 0.7646 - val_loss: 1.1216 - val_acc: 0.7410\n",
      "Epoch 30/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 1.0511 - acc: 0.7669 - val_loss: 1.1094 - val_acc: 0.7440\n",
      "Epoch 31/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 1.0447 - acc: 0.7631 - val_loss: 1.1087 - val_acc: 0.7380\n",
      "Epoch 32/1000\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 1.0438 - acc: 0.7686 - val_loss: 1.1051 - val_acc: 0.7420\n",
      "Epoch 33/1000\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 1.0473 - acc: 0.7646 - val_loss: 1.1030 - val_acc: 0.7420\n",
      "Epoch 34/1000\n",
      "6500/6500 [==============================] - 0s 29us/step - loss: 1.0394 - acc: 0.7686 - val_loss: 1.1084 - val_acc: 0.7380\n",
      "Epoch 35/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 1.0312 - acc: 0.7714 - val_loss: 1.0906 - val_acc: 0.7440\n",
      "Epoch 36/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 1.0285 - acc: 0.7708 - val_loss: 1.0962 - val_acc: 0.7520\n",
      "Epoch 37/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 1.0263 - acc: 0.7706 - val_loss: 1.0896 - val_acc: 0.7480\n",
      "Epoch 38/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 1.0255 - acc: 0.7691 - val_loss: 1.0921 - val_acc: 0.7490\n",
      "Epoch 39/1000\n",
      "6500/6500 [==============================] - 0s 44us/step - loss: 1.0246 - acc: 0.7725 - val_loss: 1.0897 - val_acc: 0.7410\n",
      "Epoch 40/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 1.0184 - acc: 0.7726 - val_loss: 1.0868 - val_acc: 0.7440\n",
      "Epoch 41/1000\n",
      "6500/6500 [==============================] - 0s 29us/step - loss: 1.0172 - acc: 0.7703 - val_loss: 1.0888 - val_acc: 0.7510\n",
      "Epoch 42/1000\n",
      "6500/6500 [==============================] - 0s 28us/step - loss: 1.0193 - acc: 0.7677 - val_loss: 1.0914 - val_acc: 0.7450\n",
      "Epoch 43/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 1.0125 - acc: 0.7751 - val_loss: 1.0824 - val_acc: 0.7480\n",
      "Epoch 44/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 1.0061 - acc: 0.7743 - val_loss: 1.0785 - val_acc: 0.7450\n",
      "Epoch 45/1000\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 1.0050 - acc: 0.7745 - val_loss: 1.0886 - val_acc: 0.7490\n",
      "Epoch 46/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 1.0052 - acc: 0.7749 - val_loss: 1.0818 - val_acc: 0.7410\n",
      "Epoch 47/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.9978 - acc: 0.7754 - val_loss: 1.0756 - val_acc: 0.7500\n",
      "Epoch 48/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.9987 - acc: 0.7778 - val_loss: 1.0819 - val_acc: 0.7450\n",
      "Epoch 49/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 1.0026 - acc: 0.7754 - val_loss: 1.0780 - val_acc: 0.7460\n",
      "Epoch 50/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 1.0032 - acc: 0.7755 - val_loss: 1.0866 - val_acc: 0.7450\n",
      "Epoch 51/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 1.0013 - acc: 0.7758 - val_loss: 1.0802 - val_acc: 0.7470\n",
      "Epoch 52/1000\n",
      "6500/6500 [==============================] - 0s 55us/step - loss: 0.9904 - acc: 0.7792 - val_loss: 1.0751 - val_acc: 0.7450\n",
      "Epoch 53/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.9892 - acc: 0.7760 - val_loss: 1.0738 - val_acc: 0.7380\n",
      "Epoch 54/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.9902 - acc: 0.7765 - val_loss: 1.0781 - val_acc: 0.7460\n",
      "Epoch 55/1000\n",
      "6500/6500 [==============================] - 0s 29us/step - loss: 0.9887 - acc: 0.7789 - val_loss: 1.0778 - val_acc: 0.7470\n",
      "Epoch 56/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.9818 - acc: 0.7806 - val_loss: 1.0734 - val_acc: 0.7450\n",
      "Epoch 57/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.9771 - acc: 0.7811 - val_loss: 1.0627 - val_acc: 0.7530\n",
      "Epoch 58/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.9780 - acc: 0.7802 - val_loss: 1.0697 - val_acc: 0.7410\n",
      "Epoch 59/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.9774 - acc: 0.7802 - val_loss: 1.0663 - val_acc: 0.7410\n",
      "Epoch 60/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.9709 - acc: 0.7845 - val_loss: 1.0682 - val_acc: 0.7430\n",
      "Epoch 61/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.9745 - acc: 0.7805 - val_loss: 1.0753 - val_acc: 0.7400\n",
      "Epoch 62/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.9749 - acc: 0.7794 - val_loss: 1.0700 - val_acc: 0.7460\n",
      "Epoch 63/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.9671 - acc: 0.7852 - val_loss: 1.0614 - val_acc: 0.7460\n",
      "Epoch 64/1000\n",
      "6500/6500 [==============================] - 0s 29us/step - loss: 0.9647 - acc: 0.7871 - val_loss: 1.0614 - val_acc: 0.7400\n",
      "Epoch 65/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.9629 - acc: 0.7851 - val_loss: 1.0553 - val_acc: 0.7530\n",
      "Epoch 66/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.9615 - acc: 0.7868 - val_loss: 1.0604 - val_acc: 0.7450\n",
      "Epoch 67/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.9591 - acc: 0.7868 - val_loss: 1.0828 - val_acc: 0.7380\n",
      "Epoch 68/1000\n",
      "6500/6500 [==============================] - 0s 29us/step - loss: 0.9663 - acc: 0.7809 - val_loss: 1.0622 - val_acc: 0.7510\n",
      "Epoch 69/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.9621 - acc: 0.7829 - val_loss: 1.0587 - val_acc: 0.7480\n",
      "Epoch 70/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.9518 - acc: 0.7858 - val_loss: 1.0511 - val_acc: 0.7500\n",
      "Epoch 71/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.9510 - acc: 0.7869 - val_loss: 1.0523 - val_acc: 0.7470\n",
      "Epoch 72/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9482 - acc: 0.7868 - val_loss: 1.0568 - val_acc: 0.7460\n",
      "Epoch 73/1000\n",
      "6500/6500 [==============================] - 0s 45us/step - loss: 0.9493 - acc: 0.7920 - val_loss: 1.0558 - val_acc: 0.7530\n",
      "Epoch 74/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.9449 - acc: 0.7874 - val_loss: 1.0526 - val_acc: 0.7450\n",
      "Epoch 75/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.9445 - acc: 0.7915 - val_loss: 1.0473 - val_acc: 0.7490\n",
      "Epoch 76/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.9412 - acc: 0.7911 - val_loss: 1.0490 - val_acc: 0.7460\n",
      "Epoch 77/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.9414 - acc: 0.7934 - val_loss: 1.0459 - val_acc: 0.7530\n",
      "Epoch 78/1000\n",
      "6500/6500 [==============================] - 0s 45us/step - loss: 0.9360 - acc: 0.7912 - val_loss: 1.0454 - val_acc: 0.7490\n",
      "Epoch 79/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.9409 - acc: 0.7915 - val_loss: 1.0551 - val_acc: 0.7460\n",
      "Epoch 80/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.9374 - acc: 0.7937 - val_loss: 1.0509 - val_acc: 0.7440\n",
      "Epoch 81/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.9330 - acc: 0.7937 - val_loss: 1.0475 - val_acc: 0.7510\n",
      "Epoch 82/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.9308 - acc: 0.7951 - val_loss: 1.0445 - val_acc: 0.7520\n",
      "Epoch 83/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 0.9347 - acc: 0.7951 - val_loss: 1.0547 - val_acc: 0.7490\n",
      "Epoch 84/1000\n",
      "6500/6500 [==============================] - 0s 44us/step - loss: 0.9311 - acc: 0.7988 - val_loss: 1.0384 - val_acc: 0.7550\n",
      "Epoch 85/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.9315 - acc: 0.8003 - val_loss: 1.0445 - val_acc: 0.7480\n",
      "Epoch 86/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.9323 - acc: 0.7940 - val_loss: 1.0556 - val_acc: 0.7400\n",
      "Epoch 87/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.9312 - acc: 0.7943 - val_loss: 1.0447 - val_acc: 0.7420\n",
      "Epoch 88/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.9226 - acc: 0.7991 - val_loss: 1.0461 - val_acc: 0.7410\n",
      "Epoch 89/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.9235 - acc: 0.7971 - val_loss: 1.0512 - val_acc: 0.7430\n",
      "Epoch 90/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.9224 - acc: 0.8003 - val_loss: 1.0376 - val_acc: 0.7560\n",
      "Epoch 91/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.9194 - acc: 0.7995 - val_loss: 1.0441 - val_acc: 0.7530\n",
      "Epoch 92/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.9206 - acc: 0.7971 - val_loss: 1.0450 - val_acc: 0.7490\n",
      "Epoch 93/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.9196 - acc: 0.7968 - val_loss: 1.0482 - val_acc: 0.7490\n",
      "Epoch 94/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.9199 - acc: 0.7972 - val_loss: 1.0427 - val_acc: 0.7530\n",
      "Epoch 95/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.9158 - acc: 0.7971 - val_loss: 1.0332 - val_acc: 0.7610\n",
      "Epoch 96/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.9147 - acc: 0.7977 - val_loss: 1.0330 - val_acc: 0.7560\n",
      "Epoch 97/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 0.9099 - acc: 0.8020 - val_loss: 1.0363 - val_acc: 0.7510\n",
      "Epoch 98/1000\n",
      "6500/6500 [==============================] - 0s 46us/step - loss: 0.9096 - acc: 0.8031 - val_loss: 1.0455 - val_acc: 0.7500\n",
      "Epoch 99/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.9073 - acc: 0.8011 - val_loss: 1.0362 - val_acc: 0.7530\n",
      "Epoch 100/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.9058 - acc: 0.8040 - val_loss: 1.0388 - val_acc: 0.7510\n",
      "Epoch 101/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.9073 - acc: 0.8046 - val_loss: 1.0373 - val_acc: 0.7530\n",
      "Epoch 102/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.9154 - acc: 0.7995 - val_loss: 1.0414 - val_acc: 0.7540\n",
      "Epoch 103/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.9052 - acc: 0.8049 - val_loss: 1.0368 - val_acc: 0.7570\n",
      "Epoch 104/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8993 - acc: 0.8065 - val_loss: 1.0410 - val_acc: 0.7510\n",
      "Epoch 105/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.9033 - acc: 0.8025 - val_loss: 1.0374 - val_acc: 0.7500\n",
      "Epoch 106/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.8999 - acc: 0.8083 - val_loss: 1.0382 - val_acc: 0.7540\n",
      "Epoch 107/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.9002 - acc: 0.8048 - val_loss: 1.0336 - val_acc: 0.7500\n",
      "Epoch 108/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8971 - acc: 0.8049 - val_loss: 1.0372 - val_acc: 0.7480\n",
      "Epoch 109/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.9002 - acc: 0.8032 - val_loss: 1.0423 - val_acc: 0.7600\n",
      "Epoch 110/1000\n",
      "6500/6500 [==============================] - 0s 29us/step - loss: 0.8953 - acc: 0.8078 - val_loss: 1.0441 - val_acc: 0.7490\n",
      "Epoch 111/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.8925 - acc: 0.8082 - val_loss: 1.0366 - val_acc: 0.7560\n",
      "Epoch 112/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.8968 - acc: 0.8054 - val_loss: 1.0455 - val_acc: 0.7530\n",
      "Epoch 113/1000\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 0.8961 - acc: 0.8115 - val_loss: 1.0315 - val_acc: 0.7560\n",
      "Epoch 114/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 0.8954 - acc: 0.8082 - val_loss: 1.0535 - val_acc: 0.7470\n",
      "Epoch 115/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.8959 - acc: 0.8042 - val_loss: 1.0457 - val_acc: 0.7550\n",
      "Epoch 116/1000\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 0.8932 - acc: 0.8066 - val_loss: 1.0357 - val_acc: 0.7550\n",
      "Epoch 117/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.8938 - acc: 0.8105 - val_loss: 1.0355 - val_acc: 0.7550\n",
      "Epoch 118/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.8834 - acc: 0.8148 - val_loss: 1.0309 - val_acc: 0.7610\n",
      "Epoch 119/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.8885 - acc: 0.8115 - val_loss: 1.0389 - val_acc: 0.7560\n",
      "Epoch 120/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.8851 - acc: 0.8138 - val_loss: 1.0286 - val_acc: 0.7670\n",
      "Epoch 121/1000\n",
      "6500/6500 [==============================] - 0s 28us/step - loss: 0.8783 - acc: 0.8160 - val_loss: 1.0231 - val_acc: 0.7640\n",
      "Epoch 122/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.8795 - acc: 0.8143 - val_loss: 1.0321 - val_acc: 0.7590\n",
      "Epoch 123/1000\n",
      "6500/6500 [==============================] - 0s 44us/step - loss: 0.8849 - acc: 0.8137 - val_loss: 1.0352 - val_acc: 0.7600\n",
      "Epoch 124/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.8784 - acc: 0.8198 - val_loss: 1.0385 - val_acc: 0.7570\n",
      "Epoch 125/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.8795 - acc: 0.8172 - val_loss: 1.0365 - val_acc: 0.7580\n",
      "Epoch 126/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.8761 - acc: 0.8197 - val_loss: 1.0367 - val_acc: 0.7570\n",
      "Epoch 127/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8769 - acc: 0.8174 - val_loss: 1.0326 - val_acc: 0.7580\n",
      "Epoch 128/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 0.8795 - acc: 0.8138 - val_loss: 1.0424 - val_acc: 0.7600\n",
      "Epoch 129/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.8719 - acc: 0.8178 - val_loss: 1.0361 - val_acc: 0.7540\n",
      "Epoch 130/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.8772 - acc: 0.8145 - val_loss: 1.0403 - val_acc: 0.7520\n",
      "Epoch 131/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.8770 - acc: 0.8152 - val_loss: 1.0291 - val_acc: 0.7570\n",
      "Epoch 132/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.8740 - acc: 0.8169 - val_loss: 1.0332 - val_acc: 0.7620\n",
      "Epoch 133/1000\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 0.8716 - acc: 0.8198 - val_loss: 1.0388 - val_acc: 0.7540\n",
      "Epoch 134/1000\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 0.8704 - acc: 0.8192 - val_loss: 1.0452 - val_acc: 0.7520\n",
      "Epoch 135/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.8750 - acc: 0.8197 - val_loss: 1.0489 - val_acc: 0.7470\n",
      "Epoch 136/1000\n",
      "6500/6500 [==============================] - 0s 29us/step - loss: 0.8763 - acc: 0.8205 - val_loss: 1.0467 - val_acc: 0.7480\n",
      "Epoch 137/1000\n",
      "6500/6500 [==============================] - 0s 29us/step - loss: 0.8656 - acc: 0.8222 - val_loss: 1.0320 - val_acc: 0.7490\n",
      "Epoch 138/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.8708 - acc: 0.8203 - val_loss: 1.0339 - val_acc: 0.7540\n",
      "Epoch 139/1000\n",
      "6500/6500 [==============================] - 0s 28us/step - loss: 0.8650 - acc: 0.8245 - val_loss: 1.0336 - val_acc: 0.7610\n",
      "Epoch 140/1000\n",
      "6500/6500 [==============================] - 0s 28us/step - loss: 0.8584 - acc: 0.8289 - val_loss: 1.0395 - val_acc: 0.7500\n",
      "Epoch 141/1000\n",
      "6500/6500 [==============================] - 0s 28us/step - loss: 0.8646 - acc: 0.8218 - val_loss: 1.0325 - val_acc: 0.7540\n",
      "Epoch 142/1000\n",
      "6500/6500 [==============================] - 0s 29us/step - loss: 0.8686 - acc: 0.8228 - val_loss: 1.0474 - val_acc: 0.7610\n",
      "Epoch 143/1000\n",
      "6500/6500 [==============================] - 0s 50us/step - loss: 0.8668 - acc: 0.8231 - val_loss: 1.0318 - val_acc: 0.7540\n",
      "Epoch 144/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.8669 - acc: 0.8246 - val_loss: 1.0687 - val_acc: 0.7410\n",
      "Epoch 145/1000\n",
      "6500/6500 [==============================] - 0s 29us/step - loss: 0.8728 - acc: 0.8234 - val_loss: 1.0425 - val_acc: 0.7610\n",
      "Epoch 146/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.8550 - acc: 0.8337 - val_loss: 1.0323 - val_acc: 0.7500\n",
      "Epoch 147/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.8555 - acc: 0.8306 - val_loss: 1.0310 - val_acc: 0.7500\n",
      "Epoch 148/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.8587 - acc: 0.8320 - val_loss: 1.0362 - val_acc: 0.7520\n",
      "Epoch 149/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8548 - acc: 0.8332 - val_loss: 1.0251 - val_acc: 0.7560\n",
      "Epoch 150/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.8547 - acc: 0.8340 - val_loss: 1.0257 - val_acc: 0.7570\n",
      "Epoch 151/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.8513 - acc: 0.8337 - val_loss: 1.0484 - val_acc: 0.7440\n",
      "Epoch 152/1000\n",
      "6500/6500 [==============================] - 0s 28us/step - loss: 0.8548 - acc: 0.8343 - val_loss: 1.0387 - val_acc: 0.7500\n",
      "Epoch 153/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.8528 - acc: 0.8331 - val_loss: 1.0491 - val_acc: 0.7520\n",
      "Epoch 154/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.8547 - acc: 0.8337 - val_loss: 1.0393 - val_acc: 0.7510\n",
      "Epoch 155/1000\n",
      "6500/6500 [==============================] - 0s 28us/step - loss: 0.8471 - acc: 0.8342 - val_loss: 1.0293 - val_acc: 0.7560\n",
      "Epoch 156/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.8485 - acc: 0.8354 - val_loss: 1.0492 - val_acc: 0.7330\n",
      "Epoch 157/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.8510 - acc: 0.8400 - val_loss: 1.0405 - val_acc: 0.7550\n",
      "Epoch 158/1000\n",
      "6500/6500 [==============================] - 0s 29us/step - loss: 0.8513 - acc: 0.8391 - val_loss: 1.0528 - val_acc: 0.7450\n",
      "Epoch 159/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.8529 - acc: 0.8386 - val_loss: 1.0471 - val_acc: 0.7530\n",
      "Epoch 160/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.8475 - acc: 0.8385 - val_loss: 1.0541 - val_acc: 0.7430\n",
      "Epoch 161/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.8477 - acc: 0.8394 - val_loss: 1.0455 - val_acc: 0.7480\n",
      "Epoch 162/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8436 - acc: 0.8437 - val_loss: 1.0499 - val_acc: 0.7530\n",
      "Epoch 163/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.8437 - acc: 0.8429 - val_loss: 1.0378 - val_acc: 0.7460\n",
      "Epoch 164/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.8383 - acc: 0.8440 - val_loss: 1.0401 - val_acc: 0.7520\n",
      "Epoch 165/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.8412 - acc: 0.8462 - val_loss: 1.0326 - val_acc: 0.7560\n",
      "Epoch 166/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.8444 - acc: 0.8445 - val_loss: 1.0396 - val_acc: 0.7520\n",
      "Epoch 167/1000\n",
      "6500/6500 [==============================] - 0s 28us/step - loss: 0.8351 - acc: 0.8500 - val_loss: 1.0432 - val_acc: 0.7380\n",
      "Epoch 168/1000\n",
      "6500/6500 [==============================] - 0s 29us/step - loss: 0.8375 - acc: 0.8465 - val_loss: 1.0340 - val_acc: 0.7520\n",
      "Epoch 169/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.8380 - acc: 0.8491 - val_loss: 1.0325 - val_acc: 0.7590\n",
      "Epoch 170/1000\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 0.8397 - acc: 0.8463 - val_loss: 1.0426 - val_acc: 0.7480\n",
      "Epoch 171/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.8429 - acc: 0.8468 - val_loss: 1.0409 - val_acc: 0.7410\n",
      "Epoch 172/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.8364 - acc: 0.8482 - val_loss: 1.0382 - val_acc: 0.7510\n",
      "Epoch 173/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.8339 - acc: 0.8495 - val_loss: 1.0397 - val_acc: 0.7500\n",
      "Epoch 174/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.8364 - acc: 0.8494 - val_loss: 1.0429 - val_acc: 0.7470\n",
      "Epoch 175/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8404 - acc: 0.8451 - val_loss: 1.0585 - val_acc: 0.7410\n",
      "Epoch 176/1000\n",
      "6500/6500 [==============================] - 0s 44us/step - loss: 0.8417 - acc: 0.8442 - val_loss: 1.0383 - val_acc: 0.7570\n",
      "Epoch 177/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8317 - acc: 0.8522 - val_loss: 1.0444 - val_acc: 0.7470\n",
      "Epoch 178/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.8294 - acc: 0.8537 - val_loss: 1.0455 - val_acc: 0.7500\n",
      "Epoch 179/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.8291 - acc: 0.8522 - val_loss: 1.0342 - val_acc: 0.7400\n",
      "Epoch 180/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 0.8356 - acc: 0.8498 - val_loss: 1.0516 - val_acc: 0.7530\n",
      "Epoch 181/1000\n",
      "6500/6500 [==============================] - 0s 46us/step - loss: 0.8370 - acc: 0.8508 - val_loss: 1.0490 - val_acc: 0.7460\n",
      "Epoch 182/1000\n",
      "6500/6500 [==============================] - 0s 29us/step - loss: 0.8314 - acc: 0.8572 - val_loss: 1.0509 - val_acc: 0.7400\n",
      "Epoch 183/1000\n",
      "6500/6500 [==============================] - 0s 26us/step - loss: 0.8240 - acc: 0.8591 - val_loss: 1.0426 - val_acc: 0.7540\n",
      "Epoch 184/1000\n",
      "6500/6500 [==============================] - 0s 29us/step - loss: 0.8217 - acc: 0.8606 - val_loss: 1.0423 - val_acc: 0.7520\n",
      "Epoch 185/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.8281 - acc: 0.8580 - val_loss: 1.0443 - val_acc: 0.7570\n",
      "Epoch 186/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.8254 - acc: 0.8578 - val_loss: 1.0339 - val_acc: 0.7510\n",
      "Epoch 187/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.8220 - acc: 0.8600 - val_loss: 1.0378 - val_acc: 0.7520\n",
      "Epoch 188/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.8186 - acc: 0.8614 - val_loss: 1.0359 - val_acc: 0.7460\n",
      "Epoch 189/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.8268 - acc: 0.8558 - val_loss: 1.0403 - val_acc: 0.7550\n",
      "Epoch 190/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 0.8186 - acc: 0.8586 - val_loss: 1.0395 - val_acc: 0.7580\n",
      "Epoch 191/1000\n",
      "6500/6500 [==============================] - 0s 47us/step - loss: 0.8193 - acc: 0.8615 - val_loss: 1.0405 - val_acc: 0.7570\n",
      "Epoch 192/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.8228 - acc: 0.8620 - val_loss: 1.0440 - val_acc: 0.7470\n",
      "Epoch 193/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.8173 - acc: 0.8617 - val_loss: 1.0414 - val_acc: 0.7480\n",
      "Epoch 194/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.8196 - acc: 0.8617 - val_loss: 1.0391 - val_acc: 0.7630\n",
      "Epoch 195/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.8190 - acc: 0.8612 - val_loss: 1.0481 - val_acc: 0.7560\n",
      "Epoch 196/1000\n",
      "6500/6500 [==============================] - 0s 27us/step - loss: 0.8255 - acc: 0.8602 - val_loss: 1.0511 - val_acc: 0.7570\n",
      "Epoch 197/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.8163 - acc: 0.8657 - val_loss: 1.0430 - val_acc: 0.7650\n",
      "Epoch 198/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.8116 - acc: 0.8688 - val_loss: 1.0586 - val_acc: 0.7470\n",
      "Epoch 199/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.8174 - acc: 0.8631 - val_loss: 1.0520 - val_acc: 0.7480\n",
      "Epoch 200/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.8151 - acc: 0.8697 - val_loss: 1.0512 - val_acc: 0.7520\n",
      "Epoch 201/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.8191 - acc: 0.8637 - val_loss: 1.0557 - val_acc: 0.7510\n",
      "Epoch 202/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.8181 - acc: 0.8685 - val_loss: 1.0467 - val_acc: 0.7500\n",
      "Epoch 203/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.8136 - acc: 0.8651 - val_loss: 1.0659 - val_acc: 0.7570\n",
      "Epoch 204/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.8113 - acc: 0.8668 - val_loss: 1.0490 - val_acc: 0.7540\n",
      "Epoch 205/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.8117 - acc: 0.8694 - val_loss: 1.0458 - val_acc: 0.7580\n",
      "Epoch 206/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8134 - acc: 0.8654 - val_loss: 1.0477 - val_acc: 0.7540\n",
      "Epoch 207/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8157 - acc: 0.8629 - val_loss: 1.0527 - val_acc: 0.7590\n",
      "Epoch 208/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.8098 - acc: 0.8665 - val_loss: 1.0519 - val_acc: 0.7550\n",
      "Epoch 209/1000\n",
      "6500/6500 [==============================] - 0s 29us/step - loss: 0.8016 - acc: 0.8718 - val_loss: 1.0460 - val_acc: 0.7460\n",
      "Epoch 210/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.8012 - acc: 0.8731 - val_loss: 1.0475 - val_acc: 0.7480\n",
      "Epoch 211/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8022 - acc: 0.8760 - val_loss: 1.0608 - val_acc: 0.7460\n",
      "Epoch 212/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.8069 - acc: 0.8722 - val_loss: 1.0738 - val_acc: 0.7440\n",
      "Epoch 213/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.8074 - acc: 0.8726 - val_loss: 1.0727 - val_acc: 0.7500\n",
      "Epoch 214/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.8145 - acc: 0.8737 - val_loss: 1.0584 - val_acc: 0.7620\n",
      "Epoch 215/1000\n",
      "6500/6500 [==============================] - 0s 29us/step - loss: 0.8092 - acc: 0.8748 - val_loss: 1.0722 - val_acc: 0.7500\n",
      "Epoch 216/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.8089 - acc: 0.8751 - val_loss: 1.0660 - val_acc: 0.7550\n",
      "Epoch 217/1000\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 0.7992 - acc: 0.8752 - val_loss: 1.0767 - val_acc: 0.7460\n",
      "Epoch 218/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.8030 - acc: 0.8749 - val_loss: 1.0749 - val_acc: 0.7550\n",
      "Epoch 219/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.8086 - acc: 0.8714 - val_loss: 1.0614 - val_acc: 0.7590\n",
      "Epoch 220/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.7995 - acc: 0.8766 - val_loss: 1.0627 - val_acc: 0.7570\n",
      "Epoch 221/1000\n",
      "6500/6500 [==============================] - 0s 27us/step - loss: 0.8012 - acc: 0.8762 - val_loss: 1.0667 - val_acc: 0.7480\n",
      "Epoch 222/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.7906 - acc: 0.8848 - val_loss: 1.0516 - val_acc: 0.7580\n",
      "Epoch 223/1000\n",
      "6500/6500 [==============================] - 0s 45us/step - loss: 0.7958 - acc: 0.8771 - val_loss: 1.0749 - val_acc: 0.7530\n",
      "Epoch 224/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.7938 - acc: 0.8768 - val_loss: 1.0584 - val_acc: 0.7560\n",
      "Epoch 225/1000\n",
      "6500/6500 [==============================] - 0s 29us/step - loss: 0.7958 - acc: 0.8780 - val_loss: 1.0595 - val_acc: 0.7540\n",
      "Epoch 226/1000\n",
      "6500/6500 [==============================] - 0s 28us/step - loss: 0.7949 - acc: 0.8760 - val_loss: 1.0646 - val_acc: 0.7470\n",
      "Epoch 227/1000\n",
      "6500/6500 [==============================] - 0s 28us/step - loss: 0.7910 - acc: 0.8771 - val_loss: 1.0590 - val_acc: 0.7560\n",
      "Epoch 228/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.7937 - acc: 0.8814 - val_loss: 1.0747 - val_acc: 0.7490\n",
      "Epoch 229/1000\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 0.7855 - acc: 0.8849 - val_loss: 1.0644 - val_acc: 0.7500\n",
      "Epoch 230/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.7886 - acc: 0.8817 - val_loss: 1.0636 - val_acc: 0.7560\n",
      "Epoch 231/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.7929 - acc: 0.8835 - val_loss: 1.0753 - val_acc: 0.7450\n",
      "Epoch 232/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.7887 - acc: 0.8831 - val_loss: 1.0656 - val_acc: 0.7480\n",
      "Epoch 233/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.7981 - acc: 0.8789 - val_loss: 1.0795 - val_acc: 0.7470\n",
      "Epoch 234/1000\n",
      "6500/6500 [==============================] - 0s 28us/step - loss: 0.7913 - acc: 0.8820 - val_loss: 1.0737 - val_acc: 0.7610\n",
      "Epoch 235/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.7866 - acc: 0.8826 - val_loss: 1.0659 - val_acc: 0.7530\n",
      "Epoch 236/1000\n",
      "6500/6500 [==============================] - 0s 29us/step - loss: 0.7846 - acc: 0.8851 - val_loss: 1.0831 - val_acc: 0.7480\n",
      "Epoch 237/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6500/6500 [==============================] - 0s 44us/step - loss: 0.7834 - acc: 0.8832 - val_loss: 1.0766 - val_acc: 0.7530\n",
      "Epoch 238/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.7817 - acc: 0.8868 - val_loss: 1.0742 - val_acc: 0.7480\n",
      "Epoch 239/1000\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 0.7889 - acc: 0.8829 - val_loss: 1.0891 - val_acc: 0.7420\n",
      "Epoch 240/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.7825 - acc: 0.8897 - val_loss: 1.0714 - val_acc: 0.7610\n",
      "Epoch 241/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.7820 - acc: 0.8869 - val_loss: 1.0772 - val_acc: 0.7490\n",
      "Epoch 242/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.7887 - acc: 0.8837 - val_loss: 1.0795 - val_acc: 0.7520\n",
      "Epoch 243/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.7839 - acc: 0.8835 - val_loss: 1.0917 - val_acc: 0.7470\n",
      "Epoch 244/1000\n",
      "6500/6500 [==============================] - 0s 29us/step - loss: 0.7831 - acc: 0.8857 - val_loss: 1.0852 - val_acc: 0.7520\n",
      "Epoch 245/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.7801 - acc: 0.8885 - val_loss: 1.0705 - val_acc: 0.7620\n",
      "Epoch 246/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.7883 - acc: 0.8829 - val_loss: 1.0980 - val_acc: 0.7480\n",
      "Epoch 247/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.7840 - acc: 0.8858 - val_loss: 1.1044 - val_acc: 0.7480\n",
      "Epoch 248/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.7813 - acc: 0.8874 - val_loss: 1.0882 - val_acc: 0.7570\n",
      "Epoch 249/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.7767 - acc: 0.8895 - val_loss: 1.0792 - val_acc: 0.7550\n",
      "Epoch 250/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.7705 - acc: 0.8923 - val_loss: 1.0788 - val_acc: 0.7510\n",
      "Epoch 251/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.7805 - acc: 0.8843 - val_loss: 1.0753 - val_acc: 0.7560\n",
      "Epoch 252/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.7792 - acc: 0.8902 - val_loss: 1.0790 - val_acc: 0.7580\n",
      "Epoch 253/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.7769 - acc: 0.8935 - val_loss: 1.0875 - val_acc: 0.7510\n",
      "Epoch 254/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.7776 - acc: 0.8877 - val_loss: 1.0937 - val_acc: 0.7500\n",
      "Epoch 255/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.7809 - acc: 0.8908 - val_loss: 1.0835 - val_acc: 0.7600\n",
      "Epoch 256/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.7793 - acc: 0.8877 - val_loss: 1.0879 - val_acc: 0.7540\n",
      "Epoch 257/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.7769 - acc: 0.8925 - val_loss: 1.1055 - val_acc: 0.7510\n",
      "Epoch 258/1000\n",
      "6500/6500 [==============================] - 0s 28us/step - loss: 0.7703 - acc: 0.8920 - val_loss: 1.0913 - val_acc: 0.7500\n",
      "Epoch 259/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.7750 - acc: 0.8886 - val_loss: 1.0921 - val_acc: 0.7550\n",
      "Epoch 260/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.7711 - acc: 0.8931 - val_loss: 1.0935 - val_acc: 0.7510\n",
      "Epoch 261/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.7681 - acc: 0.8931 - val_loss: 1.0999 - val_acc: 0.7540\n",
      "Epoch 262/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.7767 - acc: 0.8858 - val_loss: 1.0972 - val_acc: 0.7540\n",
      "Epoch 263/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.7740 - acc: 0.8912 - val_loss: 1.0939 - val_acc: 0.7560\n",
      "Epoch 264/1000\n",
      "6500/6500 [==============================] - 0s 52us/step - loss: 0.7772 - acc: 0.8871 - val_loss: 1.1036 - val_acc: 0.7510\n",
      "Epoch 265/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.7694 - acc: 0.8952 - val_loss: 1.1276 - val_acc: 0.7430\n",
      "Epoch 266/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.7710 - acc: 0.8937 - val_loss: 1.1027 - val_acc: 0.7500\n",
      "Epoch 267/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.7676 - acc: 0.8975 - val_loss: 1.0814 - val_acc: 0.7650\n",
      "Epoch 268/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.7629 - acc: 0.9017 - val_loss: 1.0874 - val_acc: 0.7560\n",
      "Epoch 269/1000\n",
      "6500/6500 [==============================] - 0s 47us/step - loss: 0.7623 - acc: 0.8945 - val_loss: 1.0842 - val_acc: 0.7570\n",
      "Epoch 270/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.7597 - acc: 0.8991 - val_loss: 1.0900 - val_acc: 0.7510\n",
      "Epoch 271/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.7598 - acc: 0.9014 - val_loss: 1.1132 - val_acc: 0.7490\n",
      "Epoch 272/1000\n",
      "6500/6500 [==============================] - 0s 28us/step - loss: 0.7675 - acc: 0.8952 - val_loss: 1.0894 - val_acc: 0.7600\n",
      "Epoch 273/1000\n",
      "6500/6500 [==============================] - 0s 28us/step - loss: 0.7615 - acc: 0.8962 - val_loss: 1.0888 - val_acc: 0.7510\n",
      "Epoch 274/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.7618 - acc: 0.8963 - val_loss: 1.0849 - val_acc: 0.7570\n",
      "Epoch 275/1000\n",
      "6500/6500 [==============================] - 0s 44us/step - loss: 0.7764 - acc: 0.8868 - val_loss: 1.1266 - val_acc: 0.7510\n",
      "Epoch 276/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.7751 - acc: 0.8949 - val_loss: 1.1022 - val_acc: 0.7570\n",
      "Epoch 277/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.7671 - acc: 0.8954 - val_loss: 1.1002 - val_acc: 0.7480\n",
      "Epoch 278/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.7611 - acc: 0.8989 - val_loss: 1.0983 - val_acc: 0.7530\n",
      "Epoch 279/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.7625 - acc: 0.8980 - val_loss: 1.0954 - val_acc: 0.7540\n",
      "Epoch 280/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.7557 - acc: 0.9012 - val_loss: 1.1188 - val_acc: 0.7470\n",
      "Epoch 281/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.7617 - acc: 0.8955 - val_loss: 1.1058 - val_acc: 0.7560\n",
      "Epoch 282/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.7590 - acc: 0.9017 - val_loss: 1.1044 - val_acc: 0.7510\n",
      "Epoch 283/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.7523 - acc: 0.8975 - val_loss: 1.1001 - val_acc: 0.7530\n",
      "Epoch 284/1000\n",
      "6500/6500 [==============================] - 0s 55us/step - loss: 0.7605 - acc: 0.8960 - val_loss: 1.1017 - val_acc: 0.7580\n",
      "Epoch 285/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.7610 - acc: 0.8975 - val_loss: 1.1088 - val_acc: 0.7560\n",
      "Epoch 286/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.7590 - acc: 0.9012 - val_loss: 1.1200 - val_acc: 0.7480\n",
      "Epoch 287/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.7551 - acc: 0.8966 - val_loss: 1.1202 - val_acc: 0.7490\n",
      "Epoch 288/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.7571 - acc: 0.9012 - val_loss: 1.1072 - val_acc: 0.7570\n",
      "Epoch 289/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.7594 - acc: 0.8995 - val_loss: 1.1069 - val_acc: 0.7390\n",
      "Epoch 290/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.7595 - acc: 0.8974 - val_loss: 1.1040 - val_acc: 0.7560\n",
      "Epoch 291/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.7535 - acc: 0.9038 - val_loss: 1.1287 - val_acc: 0.7510\n",
      "Epoch 292/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.7619 - acc: 0.8937 - val_loss: 1.1077 - val_acc: 0.7560\n",
      "Epoch 293/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.7639 - acc: 0.8949 - val_loss: 1.1064 - val_acc: 0.7570\n",
      "Epoch 294/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.7658 - acc: 0.8966 - val_loss: 1.1241 - val_acc: 0.7480\n",
      "Epoch 295/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.7756 - acc: 0.8917 - val_loss: 1.1235 - val_acc: 0.7520\n",
      "Epoch 296/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.7558 - acc: 0.9060 - val_loss: 1.1110 - val_acc: 0.7470\n",
      "Epoch 297/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.7533 - acc: 0.9000 - val_loss: 1.1353 - val_acc: 0.7430\n",
      "Epoch 298/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.7482 - acc: 0.9029 - val_loss: 1.1100 - val_acc: 0.7500\n",
      "Epoch 299/1000\n",
      "6500/6500 [==============================] - 0s 29us/step - loss: 0.7551 - acc: 0.9022 - val_loss: 1.1132 - val_acc: 0.7590\n",
      "Epoch 300/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.7436 - acc: 0.9088 - val_loss: 1.1110 - val_acc: 0.7560\n",
      "Epoch 301/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.7468 - acc: 0.9018 - val_loss: 1.1021 - val_acc: 0.7540\n",
      "Epoch 302/1000\n",
      "6500/6500 [==============================] - 0s 28us/step - loss: 0.7474 - acc: 0.9032 - val_loss: 1.1227 - val_acc: 0.7490\n",
      "Epoch 303/1000\n",
      "6500/6500 [==============================] - 0s 29us/step - loss: 0.7388 - acc: 0.9106 - val_loss: 1.1146 - val_acc: 0.7530\n",
      "Epoch 304/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.7437 - acc: 0.9068 - val_loss: 1.1200 - val_acc: 0.7550\n",
      "Epoch 305/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.7473 - acc: 0.9040 - val_loss: 1.1190 - val_acc: 0.7530\n",
      "Epoch 306/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.7514 - acc: 0.9052 - val_loss: 1.1246 - val_acc: 0.7470\n",
      "Epoch 307/1000\n",
      "6500/6500 [==============================] - 0s 29us/step - loss: 0.7441 - acc: 0.9071 - val_loss: 1.1228 - val_acc: 0.7500\n",
      "Epoch 308/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.7453 - acc: 0.9066 - val_loss: 1.1336 - val_acc: 0.7520\n",
      "Epoch 309/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.7551 - acc: 0.9037 - val_loss: 1.1359 - val_acc: 0.7560\n",
      "Epoch 310/1000\n",
      "6500/6500 [==============================] - 0s 46us/step - loss: 0.7524 - acc: 0.9065 - val_loss: 1.1319 - val_acc: 0.7550\n",
      "Epoch 311/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.7403 - acc: 0.9100 - val_loss: 1.1258 - val_acc: 0.7590\n",
      "Epoch 312/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.7346 - acc: 0.9111 - val_loss: 1.1220 - val_acc: 0.7470\n",
      "Epoch 313/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.7340 - acc: 0.9123 - val_loss: 1.1254 - val_acc: 0.7400\n",
      "Epoch 314/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.7483 - acc: 0.9071 - val_loss: 1.1465 - val_acc: 0.7470\n",
      "Epoch 315/1000\n",
      "6500/6500 [==============================] - 0s 47us/step - loss: 0.7462 - acc: 0.9043 - val_loss: 1.1286 - val_acc: 0.7470\n",
      "Epoch 316/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.7425 - acc: 0.9080 - val_loss: 1.1147 - val_acc: 0.7620\n",
      "Epoch 317/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.7367 - acc: 0.9095 - val_loss: 1.1234 - val_acc: 0.7510\n",
      "Epoch 318/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.7366 - acc: 0.9117 - val_loss: 1.1451 - val_acc: 0.7520\n",
      "Epoch 319/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 0.7408 - acc: 0.9071 - val_loss: 1.1251 - val_acc: 0.7470\n",
      "Epoch 320/1000\n",
      "6500/6500 [==============================] - 0s 47us/step - loss: 0.7311 - acc: 0.9148 - val_loss: 1.1211 - val_acc: 0.7520\n",
      "Epoch 321/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.7318 - acc: 0.9182 - val_loss: 1.1180 - val_acc: 0.7560\n",
      "Epoch 322/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.7296 - acc: 0.9158 - val_loss: 1.1355 - val_acc: 0.7380\n",
      "Epoch 323/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.7393 - acc: 0.9095 - val_loss: 1.1340 - val_acc: 0.7500\n",
      "Epoch 324/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.7408 - acc: 0.9091 - val_loss: 1.1247 - val_acc: 0.7600\n",
      "Epoch 325/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.7503 - acc: 0.9051 - val_loss: 1.1514 - val_acc: 0.7430\n",
      "Epoch 326/1000\n",
      "6500/6500 [==============================] - 0s 29us/step - loss: 0.7391 - acc: 0.9085 - val_loss: 1.1236 - val_acc: 0.7470\n",
      "Epoch 327/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.7359 - acc: 0.9125 - val_loss: 1.1353 - val_acc: 0.7490\n",
      "Epoch 328/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.7383 - acc: 0.9125 - val_loss: 1.1290 - val_acc: 0.7570\n",
      "Epoch 329/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.7327 - acc: 0.9142 - val_loss: 1.1343 - val_acc: 0.7450\n",
      "Epoch 330/1000\n",
      "6500/6500 [==============================] - 0s 49us/step - loss: 0.7312 - acc: 0.9143 - val_loss: 1.1475 - val_acc: 0.7400\n",
      "Epoch 331/1000\n",
      "6500/6500 [==============================] - 0s 45us/step - loss: 0.7375 - acc: 0.9091 - val_loss: 1.1523 - val_acc: 0.7420\n",
      "Epoch 332/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.7337 - acc: 0.9114 - val_loss: 1.1323 - val_acc: 0.7620\n",
      "Epoch 333/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.7325 - acc: 0.9155 - val_loss: 1.1378 - val_acc: 0.7450\n",
      "Epoch 334/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.7312 - acc: 0.9123 - val_loss: 1.1534 - val_acc: 0.7510\n",
      "Epoch 335/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.7324 - acc: 0.9143 - val_loss: 1.1294 - val_acc: 0.7590\n",
      "Epoch 336/1000\n",
      "6500/6500 [==============================] - 0s 29us/step - loss: 0.7309 - acc: 0.9122 - val_loss: 1.1469 - val_acc: 0.7490\n",
      "Epoch 337/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.7330 - acc: 0.9112 - val_loss: 1.1195 - val_acc: 0.7580\n",
      "Epoch 338/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.7339 - acc: 0.9120 - val_loss: 1.1341 - val_acc: 0.7560\n",
      "Epoch 339/1000\n",
      "6500/6500 [==============================] - 0s 28us/step - loss: 0.7327 - acc: 0.9162 - val_loss: 1.1436 - val_acc: 0.7500\n",
      "Epoch 340/1000\n",
      "6500/6500 [==============================] - 0s 29us/step - loss: 0.7321 - acc: 0.9148 - val_loss: 1.1738 - val_acc: 0.7440\n",
      "Epoch 341/1000\n",
      "6500/6500 [==============================] - 0s 29us/step - loss: 0.7314 - acc: 0.9146 - val_loss: 1.1342 - val_acc: 0.7550\n",
      "Epoch 342/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.7259 - acc: 0.9163 - val_loss: 1.1537 - val_acc: 0.7510\n",
      "Epoch 343/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.7279 - acc: 0.9157 - val_loss: 1.1435 - val_acc: 0.7550\n",
      "Epoch 344/1000\n",
      "6500/6500 [==============================] - 0s 29us/step - loss: 0.7214 - acc: 0.9177 - val_loss: 1.1306 - val_acc: 0.7610\n",
      "Epoch 345/1000\n",
      "6500/6500 [==============================] - 0s 29us/step - loss: 0.7242 - acc: 0.9143 - val_loss: 1.1394 - val_acc: 0.7500\n",
      "Epoch 346/1000\n",
      "6500/6500 [==============================] - 0s 29us/step - loss: 0.7334 - acc: 0.9117 - val_loss: 1.1394 - val_acc: 0.7550\n",
      "Epoch 347/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.7297 - acc: 0.9183 - val_loss: 1.1786 - val_acc: 0.7410\n",
      "Epoch 348/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.7297 - acc: 0.9151 - val_loss: 1.1424 - val_acc: 0.7600\n",
      "Epoch 349/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.7310 - acc: 0.9128 - val_loss: 1.1513 - val_acc: 0.7430\n",
      "Epoch 350/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.7364 - acc: 0.9117 - val_loss: 1.1489 - val_acc: 0.7550\n",
      "Epoch 351/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.7215 - acc: 0.9188 - val_loss: 1.1395 - val_acc: 0.7530\n",
      "Epoch 352/1000\n",
      "6500/6500 [==============================] - 0s 44us/step - loss: 0.7228 - acc: 0.9183 - val_loss: 1.1592 - val_acc: 0.7550\n",
      "Epoch 353/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.7281 - acc: 0.9155 - val_loss: 1.1602 - val_acc: 0.7480\n",
      "Epoch 354/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.7203 - acc: 0.9185 - val_loss: 1.1528 - val_acc: 0.7500\n",
      "Epoch 355/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.7214 - acc: 0.9197 - val_loss: 1.1458 - val_acc: 0.7490\n",
      "Epoch 356/1000\n",
      "6500/6500 [==============================] - 0s 44us/step - loss: 0.7254 - acc: 0.9120 - val_loss: 1.1518 - val_acc: 0.7510\n",
      "Epoch 357/1000\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 0.7210 - acc: 0.9212 - val_loss: 1.1495 - val_acc: 0.7510\n",
      "Epoch 358/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.7284 - acc: 0.9149 - val_loss: 1.1583 - val_acc: 0.7430\n",
      "Epoch 359/1000\n",
      "6500/6500 [==============================] - 0s 29us/step - loss: 0.7139 - acc: 0.9231 - val_loss: 1.1568 - val_acc: 0.7410\n",
      "Epoch 360/1000\n",
      "6500/6500 [==============================] - 0s 29us/step - loss: 0.7156 - acc: 0.9257 - val_loss: 1.1398 - val_acc: 0.7480\n",
      "Epoch 361/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.7165 - acc: 0.9206 - val_loss: 1.1799 - val_acc: 0.7480\n",
      "Epoch 362/1000\n",
      "6500/6500 [==============================] - 0s 45us/step - loss: 0.7385 - acc: 0.9040 - val_loss: 1.1577 - val_acc: 0.7570\n",
      "Epoch 363/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.7280 - acc: 0.9145 - val_loss: 1.1536 - val_acc: 0.7560\n",
      "Epoch 364/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.7241 - acc: 0.9215 - val_loss: 1.1699 - val_acc: 0.7460\n",
      "Epoch 365/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.7138 - acc: 0.9255 - val_loss: 1.1443 - val_acc: 0.7530\n",
      "Epoch 366/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.7105 - acc: 0.9249 - val_loss: 1.1582 - val_acc: 0.7360\n",
      "Epoch 367/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.7167 - acc: 0.9177 - val_loss: 1.1507 - val_acc: 0.7480\n",
      "Epoch 368/1000\n",
      "6500/6500 [==============================] - 0s 29us/step - loss: 0.7189 - acc: 0.9186 - val_loss: 1.1496 - val_acc: 0.7570\n",
      "Epoch 369/1000\n",
      "6500/6500 [==============================] - 0s 28us/step - loss: 0.7192 - acc: 0.9238 - val_loss: 1.1739 - val_acc: 0.7320\n",
      "Epoch 370/1000\n",
      "6500/6500 [==============================] - 0s 29us/step - loss: 0.7163 - acc: 0.9232 - val_loss: 1.1502 - val_acc: 0.7530\n",
      "Epoch 371/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.7109 - acc: 0.9269 - val_loss: 1.1503 - val_acc: 0.7450\n",
      "Epoch 372/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.7105 - acc: 0.9265 - val_loss: 1.1441 - val_acc: 0.7540\n",
      "Epoch 373/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.7133 - acc: 0.9217 - val_loss: 1.1448 - val_acc: 0.7560\n",
      "Epoch 374/1000\n",
      "6500/6500 [==============================] - 0s 26us/step - loss: 0.7118 - acc: 0.9235 - val_loss: 1.1588 - val_acc: 0.7400\n",
      "Epoch 375/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.7107 - acc: 0.9214 - val_loss: 1.1480 - val_acc: 0.7570\n",
      "Epoch 376/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.7219 - acc: 0.9177 - val_loss: 1.1495 - val_acc: 0.7520\n",
      "Epoch 377/1000\n",
      "6500/6500 [==============================] - 0s 48us/step - loss: 0.7158 - acc: 0.9203 - val_loss: 1.1878 - val_acc: 0.7400\n",
      "Epoch 378/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 0.7198 - acc: 0.9225 - val_loss: 1.1574 - val_acc: 0.7460\n",
      "Epoch 379/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.7110 - acc: 0.9215 - val_loss: 1.1628 - val_acc: 0.7470\n",
      "Epoch 380/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.7089 - acc: 0.9238 - val_loss: 1.1849 - val_acc: 0.7450\n",
      "Epoch 381/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.7165 - acc: 0.9205 - val_loss: 1.1630 - val_acc: 0.7530\n",
      "Epoch 382/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.7162 - acc: 0.9192 - val_loss: 1.1939 - val_acc: 0.7430\n",
      "Epoch 383/1000\n",
      "6500/6500 [==============================] - 0s 29us/step - loss: 0.7300 - acc: 0.9111 - val_loss: 1.1634 - val_acc: 0.7500\n",
      "Epoch 384/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.7202 - acc: 0.9194 - val_loss: 1.1709 - val_acc: 0.7460\n",
      "Epoch 385/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.7235 - acc: 0.9191 - val_loss: 1.1947 - val_acc: 0.7440\n",
      "Epoch 386/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.7244 - acc: 0.9168 - val_loss: 1.1799 - val_acc: 0.7570\n",
      "Epoch 387/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.7125 - acc: 0.9263 - val_loss: 1.1569 - val_acc: 0.7490\n",
      "Epoch 388/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.7062 - acc: 0.9243 - val_loss: 1.1908 - val_acc: 0.7460\n",
      "Epoch 389/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.7097 - acc: 0.9252 - val_loss: 1.1614 - val_acc: 0.7440\n",
      "Epoch 390/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.7073 - acc: 0.9248 - val_loss: 1.1595 - val_acc: 0.7570\n",
      "Epoch 391/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.7130 - acc: 0.9240 - val_loss: 1.1715 - val_acc: 0.7490\n",
      "Epoch 392/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.7184 - acc: 0.9208 - val_loss: 1.1716 - val_acc: 0.7440\n",
      "Epoch 393/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.6991 - acc: 0.9298 - val_loss: 1.1712 - val_acc: 0.7450\n",
      "Epoch 394/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.7053 - acc: 0.9225 - val_loss: 1.1792 - val_acc: 0.7450\n",
      "Epoch 395/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.7128 - acc: 0.9183 - val_loss: 1.1760 - val_acc: 0.7570\n",
      "Epoch 396/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.7202 - acc: 0.9174 - val_loss: 1.1664 - val_acc: 0.7480\n",
      "Epoch 397/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.7100 - acc: 0.9229 - val_loss: 1.1672 - val_acc: 0.7520\n",
      "Epoch 398/1000\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 0.7141 - acc: 0.9251 - val_loss: 1.1896 - val_acc: 0.7370\n",
      "Epoch 399/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.7106 - acc: 0.9258 - val_loss: 1.1890 - val_acc: 0.7380\n",
      "Epoch 400/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.7043 - acc: 0.9268 - val_loss: 1.1778 - val_acc: 0.7460\n",
      "Epoch 401/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.7001 - acc: 0.9289 - val_loss: 1.1746 - val_acc: 0.7520\n",
      "Epoch 402/1000\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 0.7129 - acc: 0.9215 - val_loss: 1.1782 - val_acc: 0.7570\n",
      "Epoch 403/1000\n",
      "6500/6500 [==============================] - 0s 45us/step - loss: 0.7135 - acc: 0.9254 - val_loss: 1.1665 - val_acc: 0.7500\n",
      "Epoch 404/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.7137 - acc: 0.9225 - val_loss: 1.1980 - val_acc: 0.7460\n",
      "Epoch 405/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.7171 - acc: 0.9211 - val_loss: 1.1875 - val_acc: 0.7490\n",
      "Epoch 406/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.7097 - acc: 0.9274 - val_loss: 1.1769 - val_acc: 0.7460\n",
      "Epoch 407/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.7066 - acc: 0.9260 - val_loss: 1.1828 - val_acc: 0.7430\n",
      "Epoch 408/1000\n",
      "6500/6500 [==============================] - 0s 45us/step - loss: 0.7116 - acc: 0.9254 - val_loss: 1.1687 - val_acc: 0.7440\n",
      "Epoch 409/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.7131 - acc: 0.9271 - val_loss: 1.1926 - val_acc: 0.7480\n",
      "Epoch 410/1000\n",
      "6500/6500 [==============================] - 0s 29us/step - loss: 0.7185 - acc: 0.9237 - val_loss: 1.1932 - val_acc: 0.7530\n",
      "Epoch 411/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.7098 - acc: 0.9265 - val_loss: 1.1812 - val_acc: 0.7470\n",
      "Epoch 412/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.7043 - acc: 0.9286 - val_loss: 1.1731 - val_acc: 0.7430\n",
      "Epoch 413/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6982 - acc: 0.9305 - val_loss: 1.2144 - val_acc: 0.7470\n",
      "Epoch 414/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.7076 - acc: 0.9249 - val_loss: 1.1820 - val_acc: 0.7510\n",
      "Epoch 415/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.7055 - acc: 0.9249 - val_loss: 1.2050 - val_acc: 0.7490\n",
      "Epoch 416/1000\n",
      "6500/6500 [==============================] - 0s 28us/step - loss: 0.7091 - acc: 0.9222 - val_loss: 1.1682 - val_acc: 0.7620\n",
      "Epoch 417/1000\n",
      "6500/6500 [==============================] - 0s 28us/step - loss: 0.6977 - acc: 0.9326 - val_loss: 1.2012 - val_acc: 0.7370\n",
      "Epoch 418/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.6979 - acc: 0.9305 - val_loss: 1.1736 - val_acc: 0.7500\n",
      "Epoch 419/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.7032 - acc: 0.9248 - val_loss: 1.1765 - val_acc: 0.7490\n",
      "Epoch 420/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.7023 - acc: 0.9262 - val_loss: 1.2090 - val_acc: 0.7330\n",
      "Epoch 421/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6967 - acc: 0.9322 - val_loss: 1.2119 - val_acc: 0.7440\n",
      "Epoch 422/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.6981 - acc: 0.9271 - val_loss: 1.1740 - val_acc: 0.7400\n",
      "Epoch 423/1000\n",
      "6500/6500 [==============================] - 0s 49us/step - loss: 0.6969 - acc: 0.9260 - val_loss: 1.1769 - val_acc: 0.7490\n",
      "Epoch 424/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 0.6954 - acc: 0.9308 - val_loss: 1.1850 - val_acc: 0.7480\n",
      "Epoch 425/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.7036 - acc: 0.9242 - val_loss: 1.2258 - val_acc: 0.7470\n",
      "Epoch 426/1000\n",
      "6500/6500 [==============================] - 0s 28us/step - loss: 0.7192 - acc: 0.9186 - val_loss: 1.2136 - val_acc: 0.7430\n",
      "Epoch 427/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.6991 - acc: 0.9312 - val_loss: 1.2009 - val_acc: 0.7430\n",
      "Epoch 428/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.6996 - acc: 0.9294 - val_loss: 1.1811 - val_acc: 0.7450\n",
      "Epoch 429/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.7044 - acc: 0.9251 - val_loss: 1.2641 - val_acc: 0.7280\n",
      "Epoch 430/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.7179 - acc: 0.9165 - val_loss: 1.1966 - val_acc: 0.7520\n",
      "Epoch 431/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.7086 - acc: 0.9271 - val_loss: 1.2041 - val_acc: 0.7480\n",
      "Epoch 432/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6994 - acc: 0.9302 - val_loss: 1.1991 - val_acc: 0.7450\n",
      "Epoch 433/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.6975 - acc: 0.9278 - val_loss: 1.2002 - val_acc: 0.7450\n",
      "Epoch 434/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6987 - acc: 0.9275 - val_loss: 1.1973 - val_acc: 0.7630\n",
      "Epoch 435/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.7037 - acc: 0.9282 - val_loss: 1.2104 - val_acc: 0.7480\n",
      "Epoch 436/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6906 - acc: 0.9322 - val_loss: 1.1800 - val_acc: 0.7540\n",
      "Epoch 437/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.6892 - acc: 0.9328 - val_loss: 1.1884 - val_acc: 0.7490\n",
      "Epoch 438/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6881 - acc: 0.9300 - val_loss: 1.1863 - val_acc: 0.7520\n",
      "Epoch 439/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6898 - acc: 0.9322 - val_loss: 1.1771 - val_acc: 0.7460\n",
      "Epoch 440/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6945 - acc: 0.9289 - val_loss: 1.2230 - val_acc: 0.7420\n",
      "Epoch 441/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.7016 - acc: 0.9294 - val_loss: 1.2158 - val_acc: 0.7430\n",
      "Epoch 442/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.6982 - acc: 0.9322 - val_loss: 1.2142 - val_acc: 0.7430\n",
      "Epoch 443/1000\n",
      "6500/6500 [==============================] - 0s 27us/step - loss: 0.6949 - acc: 0.9297 - val_loss: 1.1968 - val_acc: 0.7520\n",
      "Epoch 444/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.7051 - acc: 0.9258 - val_loss: 1.2025 - val_acc: 0.7400\n",
      "Epoch 445/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.7003 - acc: 0.9289 - val_loss: 1.2012 - val_acc: 0.7450\n",
      "Epoch 446/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.6922 - acc: 0.9354 - val_loss: 1.1885 - val_acc: 0.7480\n",
      "Epoch 447/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.6903 - acc: 0.9332 - val_loss: 1.1891 - val_acc: 0.7460\n",
      "Epoch 448/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6859 - acc: 0.9346 - val_loss: 1.1929 - val_acc: 0.7470\n",
      "Epoch 449/1000\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 0.6891 - acc: 0.9285 - val_loss: 1.1800 - val_acc: 0.7500\n",
      "Epoch 450/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 0.6808 - acc: 0.9368 - val_loss: 1.1818 - val_acc: 0.7420\n",
      "Epoch 451/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6831 - acc: 0.9363 - val_loss: 1.2040 - val_acc: 0.7390\n",
      "Epoch 452/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.7135 - acc: 0.9118 - val_loss: 1.2243 - val_acc: 0.7560\n",
      "Epoch 453/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.7263 - acc: 0.9186 - val_loss: 1.2111 - val_acc: 0.7550\n",
      "Epoch 454/1000\n",
      "6500/6500 [==============================] - 0s 46us/step - loss: 0.7051 - acc: 0.9291 - val_loss: 1.2032 - val_acc: 0.7550\n",
      "Epoch 455/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.6819 - acc: 0.9363 - val_loss: 1.2024 - val_acc: 0.7540\n",
      "Epoch 456/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.6979 - acc: 0.9263 - val_loss: 1.2359 - val_acc: 0.7440\n",
      "Epoch 457/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.7147 - acc: 0.9229 - val_loss: 1.2338 - val_acc: 0.7450\n",
      "Epoch 458/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.7011 - acc: 0.9309 - val_loss: 1.2261 - val_acc: 0.7500\n",
      "Epoch 459/1000\n",
      "6500/6500 [==============================] - 0s 45us/step - loss: 0.6966 - acc: 0.9317 - val_loss: 1.1953 - val_acc: 0.7520\n",
      "Epoch 460/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.6858 - acc: 0.9352 - val_loss: 1.2042 - val_acc: 0.7430\n",
      "Epoch 461/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6786 - acc: 0.9385 - val_loss: 1.1891 - val_acc: 0.7500\n",
      "Epoch 462/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.6897 - acc: 0.9292 - val_loss: 1.1987 - val_acc: 0.7510\n",
      "Epoch 463/1000\n",
      "6500/6500 [==============================] - 0s 29us/step - loss: 0.6901 - acc: 0.9306 - val_loss: 1.1995 - val_acc: 0.7470\n",
      "Epoch 464/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.6818 - acc: 0.9338 - val_loss: 1.2074 - val_acc: 0.7460\n",
      "Epoch 465/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6894 - acc: 0.9323 - val_loss: 1.2111 - val_acc: 0.7430\n",
      "Epoch 466/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.6844 - acc: 0.9358 - val_loss: 1.1946 - val_acc: 0.7550\n",
      "Epoch 467/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.6779 - acc: 0.9385 - val_loss: 1.2073 - val_acc: 0.7530\n",
      "Epoch 468/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.6837 - acc: 0.9329 - val_loss: 1.1970 - val_acc: 0.7480\n",
      "Epoch 469/1000\n",
      "6500/6500 [==============================] - 0s 51us/step - loss: 0.7161 - acc: 0.9115 - val_loss: 1.2620 - val_acc: 0.7300\n",
      "Epoch 470/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.7061 - acc: 0.9248 - val_loss: 1.2259 - val_acc: 0.7460\n",
      "Epoch 471/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6820 - acc: 0.9378 - val_loss: 1.2094 - val_acc: 0.7530\n",
      "Epoch 472/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6850 - acc: 0.9314 - val_loss: 1.2060 - val_acc: 0.7500\n",
      "Epoch 473/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.6778 - acc: 0.9346 - val_loss: 1.2032 - val_acc: 0.7490\n",
      "Epoch 474/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6762 - acc: 0.9398 - val_loss: 1.1898 - val_acc: 0.7500\n",
      "Epoch 475/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6851 - acc: 0.9340 - val_loss: 1.2212 - val_acc: 0.7520\n",
      "Epoch 476/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6954 - acc: 0.9294 - val_loss: 1.2167 - val_acc: 0.7390\n",
      "Epoch 477/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6806 - acc: 0.9334 - val_loss: 1.2217 - val_acc: 0.7410\n",
      "Epoch 478/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6786 - acc: 0.9385 - val_loss: 1.2159 - val_acc: 0.7440\n",
      "Epoch 479/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.6836 - acc: 0.9335 - val_loss: 1.2117 - val_acc: 0.7390\n",
      "Epoch 480/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6878 - acc: 0.9355 - val_loss: 1.2291 - val_acc: 0.7460\n",
      "Epoch 481/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.6864 - acc: 0.9335 - val_loss: 1.2078 - val_acc: 0.7460\n",
      "Epoch 482/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.6953 - acc: 0.9351 - val_loss: 1.2153 - val_acc: 0.7450\n",
      "Epoch 483/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.6813 - acc: 0.9383 - val_loss: 1.2168 - val_acc: 0.7460\n",
      "Epoch 484/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.6737 - acc: 0.9372 - val_loss: 1.2288 - val_acc: 0.7380\n",
      "Epoch 485/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.6783 - acc: 0.9366 - val_loss: 1.1904 - val_acc: 0.7500\n",
      "Epoch 486/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.6804 - acc: 0.9360 - val_loss: 1.2198 - val_acc: 0.7400\n",
      "Epoch 487/1000\n",
      "6500/6500 [==============================] - 0s 29us/step - loss: 0.6820 - acc: 0.9386 - val_loss: 1.2254 - val_acc: 0.7390\n",
      "Epoch 488/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6721 - acc: 0.9400 - val_loss: 1.1990 - val_acc: 0.7420\n",
      "Epoch 489/1000\n",
      "6500/6500 [==============================] - 0s 29us/step - loss: 0.6721 - acc: 0.9397 - val_loss: 1.2141 - val_acc: 0.7530\n",
      "Epoch 490/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.6813 - acc: 0.9298 - val_loss: 1.2003 - val_acc: 0.7460\n",
      "Epoch 491/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.6805 - acc: 0.9349 - val_loss: 1.1913 - val_acc: 0.7540\n",
      "Epoch 492/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.6733 - acc: 0.9351 - val_loss: 1.2041 - val_acc: 0.7430\n",
      "Epoch 493/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6719 - acc: 0.9388 - val_loss: 1.2038 - val_acc: 0.7490\n",
      "Epoch 494/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6694 - acc: 0.9418 - val_loss: 1.2387 - val_acc: 0.7370\n",
      "Epoch 495/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 0.7071 - acc: 0.9191 - val_loss: 1.2435 - val_acc: 0.7420\n",
      "Epoch 496/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 0.6960 - acc: 0.9288 - val_loss: 1.2593 - val_acc: 0.7350\n",
      "Epoch 497/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.6927 - acc: 0.9306 - val_loss: 1.2606 - val_acc: 0.7370\n",
      "Epoch 498/1000\n",
      "6500/6500 [==============================] - 0s 29us/step - loss: 0.6851 - acc: 0.9300 - val_loss: 1.2351 - val_acc: 0.7410\n",
      "Epoch 499/1000\n",
      "6500/6500 [==============================] - 0s 29us/step - loss: 0.6802 - acc: 0.9349 - val_loss: 1.2509 - val_acc: 0.7350\n",
      "Epoch 500/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.6784 - acc: 0.9345 - val_loss: 1.2425 - val_acc: 0.7420\n",
      "Epoch 501/1000\n",
      "6500/6500 [==============================] - 0s 45us/step - loss: 0.6904 - acc: 0.9283 - val_loss: 1.2297 - val_acc: 0.7430\n",
      "Epoch 502/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6837 - acc: 0.9337 - val_loss: 1.2193 - val_acc: 0.7520\n",
      "Epoch 503/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.6716 - acc: 0.9348 - val_loss: 1.2326 - val_acc: 0.7450\n",
      "Epoch 504/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6706 - acc: 0.9391 - val_loss: 1.2195 - val_acc: 0.7420\n",
      "Epoch 505/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.6798 - acc: 0.9351 - val_loss: 1.2261 - val_acc: 0.7450\n",
      "Epoch 506/1000\n",
      "6500/6500 [==============================] - 0s 46us/step - loss: 0.6881 - acc: 0.9335 - val_loss: 1.2297 - val_acc: 0.7410\n",
      "Epoch 507/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 0.6878 - acc: 0.9329 - val_loss: 1.2200 - val_acc: 0.7490\n",
      "Epoch 508/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.6723 - acc: 0.9380 - val_loss: 1.2302 - val_acc: 0.7480\n",
      "Epoch 509/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.6706 - acc: 0.9369 - val_loss: 1.2452 - val_acc: 0.7350\n",
      "Epoch 510/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.6824 - acc: 0.9318 - val_loss: 1.2027 - val_acc: 0.7510\n",
      "Epoch 511/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6885 - acc: 0.9320 - val_loss: 1.2563 - val_acc: 0.7400\n",
      "Epoch 512/1000\n",
      "6500/6500 [==============================] - 0s 29us/step - loss: 0.6968 - acc: 0.9297 - val_loss: 1.2353 - val_acc: 0.7480\n",
      "Epoch 513/1000\n",
      "6500/6500 [==============================] - 0s 29us/step - loss: 0.6890 - acc: 0.9348 - val_loss: 1.2206 - val_acc: 0.7540\n",
      "Epoch 514/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.6798 - acc: 0.9368 - val_loss: 1.2318 - val_acc: 0.7400\n",
      "Epoch 515/1000\n",
      "6500/6500 [==============================] - 0s 50us/step - loss: 0.6704 - acc: 0.9400 - val_loss: 1.2260 - val_acc: 0.7410\n",
      "Epoch 516/1000\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 0.6656 - acc: 0.9420 - val_loss: 1.2135 - val_acc: 0.7470\n",
      "Epoch 517/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6701 - acc: 0.9371 - val_loss: 1.2247 - val_acc: 0.7460\n",
      "Epoch 518/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6617 - acc: 0.9426 - val_loss: 1.2114 - val_acc: 0.7450\n",
      "Epoch 519/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.6612 - acc: 0.9431 - val_loss: 1.2263 - val_acc: 0.7390\n",
      "Epoch 520/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6588 - acc: 0.9425 - val_loss: 1.1925 - val_acc: 0.7490\n",
      "Epoch 521/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.6765 - acc: 0.9335 - val_loss: 1.2062 - val_acc: 0.7550\n",
      "Epoch 522/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.6697 - acc: 0.9368 - val_loss: 1.2229 - val_acc: 0.7510\n",
      "Epoch 523/1000\n",
      "6500/6500 [==============================] - 0s 29us/step - loss: 0.6760 - acc: 0.9331 - val_loss: 1.2118 - val_acc: 0.7500\n",
      "Epoch 524/1000\n",
      "6500/6500 [==============================] - 0s 29us/step - loss: 0.6831 - acc: 0.9343 - val_loss: 1.2278 - val_acc: 0.7430\n",
      "Epoch 525/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.6700 - acc: 0.9360 - val_loss: 1.2271 - val_acc: 0.7490\n",
      "Epoch 526/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6753 - acc: 0.9355 - val_loss: 1.2229 - val_acc: 0.7510\n",
      "Epoch 527/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.6672 - acc: 0.9408 - val_loss: 1.2393 - val_acc: 0.7520\n",
      "Epoch 528/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6749 - acc: 0.9355 - val_loss: 1.2372 - val_acc: 0.7490\n",
      "Epoch 529/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6734 - acc: 0.9371 - val_loss: 1.2032 - val_acc: 0.7520\n",
      "Epoch 530/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.6659 - acc: 0.9400 - val_loss: 1.2184 - val_acc: 0.7550\n",
      "Epoch 531/1000\n",
      "6500/6500 [==============================] - 0s 28us/step - loss: 0.6811 - acc: 0.9318 - val_loss: 1.2320 - val_acc: 0.7560\n",
      "Epoch 532/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6790 - acc: 0.9320 - val_loss: 1.2510 - val_acc: 0.7600\n",
      "Epoch 533/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.6818 - acc: 0.9345 - val_loss: 1.2322 - val_acc: 0.7470\n",
      "Epoch 534/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.6788 - acc: 0.9331 - val_loss: 1.2304 - val_acc: 0.7510\n",
      "Epoch 535/1000\n",
      "6500/6500 [==============================] - 0s 28us/step - loss: 0.6667 - acc: 0.9443 - val_loss: 1.2339 - val_acc: 0.7500\n",
      "Epoch 536/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.6711 - acc: 0.9352 - val_loss: 1.2484 - val_acc: 0.7430\n",
      "Epoch 537/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6639 - acc: 0.9425 - val_loss: 1.2610 - val_acc: 0.7360\n",
      "Epoch 538/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.6704 - acc: 0.9391 - val_loss: 1.2289 - val_acc: 0.7570\n",
      "Epoch 539/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6633 - acc: 0.9426 - val_loss: 1.2263 - val_acc: 0.7460\n",
      "Epoch 540/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.6610 - acc: 0.9420 - val_loss: 1.2216 - val_acc: 0.7470\n",
      "Epoch 541/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6556 - acc: 0.9462 - val_loss: 1.2303 - val_acc: 0.7430\n",
      "Epoch 542/1000\n",
      "6500/6500 [==============================] - 0s 45us/step - loss: 0.6597 - acc: 0.9398 - val_loss: 1.2286 - val_acc: 0.7520\n",
      "Epoch 543/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.6623 - acc: 0.9372 - val_loss: 1.2294 - val_acc: 0.7460\n",
      "Epoch 544/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6755 - acc: 0.9354 - val_loss: 1.2306 - val_acc: 0.7450\n",
      "Epoch 545/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6821 - acc: 0.9302 - val_loss: 1.2326 - val_acc: 0.7480\n",
      "Epoch 546/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.6697 - acc: 0.9402 - val_loss: 1.2429 - val_acc: 0.7440\n",
      "Epoch 547/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 0.6689 - acc: 0.9383 - val_loss: 1.2298 - val_acc: 0.7600\n",
      "Epoch 548/1000\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 0.6779 - acc: 0.9357 - val_loss: 1.2446 - val_acc: 0.7480\n",
      "Epoch 549/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6765 - acc: 0.9357 - val_loss: 1.2460 - val_acc: 0.7520\n",
      "Epoch 550/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6771 - acc: 0.9363 - val_loss: 1.2513 - val_acc: 0.7400\n",
      "Epoch 551/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.6631 - acc: 0.9414 - val_loss: 1.2204 - val_acc: 0.7510\n",
      "Epoch 552/1000\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 0.6548 - acc: 0.9440 - val_loss: 1.2267 - val_acc: 0.7430\n",
      "Epoch 553/1000\n",
      "6500/6500 [==============================] - 0s 46us/step - loss: 0.6630 - acc: 0.9405 - val_loss: 1.2266 - val_acc: 0.7470\n",
      "Epoch 554/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.6653 - acc: 0.9386 - val_loss: 1.2211 - val_acc: 0.7470\n",
      "Epoch 555/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.6714 - acc: 0.9348 - val_loss: 1.2449 - val_acc: 0.7450\n",
      "Epoch 556/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.6703 - acc: 0.9426 - val_loss: 1.2258 - val_acc: 0.7550\n",
      "Epoch 557/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.6582 - acc: 0.9452 - val_loss: 1.2496 - val_acc: 0.7460\n",
      "Epoch 558/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6631 - acc: 0.9423 - val_loss: 1.2362 - val_acc: 0.7490\n",
      "Epoch 559/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.6658 - acc: 0.9385 - val_loss: 1.2446 - val_acc: 0.7460\n",
      "Epoch 560/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.6758 - acc: 0.9346 - val_loss: 1.2353 - val_acc: 0.7520\n",
      "Epoch 561/1000\n",
      "6500/6500 [==============================] - 0s 49us/step - loss: 0.6704 - acc: 0.9392 - val_loss: 1.2568 - val_acc: 0.7410\n",
      "Epoch 562/1000\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 0.6697 - acc: 0.9397 - val_loss: 1.2380 - val_acc: 0.7460\n",
      "Epoch 563/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.6787 - acc: 0.9328 - val_loss: 1.2512 - val_acc: 0.7480\n",
      "Epoch 564/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6840 - acc: 0.9322 - val_loss: 1.2997 - val_acc: 0.7400\n",
      "Epoch 565/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.7180 - acc: 0.9125 - val_loss: 1.2769 - val_acc: 0.7380\n",
      "Epoch 566/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.7090 - acc: 0.9228 - val_loss: 1.2802 - val_acc: 0.7570\n",
      "Epoch 567/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6793 - acc: 0.9346 - val_loss: 1.2609 - val_acc: 0.7470\n",
      "Epoch 568/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6610 - acc: 0.9437 - val_loss: 1.2635 - val_acc: 0.7400\n",
      "Epoch 569/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.6612 - acc: 0.9415 - val_loss: 1.2435 - val_acc: 0.7530\n",
      "Epoch 570/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6572 - acc: 0.9452 - val_loss: 1.2222 - val_acc: 0.7480\n",
      "Epoch 571/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.6545 - acc: 0.9448 - val_loss: 1.2308 - val_acc: 0.7450\n",
      "Epoch 572/1000\n",
      "6500/6500 [==============================] - 0s 29us/step - loss: 0.6568 - acc: 0.9445 - val_loss: 1.2310 - val_acc: 0.7400\n",
      "Epoch 573/1000\n",
      "6500/6500 [==============================] - 0s 29us/step - loss: 0.6530 - acc: 0.9435 - val_loss: 1.2298 - val_acc: 0.7440\n",
      "Epoch 574/1000\n",
      "6500/6500 [==============================] - 0s 28us/step - loss: 0.6498 - acc: 0.9449 - val_loss: 1.2202 - val_acc: 0.7470\n",
      "Epoch 575/1000\n",
      "6500/6500 [==============================] - 0s 27us/step - loss: 0.6542 - acc: 0.9440 - val_loss: 1.2432 - val_acc: 0.7540\n",
      "Epoch 576/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.6644 - acc: 0.9371 - val_loss: 1.2388 - val_acc: 0.7440\n",
      "Epoch 577/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.6722 - acc: 0.9398 - val_loss: 1.2627 - val_acc: 0.7520\n",
      "Epoch 578/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6689 - acc: 0.9386 - val_loss: 1.2321 - val_acc: 0.7580\n",
      "Epoch 579/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6732 - acc: 0.9357 - val_loss: 1.2609 - val_acc: 0.7390\n",
      "Epoch 580/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6624 - acc: 0.9426 - val_loss: 1.2516 - val_acc: 0.7430\n",
      "Epoch 581/1000\n",
      "6500/6500 [==============================] - 0s 29us/step - loss: 0.6603 - acc: 0.9405 - val_loss: 1.2540 - val_acc: 0.7350\n",
      "Epoch 582/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6494 - acc: 0.9463 - val_loss: 1.2478 - val_acc: 0.7490\n",
      "Epoch 583/1000\n",
      "6500/6500 [==============================] - 0s 29us/step - loss: 0.6529 - acc: 0.9426 - val_loss: 1.2320 - val_acc: 0.7470\n",
      "Epoch 584/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.6563 - acc: 0.9345 - val_loss: 1.2289 - val_acc: 0.7420\n",
      "Epoch 585/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.6566 - acc: 0.9380 - val_loss: 1.2605 - val_acc: 0.7400\n",
      "Epoch 586/1000\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 0.6590 - acc: 0.9420 - val_loss: 1.2337 - val_acc: 0.7450\n",
      "Epoch 587/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 0.6633 - acc: 0.9368 - val_loss: 1.2923 - val_acc: 0.7410\n",
      "Epoch 588/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6742 - acc: 0.9386 - val_loss: 1.2717 - val_acc: 0.7480\n",
      "Epoch 589/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6632 - acc: 0.9395 - val_loss: 1.2612 - val_acc: 0.7340\n",
      "Epoch 590/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6544 - acc: 0.9446 - val_loss: 1.2451 - val_acc: 0.7420\n",
      "Epoch 591/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6500/6500 [==============================] - 0s 42us/step - loss: 0.6549 - acc: 0.9462 - val_loss: 1.2297 - val_acc: 0.7520\n",
      "Epoch 592/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 0.6545 - acc: 0.9422 - val_loss: 1.2646 - val_acc: 0.7480\n",
      "Epoch 593/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6553 - acc: 0.9417 - val_loss: 1.2697 - val_acc: 0.7360\n",
      "Epoch 594/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.6542 - acc: 0.9431 - val_loss: 1.2497 - val_acc: 0.7400\n",
      "Epoch 595/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.6537 - acc: 0.9440 - val_loss: 1.2450 - val_acc: 0.7470\n",
      "Epoch 596/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.6646 - acc: 0.9334 - val_loss: 1.2542 - val_acc: 0.7420\n",
      "Epoch 597/1000\n",
      "6500/6500 [==============================] - 0s 47us/step - loss: 0.6705 - acc: 0.9411 - val_loss: 1.2607 - val_acc: 0.7420\n",
      "Epoch 598/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.6746 - acc: 0.9398 - val_loss: 1.2434 - val_acc: 0.7470\n",
      "Epoch 599/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6667 - acc: 0.9397 - val_loss: 1.3032 - val_acc: 0.7360\n",
      "Epoch 600/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6676 - acc: 0.9365 - val_loss: 1.2378 - val_acc: 0.7430\n",
      "Epoch 601/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.6690 - acc: 0.9380 - val_loss: 1.2589 - val_acc: 0.7540\n",
      "Epoch 602/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.6520 - acc: 0.9477 - val_loss: 1.2645 - val_acc: 0.7410\n",
      "Epoch 603/1000\n",
      "6500/6500 [==============================] - 0s 29us/step - loss: 0.6495 - acc: 0.9438 - val_loss: 1.2382 - val_acc: 0.7500\n",
      "Epoch 604/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6494 - acc: 0.9455 - val_loss: 1.2474 - val_acc: 0.7390\n",
      "Epoch 605/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6494 - acc: 0.9469 - val_loss: 1.2421 - val_acc: 0.7410\n",
      "Epoch 606/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6579 - acc: 0.9385 - val_loss: 1.2560 - val_acc: 0.7350\n",
      "Epoch 607/1000\n",
      "6500/6500 [==============================] - 0s 46us/step - loss: 0.6710 - acc: 0.9345 - val_loss: 1.2589 - val_acc: 0.7410\n",
      "Epoch 608/1000\n",
      "6500/6500 [==============================] - 0s 44us/step - loss: 0.6599 - acc: 0.9391 - val_loss: 1.2889 - val_acc: 0.7360\n",
      "Epoch 609/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6708 - acc: 0.9349 - val_loss: 1.2783 - val_acc: 0.7460\n",
      "Epoch 610/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6594 - acc: 0.9445 - val_loss: 1.2536 - val_acc: 0.7450\n",
      "Epoch 611/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6466 - acc: 0.9489 - val_loss: 1.2483 - val_acc: 0.7360\n",
      "Epoch 612/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6501 - acc: 0.9448 - val_loss: 1.2432 - val_acc: 0.7430\n",
      "Epoch 613/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6599 - acc: 0.9425 - val_loss: 1.2449 - val_acc: 0.7480\n",
      "Epoch 614/1000\n",
      "6500/6500 [==============================] - 0s 29us/step - loss: 0.6664 - acc: 0.9365 - val_loss: 1.2549 - val_acc: 0.7510\n",
      "Epoch 615/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6644 - acc: 0.9426 - val_loss: 1.2671 - val_acc: 0.7410\n",
      "Epoch 616/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6547 - acc: 0.9418 - val_loss: 1.2573 - val_acc: 0.7400\n",
      "Epoch 617/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.6637 - acc: 0.9383 - val_loss: 1.2548 - val_acc: 0.7480\n",
      "Epoch 618/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.6549 - acc: 0.9426 - val_loss: 1.2614 - val_acc: 0.7440\n",
      "Epoch 619/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6503 - acc: 0.9440 - val_loss: 1.2718 - val_acc: 0.7390\n",
      "Epoch 620/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.6592 - acc: 0.9403 - val_loss: 1.2662 - val_acc: 0.7400\n",
      "Epoch 621/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.6696 - acc: 0.9369 - val_loss: 1.2516 - val_acc: 0.7500\n",
      "Epoch 622/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6598 - acc: 0.9391 - val_loss: 1.2620 - val_acc: 0.7390\n",
      "Epoch 623/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6666 - acc: 0.9380 - val_loss: 1.2741 - val_acc: 0.7480\n",
      "Epoch 624/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6693 - acc: 0.9338 - val_loss: 1.2824 - val_acc: 0.7450\n",
      "Epoch 625/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.6743 - acc: 0.9349 - val_loss: 1.2653 - val_acc: 0.7450\n",
      "Epoch 626/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6568 - acc: 0.9434 - val_loss: 1.2744 - val_acc: 0.7430\n",
      "Epoch 627/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6471 - acc: 0.9500 - val_loss: 1.2545 - val_acc: 0.7480\n",
      "Epoch 628/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6472 - acc: 0.9463 - val_loss: 1.2535 - val_acc: 0.7400\n",
      "Epoch 629/1000\n",
      "6500/6500 [==============================] - 0s 29us/step - loss: 0.6441 - acc: 0.9445 - val_loss: 1.2608 - val_acc: 0.7400\n",
      "Epoch 630/1000\n",
      "6500/6500 [==============================] - 0s 28us/step - loss: 0.6487 - acc: 0.9442 - val_loss: 1.2595 - val_acc: 0.7350\n",
      "Epoch 631/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6482 - acc: 0.9429 - val_loss: 1.2655 - val_acc: 0.7400\n",
      "Epoch 632/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6557 - acc: 0.9428 - val_loss: 1.2636 - val_acc: 0.7530\n",
      "Epoch 633/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6514 - acc: 0.9438 - val_loss: 1.2505 - val_acc: 0.7520\n",
      "Epoch 634/1000\n",
      "6500/6500 [==============================] - 0s 46us/step - loss: 0.6557 - acc: 0.9405 - val_loss: 1.2563 - val_acc: 0.7560\n",
      "Epoch 635/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.6533 - acc: 0.9442 - val_loss: 1.2549 - val_acc: 0.7500\n",
      "Epoch 636/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6408 - acc: 0.9488 - val_loss: 1.2713 - val_acc: 0.7280\n",
      "Epoch 637/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.6490 - acc: 0.9445 - val_loss: 1.2723 - val_acc: 0.7450\n",
      "Epoch 638/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.6557 - acc: 0.9440 - val_loss: 1.2499 - val_acc: 0.7480\n",
      "Epoch 639/1000\n",
      "6500/6500 [==============================] - 0s 46us/step - loss: 0.6529 - acc: 0.9409 - val_loss: 1.2553 - val_acc: 0.7440\n",
      "Epoch 640/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6663 - acc: 0.9351 - val_loss: 1.3174 - val_acc: 0.7410\n",
      "Epoch 641/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.6764 - acc: 0.9288 - val_loss: 1.3096 - val_acc: 0.7420\n",
      "Epoch 642/1000\n",
      "6500/6500 [==============================] - 0s 29us/step - loss: 0.6636 - acc: 0.9366 - val_loss: 1.2608 - val_acc: 0.7500\n",
      "Epoch 643/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.6480 - acc: 0.9478 - val_loss: 1.2500 - val_acc: 0.7370\n",
      "Epoch 644/1000\n",
      "6500/6500 [==============================] - 0s 44us/step - loss: 0.6435 - acc: 0.9475 - val_loss: 1.2495 - val_acc: 0.7500\n",
      "Epoch 645/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.6449 - acc: 0.9455 - val_loss: 1.2475 - val_acc: 0.7460\n",
      "Epoch 646/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.6374 - acc: 0.9466 - val_loss: 1.2732 - val_acc: 0.7450\n",
      "Epoch 647/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.6433 - acc: 0.9465 - val_loss: 1.2541 - val_acc: 0.7380\n",
      "Epoch 648/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.6447 - acc: 0.9448 - val_loss: 1.2497 - val_acc: 0.7430\n",
      "Epoch 649/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.6509 - acc: 0.9435 - val_loss: 1.2452 - val_acc: 0.7530\n",
      "Epoch 650/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6540 - acc: 0.9422 - val_loss: 1.2566 - val_acc: 0.7490\n",
      "Epoch 651/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6469 - acc: 0.9460 - val_loss: 1.2520 - val_acc: 0.7550\n",
      "Epoch 652/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6384 - acc: 0.9454 - val_loss: 1.2594 - val_acc: 0.7540\n",
      "Epoch 653/1000\n",
      "6500/6500 [==============================] - 0s 48us/step - loss: 0.6426 - acc: 0.9442 - val_loss: 1.2595 - val_acc: 0.7530\n",
      "Epoch 654/1000\n",
      "6500/6500 [==============================] - 0s 45us/step - loss: 0.6432 - acc: 0.9460 - val_loss: 1.2521 - val_acc: 0.7480\n",
      "Epoch 655/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6439 - acc: 0.9445 - val_loss: 1.2590 - val_acc: 0.7480\n",
      "Epoch 656/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6484 - acc: 0.9463 - val_loss: 1.2520 - val_acc: 0.7430\n",
      "Epoch 657/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.6376 - acc: 0.9477 - val_loss: 1.2382 - val_acc: 0.7480\n",
      "Epoch 658/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6448 - acc: 0.9457 - val_loss: 1.2683 - val_acc: 0.7440\n",
      "Epoch 659/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6430 - acc: 0.9455 - val_loss: 1.2725 - val_acc: 0.7410\n",
      "Epoch 660/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.6441 - acc: 0.9428 - val_loss: 1.2635 - val_acc: 0.7440\n",
      "Epoch 661/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6381 - acc: 0.9485 - val_loss: 1.2577 - val_acc: 0.7390\n",
      "Epoch 662/1000\n",
      "6500/6500 [==============================] - 0s 29us/step - loss: 0.6433 - acc: 0.9426 - val_loss: 1.3032 - val_acc: 0.7520\n",
      "Epoch 663/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6619 - acc: 0.9351 - val_loss: 1.2795 - val_acc: 0.7400\n",
      "Epoch 664/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6667 - acc: 0.9375 - val_loss: 1.2803 - val_acc: 0.7490\n",
      "Epoch 665/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6443 - acc: 0.9463 - val_loss: 1.2662 - val_acc: 0.7510\n",
      "Epoch 666/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6364 - acc: 0.9512 - val_loss: 1.2486 - val_acc: 0.7400\n",
      "Epoch 667/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.6370 - acc: 0.9466 - val_loss: 1.2647 - val_acc: 0.7430\n",
      "Epoch 668/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6330 - acc: 0.9489 - val_loss: 1.2450 - val_acc: 0.7460\n",
      "Epoch 669/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6330 - acc: 0.9498 - val_loss: 1.2538 - val_acc: 0.7560\n",
      "Epoch 670/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6398 - acc: 0.9415 - val_loss: 1.2691 - val_acc: 0.7500\n",
      "Epoch 671/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.6413 - acc: 0.9462 - val_loss: 1.2845 - val_acc: 0.7470\n",
      "Epoch 672/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6336 - acc: 0.9482 - val_loss: 1.2522 - val_acc: 0.7500\n",
      "Epoch 673/1000\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 0.6374 - acc: 0.9440 - val_loss: 1.2547 - val_acc: 0.7470\n",
      "Epoch 674/1000\n",
      "6500/6500 [==============================] - 0s 45us/step - loss: 0.6317 - acc: 0.9472 - val_loss: 1.2496 - val_acc: 0.7510\n",
      "Epoch 675/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.6436 - acc: 0.9472 - val_loss: 1.2669 - val_acc: 0.7480\n",
      "Epoch 676/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.6537 - acc: 0.9414 - val_loss: 1.3003 - val_acc: 0.7410\n",
      "Epoch 677/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.6802 - acc: 0.9266 - val_loss: 1.2558 - val_acc: 0.7530\n",
      "Epoch 678/1000\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 0.6514 - acc: 0.9472 - val_loss: 1.2794 - val_acc: 0.7460\n",
      "Epoch 679/1000\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 0.6432 - acc: 0.9452 - val_loss: 1.2623 - val_acc: 0.7410\n",
      "Epoch 680/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.6324 - acc: 0.9492 - val_loss: 1.2717 - val_acc: 0.7410\n",
      "Epoch 681/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.6536 - acc: 0.9386 - val_loss: 1.3134 - val_acc: 0.7480\n",
      "Epoch 682/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6525 - acc: 0.9446 - val_loss: 1.2708 - val_acc: 0.7380\n",
      "Epoch 683/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 0.6380 - acc: 0.9462 - val_loss: 1.2989 - val_acc: 0.7400\n",
      "Epoch 684/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 0.6475 - acc: 0.9435 - val_loss: 1.2767 - val_acc: 0.7390\n",
      "Epoch 685/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.6434 - acc: 0.9429 - val_loss: 1.2916 - val_acc: 0.7490\n",
      "Epoch 686/1000\n",
      "6500/6500 [==============================] - 0s 29us/step - loss: 0.6422 - acc: 0.9448 - val_loss: 1.2824 - val_acc: 0.7480\n",
      "Epoch 687/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6432 - acc: 0.9417 - val_loss: 1.2734 - val_acc: 0.7480\n",
      "Epoch 688/1000\n",
      "6500/6500 [==============================] - 0s 29us/step - loss: 0.6485 - acc: 0.9406 - val_loss: 1.2753 - val_acc: 0.7440\n",
      "Epoch 689/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.6394 - acc: 0.9469 - val_loss: 1.2684 - val_acc: 0.7320\n",
      "Epoch 690/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.6407 - acc: 0.9432 - val_loss: 1.2517 - val_acc: 0.7460\n",
      "Epoch 691/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6428 - acc: 0.9460 - val_loss: 1.2961 - val_acc: 0.7430\n",
      "Epoch 692/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6474 - acc: 0.9452 - val_loss: 1.2698 - val_acc: 0.7400\n",
      "Epoch 693/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.6620 - acc: 0.9371 - val_loss: 1.2865 - val_acc: 0.7500\n",
      "Epoch 694/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.6458 - acc: 0.9452 - val_loss: 1.2908 - val_acc: 0.7410\n",
      "Epoch 695/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6429 - acc: 0.9445 - val_loss: 1.2857 - val_acc: 0.7390\n",
      "Epoch 696/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6393 - acc: 0.9494 - val_loss: 1.3182 - val_acc: 0.7420\n",
      "Epoch 697/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6494 - acc: 0.9385 - val_loss: 1.3115 - val_acc: 0.7430\n",
      "Epoch 698/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6452 - acc: 0.9411 - val_loss: 1.3109 - val_acc: 0.7340\n",
      "Epoch 699/1000\n",
      "6500/6500 [==============================] - 0s 49us/step - loss: 0.6485 - acc: 0.9423 - val_loss: 1.2962 - val_acc: 0.7420\n",
      "Epoch 700/1000\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 0.6395 - acc: 0.9434 - val_loss: 1.2985 - val_acc: 0.7440\n",
      "Epoch 701/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6344 - acc: 0.9511 - val_loss: 1.2743 - val_acc: 0.7560\n",
      "Epoch 702/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6428 - acc: 0.9418 - val_loss: 1.3467 - val_acc: 0.7350\n",
      "Epoch 703/1000\n",
      "6500/6500 [==============================] - 0s 29us/step - loss: 0.6518 - acc: 0.9391 - val_loss: 1.2937 - val_acc: 0.7400\n",
      "Epoch 704/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6428 - acc: 0.9455 - val_loss: 1.2708 - val_acc: 0.7510\n",
      "Epoch 705/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6273 - acc: 0.9512 - val_loss: 1.2718 - val_acc: 0.7500\n",
      "Epoch 706/1000\n",
      "6500/6500 [==============================] - 0s 29us/step - loss: 0.6299 - acc: 0.9463 - val_loss: 1.2619 - val_acc: 0.7500\n",
      "Epoch 707/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.6360 - acc: 0.9466 - val_loss: 1.3206 - val_acc: 0.7380\n",
      "Epoch 708/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6600 - acc: 0.9374 - val_loss: 1.2955 - val_acc: 0.7550\n",
      "Epoch 709/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.6928 - acc: 0.9174 - val_loss: 1.3391 - val_acc: 0.7360\n",
      "Epoch 710/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6550 - acc: 0.9400 - val_loss: 1.3052 - val_acc: 0.7450\n",
      "Epoch 711/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6315 - acc: 0.9500 - val_loss: 1.3046 - val_acc: 0.7420\n",
      "Epoch 712/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6495 - acc: 0.9460 - val_loss: 1.2926 - val_acc: 0.7400\n",
      "Epoch 713/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6465 - acc: 0.9488 - val_loss: 1.2843 - val_acc: 0.7380\n",
      "Epoch 714/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6455 - acc: 0.9460 - val_loss: 1.3071 - val_acc: 0.7510\n",
      "Epoch 715/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.6439 - acc: 0.9429 - val_loss: 1.2896 - val_acc: 0.7510\n",
      "Epoch 716/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6428 - acc: 0.9443 - val_loss: 1.2769 - val_acc: 0.7530\n",
      "Epoch 717/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6413 - acc: 0.9478 - val_loss: 1.2801 - val_acc: 0.7550\n",
      "Epoch 718/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6299 - acc: 0.9491 - val_loss: 1.2784 - val_acc: 0.7380\n",
      "Epoch 719/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6275 - acc: 0.9457 - val_loss: 1.2777 - val_acc: 0.7440\n",
      "Epoch 720/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6315 - acc: 0.9454 - val_loss: 1.2842 - val_acc: 0.7390\n",
      "Epoch 721/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6265 - acc: 0.9497 - val_loss: 1.2920 - val_acc: 0.7330\n",
      "Epoch 722/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6199 - acc: 0.9526 - val_loss: 1.2693 - val_acc: 0.7460\n",
      "Epoch 723/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.6234 - acc: 0.9505 - val_loss: 1.2755 - val_acc: 0.7510\n",
      "Epoch 724/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.6405 - acc: 0.9412 - val_loss: 1.3141 - val_acc: 0.7410\n",
      "Epoch 725/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.6761 - acc: 0.9271 - val_loss: 1.3291 - val_acc: 0.7450\n",
      "Epoch 726/1000\n",
      "6500/6500 [==============================] - 0s 46us/step - loss: 0.6582 - acc: 0.9418 - val_loss: 1.2811 - val_acc: 0.7490\n",
      "Epoch 727/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.6319 - acc: 0.9538 - val_loss: 1.2878 - val_acc: 0.7420\n",
      "Epoch 728/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6373 - acc: 0.9465 - val_loss: 1.2659 - val_acc: 0.7620\n",
      "Epoch 729/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6456 - acc: 0.9460 - val_loss: 1.3149 - val_acc: 0.7370\n",
      "Epoch 730/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.6509 - acc: 0.9422 - val_loss: 1.2773 - val_acc: 0.7590\n",
      "Epoch 731/1000\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 0.6303 - acc: 0.9486 - val_loss: 1.2748 - val_acc: 0.7490\n",
      "Epoch 732/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.6247 - acc: 0.9509 - val_loss: 1.2646 - val_acc: 0.7590\n",
      "Epoch 733/1000\n",
      "6500/6500 [==============================] - 0s 29us/step - loss: 0.6353 - acc: 0.9442 - val_loss: 1.2925 - val_acc: 0.7430\n",
      "Epoch 734/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.6454 - acc: 0.9423 - val_loss: 1.3046 - val_acc: 0.7440\n",
      "Epoch 735/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6395 - acc: 0.9435 - val_loss: 1.3007 - val_acc: 0.7510\n",
      "Epoch 736/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.6525 - acc: 0.9403 - val_loss: 1.2907 - val_acc: 0.7430\n",
      "Epoch 737/1000\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 0.6402 - acc: 0.9474 - val_loss: 1.2946 - val_acc: 0.7360\n",
      "Epoch 738/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6425 - acc: 0.9412 - val_loss: 1.2902 - val_acc: 0.7410\n",
      "Epoch 739/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.6346 - acc: 0.9468 - val_loss: 1.2973 - val_acc: 0.7420\n",
      "Epoch 740/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6398 - acc: 0.9466 - val_loss: 1.3165 - val_acc: 0.7320\n",
      "Epoch 741/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6307 - acc: 0.9478 - val_loss: 1.2834 - val_acc: 0.7410\n",
      "Epoch 742/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.6238 - acc: 0.9506 - val_loss: 1.2674 - val_acc: 0.7470\n",
      "Epoch 743/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6155 - acc: 0.9526 - val_loss: 1.2701 - val_acc: 0.7360\n",
      "Epoch 744/1000\n",
      "6500/6500 [==============================] - 0s 29us/step - loss: 0.6211 - acc: 0.9497 - val_loss: 1.2807 - val_acc: 0.7450\n",
      "Epoch 745/1000\n",
      "6500/6500 [==============================] - 0s 45us/step - loss: 0.6557 - acc: 0.9362 - val_loss: 1.2781 - val_acc: 0.7570\n",
      "Epoch 746/1000\n",
      "6500/6500 [==============================] - 0s 46us/step - loss: 0.6465 - acc: 0.9440 - val_loss: 1.2921 - val_acc: 0.7440\n",
      "Epoch 747/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.6255 - acc: 0.9508 - val_loss: 1.2903 - val_acc: 0.7330\n",
      "Epoch 748/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.6196 - acc: 0.9522 - val_loss: 1.2720 - val_acc: 0.7460\n",
      "Epoch 749/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.6343 - acc: 0.9426 - val_loss: 1.2962 - val_acc: 0.7430\n",
      "Epoch 750/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.6563 - acc: 0.9391 - val_loss: 1.2887 - val_acc: 0.7410\n",
      "Epoch 751/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6356 - acc: 0.9462 - val_loss: 1.2856 - val_acc: 0.7460\n",
      "Epoch 752/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6320 - acc: 0.9489 - val_loss: 1.2857 - val_acc: 0.7430\n",
      "Epoch 753/1000\n",
      "6500/6500 [==============================] - 0s 29us/step - loss: 0.6235 - acc: 0.9502 - val_loss: 1.2852 - val_acc: 0.7470\n",
      "Epoch 754/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6296 - acc: 0.9462 - val_loss: 1.2733 - val_acc: 0.7500\n",
      "Epoch 755/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6297 - acc: 0.9482 - val_loss: 1.3036 - val_acc: 0.7320\n",
      "Epoch 756/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.6347 - acc: 0.9486 - val_loss: 1.2868 - val_acc: 0.7420\n",
      "Epoch 757/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.6307 - acc: 0.9489 - val_loss: 1.2931 - val_acc: 0.7350\n",
      "Epoch 758/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6150 - acc: 0.9538 - val_loss: 1.2618 - val_acc: 0.7450\n",
      "Epoch 759/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.6096 - acc: 0.9532 - val_loss: 1.2770 - val_acc: 0.7490\n",
      "Epoch 760/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6134 - acc: 0.9537 - val_loss: 1.2956 - val_acc: 0.7470\n",
      "Epoch 761/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6153 - acc: 0.9478 - val_loss: 1.2739 - val_acc: 0.7460\n",
      "Epoch 762/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.6130 - acc: 0.9528 - val_loss: 1.2690 - val_acc: 0.7360\n",
      "Epoch 763/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6270 - acc: 0.9440 - val_loss: 1.2852 - val_acc: 0.7470\n",
      "Epoch 764/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6260 - acc: 0.9502 - val_loss: 1.2799 - val_acc: 0.7420\n",
      "Epoch 765/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6287 - acc: 0.9431 - val_loss: 1.2884 - val_acc: 0.7490\n",
      "Epoch 766/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6336 - acc: 0.9446 - val_loss: 1.2789 - val_acc: 0.7510\n",
      "Epoch 767/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.6377 - acc: 0.9449 - val_loss: 1.3144 - val_acc: 0.7410\n",
      "Epoch 768/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6500/6500 [==============================] - 0s 29us/step - loss: 0.6375 - acc: 0.9448 - val_loss: 1.2850 - val_acc: 0.7390\n",
      "Epoch 769/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6302 - acc: 0.9477 - val_loss: 1.2955 - val_acc: 0.7480\n",
      "Epoch 770/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6404 - acc: 0.9411 - val_loss: 1.3326 - val_acc: 0.7370\n",
      "Epoch 771/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.6516 - acc: 0.9363 - val_loss: 1.3094 - val_acc: 0.7410\n",
      "Epoch 772/1000\n",
      "6500/6500 [==============================] - 0s 45us/step - loss: 0.6523 - acc: 0.9345 - val_loss: 1.2985 - val_acc: 0.7470\n",
      "Epoch 773/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6326 - acc: 0.9472 - val_loss: 1.3054 - val_acc: 0.7510\n",
      "Epoch 774/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6254 - acc: 0.9508 - val_loss: 1.2854 - val_acc: 0.7440\n",
      "Epoch 775/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6150 - acc: 0.9529 - val_loss: 1.2953 - val_acc: 0.7450\n",
      "Epoch 776/1000\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 0.6225 - acc: 0.9494 - val_loss: 1.2997 - val_acc: 0.7400\n",
      "Epoch 777/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.6419 - acc: 0.9394 - val_loss: 1.3007 - val_acc: 0.7490\n",
      "Epoch 778/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.6483 - acc: 0.9372 - val_loss: 1.3056 - val_acc: 0.7320\n",
      "Epoch 779/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.6329 - acc: 0.9457 - val_loss: 1.3047 - val_acc: 0.7480\n",
      "Epoch 780/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.6295 - acc: 0.9469 - val_loss: 1.3104 - val_acc: 0.7430\n",
      "Epoch 781/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.6279 - acc: 0.9495 - val_loss: 1.3087 - val_acc: 0.7370\n",
      "Epoch 782/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 0.6211 - acc: 0.9477 - val_loss: 1.2949 - val_acc: 0.7410\n",
      "Epoch 783/1000\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 0.6281 - acc: 0.9477 - val_loss: 1.2987 - val_acc: 0.7340\n",
      "Epoch 784/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6406 - acc: 0.9423 - val_loss: 1.3458 - val_acc: 0.7420\n",
      "Epoch 785/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6372 - acc: 0.9462 - val_loss: 1.3484 - val_acc: 0.7330\n",
      "Epoch 786/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6367 - acc: 0.9438 - val_loss: 1.3020 - val_acc: 0.7390\n",
      "Epoch 787/1000\n",
      "6500/6500 [==============================] - 0s 29us/step - loss: 0.6288 - acc: 0.9485 - val_loss: 1.2903 - val_acc: 0.7440\n",
      "Epoch 788/1000\n",
      "6500/6500 [==============================] - 0s 27us/step - loss: 0.6220 - acc: 0.9492 - val_loss: 1.2962 - val_acc: 0.7420\n",
      "Epoch 789/1000\n",
      "6500/6500 [==============================] - 0s 29us/step - loss: 0.6156 - acc: 0.9537 - val_loss: 1.2983 - val_acc: 0.7370\n",
      "Epoch 790/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.6223 - acc: 0.9486 - val_loss: 1.2757 - val_acc: 0.7470\n",
      "Epoch 791/1000\n",
      "6500/6500 [==============================] - 0s 48us/step - loss: 0.6408 - acc: 0.9420 - val_loss: 1.2957 - val_acc: 0.7490\n",
      "Epoch 792/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.6386 - acc: 0.9440 - val_loss: 1.2884 - val_acc: 0.7450\n",
      "Epoch 793/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.6263 - acc: 0.9502 - val_loss: 1.2961 - val_acc: 0.7440\n",
      "Epoch 794/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6133 - acc: 0.9574 - val_loss: 1.2816 - val_acc: 0.7440\n",
      "Epoch 795/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6067 - acc: 0.9563 - val_loss: 1.3043 - val_acc: 0.7490\n",
      "Epoch 796/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.6229 - acc: 0.9463 - val_loss: 1.2917 - val_acc: 0.7400\n",
      "Epoch 797/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6316 - acc: 0.9446 - val_loss: 1.3215 - val_acc: 0.7400\n",
      "Epoch 798/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6323 - acc: 0.9448 - val_loss: 1.3184 - val_acc: 0.7330\n",
      "Epoch 799/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6217 - acc: 0.9478 - val_loss: 1.3018 - val_acc: 0.7460\n",
      "Epoch 800/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6218 - acc: 0.9495 - val_loss: 1.2910 - val_acc: 0.7510\n",
      "Epoch 801/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6321 - acc: 0.9445 - val_loss: 1.2968 - val_acc: 0.7490\n",
      "Epoch 802/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.6229 - acc: 0.9494 - val_loss: 1.3081 - val_acc: 0.7440\n",
      "Epoch 803/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.6196 - acc: 0.9520 - val_loss: 1.2764 - val_acc: 0.7480\n",
      "Epoch 804/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.6197 - acc: 0.9474 - val_loss: 1.2739 - val_acc: 0.7570\n",
      "Epoch 805/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.6195 - acc: 0.9483 - val_loss: 1.3022 - val_acc: 0.7380\n",
      "Epoch 806/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6227 - acc: 0.9474 - val_loss: 1.2900 - val_acc: 0.7380\n",
      "Epoch 807/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.6110 - acc: 0.9511 - val_loss: 1.2717 - val_acc: 0.7470\n",
      "Epoch 808/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.6242 - acc: 0.9474 - val_loss: 1.3140 - val_acc: 0.7450\n",
      "Epoch 809/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6172 - acc: 0.9500 - val_loss: 1.2861 - val_acc: 0.7380\n",
      "Epoch 810/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.6256 - acc: 0.9489 - val_loss: 1.3040 - val_acc: 0.7370\n",
      "Epoch 811/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 0.6471 - acc: 0.9357 - val_loss: 1.3201 - val_acc: 0.7430\n",
      "Epoch 812/1000\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 0.6428 - acc: 0.9420 - val_loss: 1.3073 - val_acc: 0.7460\n",
      "Epoch 813/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6252 - acc: 0.9509 - val_loss: 1.3034 - val_acc: 0.7440\n",
      "Epoch 814/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6266 - acc: 0.9497 - val_loss: 1.3310 - val_acc: 0.7340\n",
      "Epoch 815/1000\n",
      "6500/6500 [==============================] - 0s 27us/step - loss: 0.6616 - acc: 0.9375 - val_loss: 1.3409 - val_acc: 0.7410\n",
      "Epoch 816/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 0.6608 - acc: 0.9389 - val_loss: 1.3242 - val_acc: 0.7420\n",
      "Epoch 817/1000\n",
      "6500/6500 [==============================] - 0s 47us/step - loss: 0.6185 - acc: 0.9560 - val_loss: 1.2990 - val_acc: 0.7480\n",
      "Epoch 818/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.6113 - acc: 0.9557 - val_loss: 1.2907 - val_acc: 0.7510\n",
      "Epoch 819/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6095 - acc: 0.9523 - val_loss: 1.3053 - val_acc: 0.7410\n",
      "Epoch 820/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.6047 - acc: 0.9558 - val_loss: 1.2769 - val_acc: 0.7460\n",
      "Epoch 821/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.6212 - acc: 0.9485 - val_loss: 1.3071 - val_acc: 0.7530\n",
      "Epoch 822/1000\n",
      "6500/6500 [==============================] - 0s 45us/step - loss: 0.6532 - acc: 0.9363 - val_loss: 1.3583 - val_acc: 0.7280\n",
      "Epoch 823/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.6530 - acc: 0.9398 - val_loss: 1.3443 - val_acc: 0.7300\n",
      "Epoch 824/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6315 - acc: 0.9463 - val_loss: 1.3383 - val_acc: 0.7360\n",
      "Epoch 825/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.6128 - acc: 0.9543 - val_loss: 1.2949 - val_acc: 0.7500\n",
      "Epoch 826/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6147 - acc: 0.9522 - val_loss: 1.3127 - val_acc: 0.7480\n",
      "Epoch 827/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6179 - acc: 0.9531 - val_loss: 1.2891 - val_acc: 0.7400\n",
      "Epoch 828/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.6127 - acc: 0.9492 - val_loss: 1.2807 - val_acc: 0.7410\n",
      "Epoch 829/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.6075 - acc: 0.9532 - val_loss: 1.2901 - val_acc: 0.7410\n",
      "Epoch 830/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6234 - acc: 0.9466 - val_loss: 1.2956 - val_acc: 0.7470\n",
      "Epoch 831/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.6314 - acc: 0.9454 - val_loss: 1.3720 - val_acc: 0.7370\n",
      "Epoch 832/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6374 - acc: 0.9448 - val_loss: 1.3102 - val_acc: 0.7420\n",
      "Epoch 833/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6330 - acc: 0.9443 - val_loss: 1.3793 - val_acc: 0.7320\n",
      "Epoch 834/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.6837 - acc: 0.9218 - val_loss: 1.3718 - val_acc: 0.7530\n",
      "Epoch 835/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.6558 - acc: 0.9432 - val_loss: 1.3355 - val_acc: 0.7430\n",
      "Epoch 836/1000\n",
      "6500/6500 [==============================] - 0s 46us/step - loss: 0.6193 - acc: 0.9577 - val_loss: 1.3289 - val_acc: 0.7400\n",
      "Epoch 837/1000\n",
      "6500/6500 [==============================] - 0s 48us/step - loss: 0.6073 - acc: 0.9563 - val_loss: 1.2937 - val_acc: 0.7440\n",
      "Epoch 838/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6049 - acc: 0.9560 - val_loss: 1.2895 - val_acc: 0.7420\n",
      "Epoch 839/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.6057 - acc: 0.9537 - val_loss: 1.3001 - val_acc: 0.7430\n",
      "Epoch 840/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6071 - acc: 0.9509 - val_loss: 1.2994 - val_acc: 0.7400\n",
      "Epoch 841/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6126 - acc: 0.9488 - val_loss: 1.3370 - val_acc: 0.7460\n",
      "Epoch 842/1000\n",
      "6500/6500 [==============================] - 0s 29us/step - loss: 0.6176 - acc: 0.9488 - val_loss: 1.2963 - val_acc: 0.7510\n",
      "Epoch 843/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.6139 - acc: 0.9494 - val_loss: 1.3192 - val_acc: 0.7440\n",
      "Epoch 844/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6181 - acc: 0.9488 - val_loss: 1.2921 - val_acc: 0.7460\n",
      "Epoch 845/1000\n",
      "6500/6500 [==============================] - 0s 29us/step - loss: 0.6038 - acc: 0.9532 - val_loss: 1.2882 - val_acc: 0.7520\n",
      "Epoch 846/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.6153 - acc: 0.9532 - val_loss: 1.3055 - val_acc: 0.7410\n",
      "Epoch 847/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6239 - acc: 0.9457 - val_loss: 1.3246 - val_acc: 0.7470\n",
      "Epoch 848/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.6143 - acc: 0.9548 - val_loss: 1.3036 - val_acc: 0.7510\n",
      "Epoch 849/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6111 - acc: 0.9526 - val_loss: 1.2830 - val_acc: 0.7490\n",
      "Epoch 850/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6175 - acc: 0.9505 - val_loss: 1.2962 - val_acc: 0.7420\n",
      "Epoch 851/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6230 - acc: 0.9460 - val_loss: 1.3027 - val_acc: 0.7430\n",
      "Epoch 852/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.6145 - acc: 0.9515 - val_loss: 1.3260 - val_acc: 0.7380\n",
      "Epoch 853/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6248 - acc: 0.9472 - val_loss: 1.2997 - val_acc: 0.7520\n",
      "Epoch 854/1000\n",
      "6500/6500 [==============================] - 0s 29us/step - loss: 0.6124 - acc: 0.9518 - val_loss: 1.3403 - val_acc: 0.7420\n",
      "Epoch 855/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6204 - acc: 0.9457 - val_loss: 1.3401 - val_acc: 0.7400\n",
      "Epoch 856/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.6371 - acc: 0.9434 - val_loss: 1.3415 - val_acc: 0.7370\n",
      "Epoch 857/1000\n",
      "6500/6500 [==============================] - 0s 44us/step - loss: 0.6387 - acc: 0.9457 - val_loss: 1.3636 - val_acc: 0.7380\n",
      "Epoch 858/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.6288 - acc: 0.9465 - val_loss: 1.3349 - val_acc: 0.7460\n",
      "Epoch 859/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6187 - acc: 0.9502 - val_loss: 1.3205 - val_acc: 0.7540\n",
      "Epoch 860/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.6138 - acc: 0.9505 - val_loss: 1.3058 - val_acc: 0.7430\n",
      "Epoch 861/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.6066 - acc: 0.9569 - val_loss: 1.2993 - val_acc: 0.7430\n",
      "Epoch 862/1000\n",
      "6500/6500 [==============================] - 0s 44us/step - loss: 0.6032 - acc: 0.9545 - val_loss: 1.2891 - val_acc: 0.7530\n",
      "Epoch 863/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.6074 - acc: 0.9522 - val_loss: 1.2929 - val_acc: 0.7480\n",
      "Epoch 864/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.6060 - acc: 0.9509 - val_loss: 1.3213 - val_acc: 0.7490\n",
      "Epoch 865/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6082 - acc: 0.9511 - val_loss: 1.3284 - val_acc: 0.7400\n",
      "Epoch 866/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.6127 - acc: 0.9491 - val_loss: 1.2878 - val_acc: 0.7460\n",
      "Epoch 867/1000\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 0.6322 - acc: 0.9415 - val_loss: 1.3670 - val_acc: 0.7380\n",
      "Epoch 868/1000\n",
      "6500/6500 [==============================] - 0s 44us/step - loss: 0.6275 - acc: 0.9452 - val_loss: 1.3281 - val_acc: 0.7540\n",
      "Epoch 869/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6266 - acc: 0.9460 - val_loss: 1.3466 - val_acc: 0.7440\n",
      "Epoch 870/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6411 - acc: 0.9432 - val_loss: 1.3984 - val_acc: 0.7370\n",
      "Epoch 871/1000\n",
      "6500/6500 [==============================] - 0s 29us/step - loss: 0.6466 - acc: 0.9386 - val_loss: 1.3592 - val_acc: 0.7320\n",
      "Epoch 872/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.6226 - acc: 0.9534 - val_loss: 1.3241 - val_acc: 0.7440\n",
      "Epoch 873/1000\n",
      "6500/6500 [==============================] - 0s 29us/step - loss: 0.6030 - acc: 0.9534 - val_loss: 1.2955 - val_acc: 0.7390\n",
      "Epoch 874/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6079 - acc: 0.9537 - val_loss: 1.3198 - val_acc: 0.7410\n",
      "Epoch 875/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.6195 - acc: 0.9497 - val_loss: 1.3296 - val_acc: 0.7330\n",
      "Epoch 876/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6339 - acc: 0.9469 - val_loss: 1.3433 - val_acc: 0.7350\n",
      "Epoch 877/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6299 - acc: 0.9431 - val_loss: 1.3352 - val_acc: 0.7410\n",
      "Epoch 878/1000\n",
      "6500/6500 [==============================] - 0s 29us/step - loss: 0.6323 - acc: 0.9417 - val_loss: 1.3648 - val_acc: 0.7330\n",
      "Epoch 879/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6223 - acc: 0.9522 - val_loss: 1.3459 - val_acc: 0.7330\n",
      "Epoch 880/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6106 - acc: 0.9525 - val_loss: 1.3202 - val_acc: 0.7510\n",
      "Epoch 881/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.6033 - acc: 0.9562 - val_loss: 1.3077 - val_acc: 0.7490\n",
      "Epoch 882/1000\n",
      "6500/6500 [==============================] - 0s 46us/step - loss: 0.6049 - acc: 0.9549 - val_loss: 1.3228 - val_acc: 0.7470\n",
      "Epoch 883/1000\n",
      "6500/6500 [==============================] - 0s 47us/step - loss: 0.6007 - acc: 0.9555 - val_loss: 1.2880 - val_acc: 0.7520\n",
      "Epoch 884/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6046 - acc: 0.9538 - val_loss: 1.3093 - val_acc: 0.7430\n",
      "Epoch 885/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6087 - acc: 0.9538 - val_loss: 1.3025 - val_acc: 0.7470\n",
      "Epoch 886/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6001 - acc: 0.9542 - val_loss: 1.3223 - val_acc: 0.7340\n",
      "Epoch 887/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.6041 - acc: 0.9532 - val_loss: 1.3239 - val_acc: 0.7330\n",
      "Epoch 888/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6430 - acc: 0.9371 - val_loss: 1.3223 - val_acc: 0.7370\n",
      "Epoch 889/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6414 - acc: 0.9383 - val_loss: 1.3859 - val_acc: 0.7390\n",
      "Epoch 890/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.6405 - acc: 0.9429 - val_loss: 1.3585 - val_acc: 0.7380\n",
      "Epoch 891/1000\n",
      "6500/6500 [==============================] - 0s 29us/step - loss: 0.6216 - acc: 0.9523 - val_loss: 1.3519 - val_acc: 0.7400\n",
      "Epoch 892/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6073 - acc: 0.9545 - val_loss: 1.3613 - val_acc: 0.7370\n",
      "Epoch 893/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.6091 - acc: 0.9502 - val_loss: 1.3321 - val_acc: 0.7410\n",
      "Epoch 894/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6186 - acc: 0.9466 - val_loss: 1.3187 - val_acc: 0.7470\n",
      "Epoch 895/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6057 - acc: 0.9498 - val_loss: 1.3182 - val_acc: 0.7470\n",
      "Epoch 896/1000\n",
      "6500/6500 [==============================] - 0s 29us/step - loss: 0.5966 - acc: 0.9562 - val_loss: 1.3560 - val_acc: 0.7460\n",
      "Epoch 897/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.6044 - acc: 0.9528 - val_loss: 1.3704 - val_acc: 0.7250\n",
      "Epoch 898/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6150 - acc: 0.9517 - val_loss: 1.3521 - val_acc: 0.7300\n",
      "Epoch 899/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6134 - acc: 0.9523 - val_loss: 1.3192 - val_acc: 0.7400\n",
      "Epoch 900/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.6081 - acc: 0.9515 - val_loss: 1.3085 - val_acc: 0.7460\n",
      "Epoch 901/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6128 - acc: 0.9518 - val_loss: 1.3259 - val_acc: 0.7430\n",
      "Epoch 902/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6098 - acc: 0.9540 - val_loss: 1.3461 - val_acc: 0.7340\n",
      "Epoch 903/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.6057 - acc: 0.9546 - val_loss: 1.2994 - val_acc: 0.7440\n",
      "Epoch 904/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6038 - acc: 0.9542 - val_loss: 1.2966 - val_acc: 0.7450\n",
      "Epoch 905/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.6023 - acc: 0.9551 - val_loss: 1.3227 - val_acc: 0.7440\n",
      "Epoch 906/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6211 - acc: 0.9492 - val_loss: 1.3471 - val_acc: 0.7310\n",
      "Epoch 907/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.6236 - acc: 0.9483 - val_loss: 1.3329 - val_acc: 0.7470\n",
      "Epoch 908/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6243 - acc: 0.9458 - val_loss: 1.3876 - val_acc: 0.7270\n",
      "Epoch 909/1000\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 0.6727 - acc: 0.9237 - val_loss: 1.3572 - val_acc: 0.7490\n",
      "Epoch 910/1000\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 0.6545 - acc: 0.9385 - val_loss: 1.3787 - val_acc: 0.7440\n",
      "Epoch 911/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6289 - acc: 0.9506 - val_loss: 1.3864 - val_acc: 0.7360\n",
      "Epoch 912/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.6199 - acc: 0.9515 - val_loss: 1.3460 - val_acc: 0.7430\n",
      "Epoch 913/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.6098 - acc: 0.9520 - val_loss: 1.3135 - val_acc: 0.7420\n",
      "Epoch 914/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.6093 - acc: 0.9568 - val_loss: 1.3323 - val_acc: 0.7400\n",
      "Epoch 915/1000\n",
      "6500/6500 [==============================] - 0s 44us/step - loss: 0.6035 - acc: 0.9538 - val_loss: 1.3304 - val_acc: 0.7420\n",
      "Epoch 916/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6039 - acc: 0.9525 - val_loss: 1.3190 - val_acc: 0.7470\n",
      "Epoch 917/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6139 - acc: 0.9485 - val_loss: 1.3762 - val_acc: 0.7370\n",
      "Epoch 918/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6309 - acc: 0.9385 - val_loss: 1.3228 - val_acc: 0.7360\n",
      "Epoch 919/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 0.6461 - acc: 0.9374 - val_loss: 1.3461 - val_acc: 0.7400\n",
      "Epoch 920/1000\n",
      "6500/6500 [==============================] - 0s 48us/step - loss: 0.6191 - acc: 0.9518 - val_loss: 1.3295 - val_acc: 0.7330\n",
      "Epoch 921/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6026 - acc: 0.9585 - val_loss: 1.3235 - val_acc: 0.7410\n",
      "Epoch 922/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.5935 - acc: 0.9582 - val_loss: 1.3302 - val_acc: 0.7320\n",
      "Epoch 923/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.5940 - acc: 0.9565 - val_loss: 1.3252 - val_acc: 0.7380\n",
      "Epoch 924/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.5972 - acc: 0.9554 - val_loss: 1.3382 - val_acc: 0.7360\n",
      "Epoch 925/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6027 - acc: 0.9503 - val_loss: 1.3407 - val_acc: 0.7400\n",
      "Epoch 926/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.6232 - acc: 0.9442 - val_loss: 1.3558 - val_acc: 0.7430\n",
      "Epoch 927/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.6349 - acc: 0.9471 - val_loss: 1.3768 - val_acc: 0.7380\n",
      "Epoch 928/1000\n",
      "6500/6500 [==============================] - 0s 46us/step - loss: 0.6196 - acc: 0.9529 - val_loss: 1.3522 - val_acc: 0.7400\n",
      "Epoch 929/1000\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 0.6109 - acc: 0.9578 - val_loss: 1.3400 - val_acc: 0.7380\n",
      "Epoch 930/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6111 - acc: 0.9518 - val_loss: 1.3578 - val_acc: 0.7360\n",
      "Epoch 931/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.6209 - acc: 0.9482 - val_loss: 1.3388 - val_acc: 0.7410\n",
      "Epoch 932/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.6141 - acc: 0.9540 - val_loss: 1.3439 - val_acc: 0.7430\n",
      "Epoch 933/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.6014 - acc: 0.9572 - val_loss: 1.3492 - val_acc: 0.7370\n",
      "Epoch 934/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.5914 - acc: 0.9578 - val_loss: 1.3182 - val_acc: 0.7480\n",
      "Epoch 935/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6027 - acc: 0.9523 - val_loss: 1.3295 - val_acc: 0.7400\n",
      "Epoch 936/1000\n",
      "6500/6500 [==============================] - 0s 29us/step - loss: 0.6079 - acc: 0.9515 - val_loss: 1.3374 - val_acc: 0.7490\n",
      "Epoch 937/1000\n",
      "6500/6500 [==============================] - 0s 29us/step - loss: 0.6067 - acc: 0.9506 - val_loss: 1.3321 - val_acc: 0.7390\n",
      "Epoch 938/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.5973 - acc: 0.9572 - val_loss: 1.3332 - val_acc: 0.7390\n",
      "Epoch 939/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6081 - acc: 0.9506 - val_loss: 1.3509 - val_acc: 0.7300\n",
      "Epoch 940/1000\n",
      "6500/6500 [==============================] - 0s 29us/step - loss: 0.6175 - acc: 0.9492 - val_loss: 1.3518 - val_acc: 0.7360\n",
      "Epoch 941/1000\n",
      "6500/6500 [==============================] - 0s 29us/step - loss: 0.6197 - acc: 0.9494 - val_loss: 1.3948 - val_acc: 0.7240\n",
      "Epoch 942/1000\n",
      "6500/6500 [==============================] - 0s 29us/step - loss: 0.6233 - acc: 0.9462 - val_loss: 1.3399 - val_acc: 0.7430\n",
      "Epoch 943/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6213 - acc: 0.9500 - val_loss: 1.3782 - val_acc: 0.7300\n",
      "Epoch 944/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.6276 - acc: 0.9458 - val_loss: 1.3624 - val_acc: 0.7370\n",
      "Epoch 945/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.6258 - acc: 0.9478 - val_loss: 1.3548 - val_acc: 0.7410\n",
      "Epoch 946/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6264 - acc: 0.9460 - val_loss: 1.3726 - val_acc: 0.7400\n",
      "Epoch 947/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.6108 - acc: 0.9531 - val_loss: 1.3573 - val_acc: 0.7420\n",
      "Epoch 948/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.5970 - acc: 0.9578 - val_loss: 1.3398 - val_acc: 0.7440\n",
      "Epoch 949/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.5880 - acc: 0.9597 - val_loss: 1.3383 - val_acc: 0.7350\n",
      "Epoch 950/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6009 - acc: 0.9554 - val_loss: 1.3432 - val_acc: 0.7310\n",
      "Epoch 951/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6020 - acc: 0.9537 - val_loss: 1.3322 - val_acc: 0.7390\n",
      "Epoch 952/1000\n",
      "6500/6500 [==============================] - 0s 28us/step - loss: 0.6052 - acc: 0.9540 - val_loss: 1.3781 - val_acc: 0.7320\n",
      "Epoch 953/1000\n",
      "6500/6500 [==============================] - 0s 29us/step - loss: 0.6233 - acc: 0.9469 - val_loss: 1.3450 - val_acc: 0.7470\n",
      "Epoch 954/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.6120 - acc: 0.9517 - val_loss: 1.3294 - val_acc: 0.7400\n",
      "Epoch 955/1000\n",
      "6500/6500 [==============================] - 0s 44us/step - loss: 0.6121 - acc: 0.9478 - val_loss: 1.3243 - val_acc: 0.7440\n",
      "Epoch 956/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.6052 - acc: 0.9529 - val_loss: 1.3390 - val_acc: 0.7380\n",
      "Epoch 957/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.5946 - acc: 0.9572 - val_loss: 1.3411 - val_acc: 0.7380\n",
      "Epoch 958/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6077 - acc: 0.9512 - val_loss: 1.3547 - val_acc: 0.7500\n",
      "Epoch 959/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.6119 - acc: 0.9503 - val_loss: 1.3408 - val_acc: 0.7510\n",
      "Epoch 960/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.6100 - acc: 0.9502 - val_loss: 1.3305 - val_acc: 0.7430\n",
      "Epoch 961/1000\n",
      "6500/6500 [==============================] - 0s 46us/step - loss: 0.6110 - acc: 0.9488 - val_loss: 1.3579 - val_acc: 0.7410\n",
      "Epoch 962/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.6291 - acc: 0.9405 - val_loss: 1.3724 - val_acc: 0.7380\n",
      "Epoch 963/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.6123 - acc: 0.9538 - val_loss: 1.4187 - val_acc: 0.7220\n",
      "Epoch 964/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.6059 - acc: 0.9568 - val_loss: 1.3680 - val_acc: 0.7380\n",
      "Epoch 965/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.6087 - acc: 0.9548 - val_loss: 1.3526 - val_acc: 0.7360\n",
      "Epoch 966/1000\n",
      "6500/6500 [==============================] - 0s 45us/step - loss: 0.5974 - acc: 0.9569 - val_loss: 1.3241 - val_acc: 0.7420\n",
      "Epoch 967/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.5883 - acc: 0.9562 - val_loss: 1.3728 - val_acc: 0.7340\n",
      "Epoch 968/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.5936 - acc: 0.9562 - val_loss: 1.3340 - val_acc: 0.7500\n",
      "Epoch 969/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.5916 - acc: 0.9552 - val_loss: 1.3402 - val_acc: 0.7340\n",
      "Epoch 970/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.5971 - acc: 0.9515 - val_loss: 1.3389 - val_acc: 0.7420\n",
      "Epoch 971/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.5954 - acc: 0.9542 - val_loss: 1.3592 - val_acc: 0.7400\n",
      "Epoch 972/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.6167 - acc: 0.9478 - val_loss: 1.3367 - val_acc: 0.7430\n",
      "Epoch 973/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6024 - acc: 0.9538 - val_loss: 1.3434 - val_acc: 0.7360\n",
      "Epoch 974/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 0.6107 - acc: 0.9482 - val_loss: 1.3586 - val_acc: 0.7350\n",
      "Epoch 975/1000\n",
      "6500/6500 [==============================] - 0s 48us/step - loss: 0.6127 - acc: 0.9531 - val_loss: 1.3818 - val_acc: 0.7280\n",
      "Epoch 976/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.6235 - acc: 0.9429 - val_loss: 1.3806 - val_acc: 0.7290\n",
      "Epoch 977/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.6295 - acc: 0.9420 - val_loss: 1.4136 - val_acc: 0.7350\n",
      "Epoch 978/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.6189 - acc: 0.9498 - val_loss: 1.3751 - val_acc: 0.7340\n",
      "Epoch 979/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.6216 - acc: 0.9494 - val_loss: 1.3645 - val_acc: 0.7390\n",
      "Epoch 980/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.6123 - acc: 0.9495 - val_loss: 1.3883 - val_acc: 0.7390\n",
      "Epoch 981/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6182 - acc: 0.9498 - val_loss: 1.3545 - val_acc: 0.7330\n",
      "Epoch 982/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6001 - acc: 0.9591 - val_loss: 1.3596 - val_acc: 0.7390\n",
      "Epoch 983/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.5950 - acc: 0.9563 - val_loss: 1.3684 - val_acc: 0.7350\n",
      "Epoch 984/1000\n",
      "6500/6500 [==============================] - 0s 28us/step - loss: 0.5907 - acc: 0.9580 - val_loss: 1.3873 - val_acc: 0.7310\n",
      "Epoch 985/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.5875 - acc: 0.9569 - val_loss: 1.3285 - val_acc: 0.7350\n",
      "Epoch 986/1000\n",
      "6500/6500 [==============================] - 0s 29us/step - loss: 0.5906 - acc: 0.9538 - val_loss: 1.3439 - val_acc: 0.7390\n",
      "Epoch 987/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.5918 - acc: 0.9545 - val_loss: 1.3433 - val_acc: 0.7380\n",
      "Epoch 988/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6021 - acc: 0.9523 - val_loss: 1.3528 - val_acc: 0.7420\n",
      "Epoch 989/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6145 - acc: 0.9505 - val_loss: 1.3658 - val_acc: 0.7410\n",
      "Epoch 990/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.6178 - acc: 0.9491 - val_loss: 1.3323 - val_acc: 0.7430\n",
      "Epoch 991/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.6051 - acc: 0.9546 - val_loss: 1.3440 - val_acc: 0.7380\n",
      "Epoch 992/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.5981 - acc: 0.9569 - val_loss: 1.3514 - val_acc: 0.7510\n",
      "Epoch 993/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6010 - acc: 0.9558 - val_loss: 1.3738 - val_acc: 0.7370\n",
      "Epoch 994/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.5973 - acc: 0.9537 - val_loss: 1.3384 - val_acc: 0.7380\n",
      "Epoch 995/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.5955 - acc: 0.9537 - val_loss: 1.3435 - val_acc: 0.7290\n",
      "Epoch 996/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6035 - acc: 0.9523 - val_loss: 1.3529 - val_acc: 0.7330\n",
      "Epoch 997/1000\n",
      "6500/6500 [==============================] - 0s 54us/step - loss: 0.6106 - acc: 0.9518 - val_loss: 1.3501 - val_acc: 0.7360\n",
      "Epoch 998/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.6208 - acc: 0.9474 - val_loss: 1.3607 - val_acc: 0.7370\n",
      "Epoch 999/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6269 - acc: 0.9465 - val_loss: 1.3843 - val_acc: 0.7370\n",
      "Epoch 1000/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6061 - acc: 0.9565 - val_loss: 1.3794 - val_acc: 0.7310\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = Sequential()\n",
    "model.add(Dense(50, activation='relu',kernel_regularizer=regularizers.l1(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(Dense(25, kernel_regularizer=regularizers.l1(0.005), activation='relu'))\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer=adam,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L1_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=1000,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "L1_model_dict = L1_model.history\n",
    "plt.clf()\n",
    "\n",
    "acc_values = L1_model_dict['acc'] \n",
    "val_acc_values = L1_model_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L1')\n",
    "plt.plot(epochs, val_acc_values, 'g,', label='Validation acc L1')\n",
    "plt.title('Training & validation accuracy L2 vs regular')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6500/6500 [==============================] - 0s 46us/step\n",
      "2500/2500 [==============================] - 0s 45us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "\n",
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5590040196272044, 0.9778461538461538]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.360051566028595, 0.7472]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is about the best we've seen so far, but we were training for quite a while! Let's see if dropout regularization can do even better and/or be more efficient!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6500 samples, validate on 1000 samples\n",
      "Epoch 1/200\n",
      "6500/6500 [==============================] - 1s 100us/step - loss: 1.6939 - acc: 0.3289 - val_loss: 1.0929 - val_acc: 0.6590\n",
      "Epoch 2/200\n",
      "6500/6500 [==============================] - 0s 52us/step - loss: 1.1073 - acc: 0.5855 - val_loss: 0.7972 - val_acc: 0.7280\n",
      "Epoch 3/200\n",
      "6500/6500 [==============================] - 0s 52us/step - loss: 0.8754 - acc: 0.6768 - val_loss: 0.6929 - val_acc: 0.7350\n",
      "Epoch 4/200\n",
      "6500/6500 [==============================] - 0s 53us/step - loss: 0.7593 - acc: 0.7122 - val_loss: 0.6568 - val_acc: 0.7460\n",
      "Epoch 5/200\n",
      "6500/6500 [==============================] - 0s 54us/step - loss: 0.7132 - acc: 0.7328 - val_loss: 0.6465 - val_acc: 0.7470\n",
      "Epoch 6/200\n",
      "6500/6500 [==============================] - 0s 74us/step - loss: 0.6584 - acc: 0.7565 - val_loss: 0.6329 - val_acc: 0.7530\n",
      "Epoch 7/200\n",
      "6500/6500 [==============================] - 0s 64us/step - loss: 0.6279 - acc: 0.7709 - val_loss: 0.6392 - val_acc: 0.7560\n",
      "Epoch 8/200\n",
      "6500/6500 [==============================] - 0s 51us/step - loss: 0.6065 - acc: 0.7714 - val_loss: 0.6321 - val_acc: 0.7540\n",
      "Epoch 9/200\n",
      "6500/6500 [==============================] - 0s 56us/step - loss: 0.5625 - acc: 0.7888 - val_loss: 0.6429 - val_acc: 0.7590\n",
      "Epoch 10/200\n",
      "6500/6500 [==============================] - 0s 51us/step - loss: 0.5575 - acc: 0.7922 - val_loss: 0.6432 - val_acc: 0.7560\n",
      "Epoch 11/200\n",
      "6500/6500 [==============================] - 0s 50us/step - loss: 0.5251 - acc: 0.8052 - val_loss: 0.6435 - val_acc: 0.7640\n",
      "Epoch 12/200\n",
      "6500/6500 [==============================] - 0s 53us/step - loss: 0.5069 - acc: 0.8108 - val_loss: 0.6366 - val_acc: 0.7720\n",
      "Epoch 13/200\n",
      "6500/6500 [==============================] - 0s 51us/step - loss: 0.4914 - acc: 0.8171 - val_loss: 0.6420 - val_acc: 0.7590\n",
      "Epoch 14/200\n",
      "6500/6500 [==============================] - 0s 54us/step - loss: 0.4875 - acc: 0.8189 - val_loss: 0.6399 - val_acc: 0.7570\n",
      "Epoch 15/200\n",
      "6500/6500 [==============================] - 0s 50us/step - loss: 0.4774 - acc: 0.8211 - val_loss: 0.6326 - val_acc: 0.7760\n",
      "Epoch 16/200\n",
      "6500/6500 [==============================] - 0s 54us/step - loss: 0.4611 - acc: 0.8326 - val_loss: 0.6453 - val_acc: 0.7710\n",
      "Epoch 17/200\n",
      "6500/6500 [==============================] - 0s 57us/step - loss: 0.4527 - acc: 0.8352 - val_loss: 0.6414 - val_acc: 0.7700\n",
      "Epoch 18/200\n",
      "6500/6500 [==============================] - 0s 66us/step - loss: 0.4348 - acc: 0.8440 - val_loss: 0.6435 - val_acc: 0.7760\n",
      "Epoch 19/200\n",
      "6500/6500 [==============================] - 0s 56us/step - loss: 0.4225 - acc: 0.8488 - val_loss: 0.6550 - val_acc: 0.7670\n",
      "Epoch 20/200\n",
      "6500/6500 [==============================] - 0s 53us/step - loss: 0.4339 - acc: 0.8452 - val_loss: 0.6464 - val_acc: 0.7700\n",
      "Epoch 21/200\n",
      "6500/6500 [==============================] - 0s 65us/step - loss: 0.4155 - acc: 0.8549 - val_loss: 0.6555 - val_acc: 0.7750\n",
      "Epoch 22/200\n",
      "6500/6500 [==============================] - 0s 65us/step - loss: 0.4073 - acc: 0.8531 - val_loss: 0.6420 - val_acc: 0.7830\n",
      "Epoch 23/200\n",
      "6500/6500 [==============================] - 0s 56us/step - loss: 0.3905 - acc: 0.8631 - val_loss: 0.6390 - val_acc: 0.7740\n",
      "Epoch 24/200\n",
      "6500/6500 [==============================] - 0s 72us/step - loss: 0.3863 - acc: 0.8612 - val_loss: 0.6504 - val_acc: 0.7780\n",
      "Epoch 25/200\n",
      "6500/6500 [==============================] - 0s 53us/step - loss: 0.3850 - acc: 0.8617 - val_loss: 0.6533 - val_acc: 0.7780\n",
      "Epoch 26/200\n",
      "6500/6500 [==============================] - 0s 57us/step - loss: 0.3711 - acc: 0.8666 - val_loss: 0.6601 - val_acc: 0.7770\n",
      "Epoch 27/200\n",
      "6500/6500 [==============================] - 0s 53us/step - loss: 0.3771 - acc: 0.8700 - val_loss: 0.6586 - val_acc: 0.7690\n",
      "Epoch 28/200\n",
      "6500/6500 [==============================] - 0s 59us/step - loss: 0.3693 - acc: 0.8702 - val_loss: 0.6635 - val_acc: 0.7750\n",
      "Epoch 29/200\n",
      "6500/6500 [==============================] - 0s 58us/step - loss: 0.3529 - acc: 0.8797 - val_loss: 0.6559 - val_acc: 0.7760\n",
      "Epoch 30/200\n",
      "6500/6500 [==============================] - 0s 59us/step - loss: 0.3469 - acc: 0.8775 - val_loss: 0.6717 - val_acc: 0.7730\n",
      "Epoch 31/200\n",
      "6500/6500 [==============================] - 0s 55us/step - loss: 0.3455 - acc: 0.8785 - val_loss: 0.6673 - val_acc: 0.7840\n",
      "Epoch 32/200\n",
      "6500/6500 [==============================] - 0s 61us/step - loss: 0.3516 - acc: 0.8765 - val_loss: 0.6691 - val_acc: 0.7780\n",
      "Epoch 33/200\n",
      "6500/6500 [==============================] - 1s 87us/step - loss: 0.3296 - acc: 0.8860 - val_loss: 0.6689 - val_acc: 0.7820\n",
      "Epoch 34/200\n",
      "6500/6500 [==============================] - 0s 58us/step - loss: 0.3309 - acc: 0.8849 - val_loss: 0.6730 - val_acc: 0.7790\n",
      "Epoch 35/200\n",
      "6500/6500 [==============================] - 0s 54us/step - loss: 0.3198 - acc: 0.8857 - val_loss: 0.6770 - val_acc: 0.7770\n",
      "Epoch 36/200\n",
      "6500/6500 [==============================] - 0s 58us/step - loss: 0.3223 - acc: 0.8871 - val_loss: 0.6857 - val_acc: 0.7790\n",
      "Epoch 37/200\n",
      "6500/6500 [==============================] - 0s 56us/step - loss: 0.3291 - acc: 0.8865 - val_loss: 0.6820 - val_acc: 0.7770\n",
      "Epoch 38/200\n",
      "6500/6500 [==============================] - 0s 54us/step - loss: 0.3270 - acc: 0.8886 - val_loss: 0.6887 - val_acc: 0.7830\n",
      "Epoch 39/200\n",
      "6500/6500 [==============================] - 0s 55us/step - loss: 0.3126 - acc: 0.8914 - val_loss: 0.6917 - val_acc: 0.7750\n",
      "Epoch 40/200\n",
      "6500/6500 [==============================] - 0s 55us/step - loss: 0.3136 - acc: 0.8922 - val_loss: 0.6912 - val_acc: 0.7830\n",
      "Epoch 41/200\n",
      "6500/6500 [==============================] - 0s 58us/step - loss: 0.3014 - acc: 0.8983 - val_loss: 0.6945 - val_acc: 0.7760\n",
      "Epoch 42/200\n",
      "6500/6500 [==============================] - 0s 57us/step - loss: 0.3036 - acc: 0.8918 - val_loss: 0.7040 - val_acc: 0.7750\n",
      "Epoch 43/200\n",
      "6500/6500 [==============================] - 0s 49us/step - loss: 0.3150 - acc: 0.8905 - val_loss: 0.7000 - val_acc: 0.7770\n",
      "Epoch 44/200\n",
      "6500/6500 [==============================] - 0s 55us/step - loss: 0.3078 - acc: 0.8932 - val_loss: 0.6978 - val_acc: 0.7680\n",
      "Epoch 45/200\n",
      "6500/6500 [==============================] - 0s 75us/step - loss: 0.2956 - acc: 0.8980 - val_loss: 0.6960 - val_acc: 0.7760\n",
      "Epoch 46/200\n",
      "6500/6500 [==============================] - 0s 53us/step - loss: 0.3028 - acc: 0.8949 - val_loss: 0.7087 - val_acc: 0.7710\n",
      "Epoch 47/200\n",
      "6500/6500 [==============================] - 0s 52us/step - loss: 0.2820 - acc: 0.9046 - val_loss: 0.7056 - val_acc: 0.7700\n",
      "Epoch 48/200\n",
      "6500/6500 [==============================] - 0s 72us/step - loss: 0.2922 - acc: 0.9051 - val_loss: 0.7220 - val_acc: 0.7710\n",
      "Epoch 49/200\n",
      "6500/6500 [==============================] - 0s 58us/step - loss: 0.2979 - acc: 0.8980 - val_loss: 0.7083 - val_acc: 0.7600\n",
      "Epoch 50/200\n",
      "6500/6500 [==============================] - 0s 54us/step - loss: 0.2777 - acc: 0.9035 - val_loss: 0.7205 - val_acc: 0.7710\n",
      "Epoch 51/200\n",
      "6500/6500 [==============================] - 0s 71us/step - loss: 0.2775 - acc: 0.9028 - val_loss: 0.7290 - val_acc: 0.7750\n",
      "Epoch 52/200\n",
      "6500/6500 [==============================] - 0s 58us/step - loss: 0.2955 - acc: 0.9000 - val_loss: 0.7249 - val_acc: 0.7670\n",
      "Epoch 53/200\n",
      "6500/6500 [==============================] - 0s 53us/step - loss: 0.2750 - acc: 0.9066 - val_loss: 0.7419 - val_acc: 0.7660\n",
      "Epoch 54/200\n",
      "6500/6500 [==============================] - 0s 58us/step - loss: 0.2647 - acc: 0.9095 - val_loss: 0.7327 - val_acc: 0.7670\n",
      "Epoch 55/200\n",
      "6500/6500 [==============================] - 0s 54us/step - loss: 0.2833 - acc: 0.9082 - val_loss: 0.7287 - val_acc: 0.7660\n",
      "Epoch 56/200\n",
      "6500/6500 [==============================] - 0s 53us/step - loss: 0.2713 - acc: 0.9035 - val_loss: 0.7338 - val_acc: 0.7650\n",
      "Epoch 57/200\n",
      "6500/6500 [==============================] - 0s 57us/step - loss: 0.2717 - acc: 0.9060 - val_loss: 0.7317 - val_acc: 0.7600\n",
      "Epoch 58/200\n",
      "6500/6500 [==============================] - 0s 53us/step - loss: 0.2858 - acc: 0.9085 - val_loss: 0.7436 - val_acc: 0.7710\n",
      "Epoch 59/200\n",
      "6500/6500 [==============================] - 1s 84us/step - loss: 0.2555 - acc: 0.9166 - val_loss: 0.7403 - val_acc: 0.7710\n",
      "Epoch 60/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6500/6500 [==============================] - 0s 59us/step - loss: 0.2710 - acc: 0.9071 - val_loss: 0.7419 - val_acc: 0.7700\n",
      "Epoch 61/200\n",
      "6500/6500 [==============================] - 0s 50us/step - loss: 0.2608 - acc: 0.9102 - val_loss: 0.7368 - val_acc: 0.7650\n",
      "Epoch 62/200\n",
      "6500/6500 [==============================] - 0s 55us/step - loss: 0.2743 - acc: 0.9085 - val_loss: 0.7541 - val_acc: 0.7630\n",
      "Epoch 63/200\n",
      "6500/6500 [==============================] - 0s 55us/step - loss: 0.2664 - acc: 0.9117 - val_loss: 0.7506 - val_acc: 0.7670\n",
      "Epoch 64/200\n",
      "6500/6500 [==============================] - 0s 56us/step - loss: 0.2490 - acc: 0.9155 - val_loss: 0.7599 - val_acc: 0.7670\n",
      "Epoch 65/200\n",
      "6500/6500 [==============================] - 0s 52us/step - loss: 0.2608 - acc: 0.9140 - val_loss: 0.7661 - val_acc: 0.7690\n",
      "Epoch 66/200\n",
      "6500/6500 [==============================] - 0s 57us/step - loss: 0.2615 - acc: 0.9123 - val_loss: 0.7502 - val_acc: 0.7600\n",
      "Epoch 67/200\n",
      "6500/6500 [==============================] - 0s 58us/step - loss: 0.2405 - acc: 0.9242 - val_loss: 0.7718 - val_acc: 0.7650\n",
      "Epoch 68/200\n",
      "6500/6500 [==============================] - 0s 54us/step - loss: 0.2551 - acc: 0.9148 - val_loss: 0.7598 - val_acc: 0.7660\n",
      "Epoch 69/200\n",
      "6500/6500 [==============================] - 0s 55us/step - loss: 0.2546 - acc: 0.9160 - val_loss: 0.7721 - val_acc: 0.7650\n",
      "Epoch 70/200\n",
      "6500/6500 [==============================] - 0s 50us/step - loss: 0.2450 - acc: 0.9180 - val_loss: 0.7589 - val_acc: 0.7700\n",
      "Epoch 71/200\n",
      "6500/6500 [==============================] - 0s 64us/step - loss: 0.2475 - acc: 0.9134 - val_loss: 0.7625 - val_acc: 0.7700\n",
      "Epoch 72/200\n",
      "6500/6500 [==============================] - 0s 63us/step - loss: 0.2625 - acc: 0.9094 - val_loss: 0.7481 - val_acc: 0.7660\n",
      "Epoch 73/200\n",
      "6500/6500 [==============================] - 0s 56us/step - loss: 0.2439 - acc: 0.9182 - val_loss: 0.7546 - val_acc: 0.7650\n",
      "Epoch 74/200\n",
      "6500/6500 [==============================] - 0s 74us/step - loss: 0.2336 - acc: 0.9229 - val_loss: 0.7693 - val_acc: 0.7700\n",
      "Epoch 75/200\n",
      "6500/6500 [==============================] - 0s 60us/step - loss: 0.2388 - acc: 0.9245 - val_loss: 0.7763 - val_acc: 0.7680\n",
      "Epoch 76/200\n",
      "6500/6500 [==============================] - 0s 53us/step - loss: 0.2452 - acc: 0.9200 - val_loss: 0.7652 - val_acc: 0.7720\n",
      "Epoch 77/200\n",
      "6500/6500 [==============================] - 0s 72us/step - loss: 0.2350 - acc: 0.9208 - val_loss: 0.7701 - val_acc: 0.7700\n",
      "Epoch 78/200\n",
      "6500/6500 [==============================] - 0s 60us/step - loss: 0.2333 - acc: 0.9212 - val_loss: 0.7719 - val_acc: 0.7590\n",
      "Epoch 79/200\n",
      "6500/6500 [==============================] - 0s 52us/step - loss: 0.2408 - acc: 0.9248 - val_loss: 0.7692 - val_acc: 0.7620\n",
      "Epoch 80/200\n",
      "6500/6500 [==============================] - 0s 53us/step - loss: 0.2151 - acc: 0.9298 - val_loss: 0.7795 - val_acc: 0.7710\n",
      "Epoch 81/200\n",
      "6500/6500 [==============================] - 0s 54us/step - loss: 0.2214 - acc: 0.9255 - val_loss: 0.7738 - val_acc: 0.7690\n",
      "Epoch 82/200\n",
      "6500/6500 [==============================] - 0s 56us/step - loss: 0.2368 - acc: 0.9203 - val_loss: 0.7885 - val_acc: 0.7610\n",
      "Epoch 83/200\n",
      "6500/6500 [==============================] - 0s 53us/step - loss: 0.2328 - acc: 0.9208 - val_loss: 0.7676 - val_acc: 0.7690\n",
      "Epoch 84/200\n",
      "6500/6500 [==============================] - 0s 56us/step - loss: 0.2377 - acc: 0.9208 - val_loss: 0.7898 - val_acc: 0.7660\n",
      "Epoch 85/200\n",
      "6500/6500 [==============================] - 0s 60us/step - loss: 0.2308 - acc: 0.9192 - val_loss: 0.7894 - val_acc: 0.7660\n",
      "Epoch 86/200\n",
      "6500/6500 [==============================] - 1s 78us/step - loss: 0.2259 - acc: 0.9240 - val_loss: 0.7840 - val_acc: 0.7680\n",
      "Epoch 87/200\n",
      "6500/6500 [==============================] - 0s 55us/step - loss: 0.2455 - acc: 0.9182 - val_loss: 0.7743 - val_acc: 0.7660\n",
      "Epoch 88/200\n",
      "6500/6500 [==============================] - 0s 56us/step - loss: 0.2279 - acc: 0.9246 - val_loss: 0.8001 - val_acc: 0.7690\n",
      "Epoch 89/200\n",
      "6500/6500 [==============================] - 0s 57us/step - loss: 0.2157 - acc: 0.9300 - val_loss: 0.7959 - val_acc: 0.7700\n",
      "Epoch 90/200\n",
      "6500/6500 [==============================] - 0s 57us/step - loss: 0.2172 - acc: 0.9297 - val_loss: 0.8098 - val_acc: 0.7720\n",
      "Epoch 91/200\n",
      "6500/6500 [==============================] - 0s 54us/step - loss: 0.2323 - acc: 0.9192 - val_loss: 0.8012 - val_acc: 0.7730\n",
      "Epoch 92/200\n",
      "6500/6500 [==============================] - 0s 58us/step - loss: 0.2173 - acc: 0.9252 - val_loss: 0.7935 - val_acc: 0.7680\n",
      "Epoch 93/200\n",
      "6500/6500 [==============================] - 0s 52us/step - loss: 0.2302 - acc: 0.9218 - val_loss: 0.7927 - val_acc: 0.7650\n",
      "Epoch 94/200\n",
      "6500/6500 [==============================] - 0s 59us/step - loss: 0.2228 - acc: 0.9243 - val_loss: 0.7881 - val_acc: 0.7730\n",
      "Epoch 95/200\n",
      "6500/6500 [==============================] - 0s 57us/step - loss: 0.2304 - acc: 0.9208 - val_loss: 0.7865 - val_acc: 0.7680\n",
      "Epoch 96/200\n",
      "6500/6500 [==============================] - 0s 50us/step - loss: 0.2297 - acc: 0.9209 - val_loss: 0.7863 - val_acc: 0.7640\n",
      "Epoch 97/200\n",
      "6500/6500 [==============================] - 0s 58us/step - loss: 0.2229 - acc: 0.9266 - val_loss: 0.8037 - val_acc: 0.7710\n",
      "Epoch 98/200\n",
      "6500/6500 [==============================] - 0s 57us/step - loss: 0.2302 - acc: 0.9266 - val_loss: 0.7933 - val_acc: 0.7600\n",
      "Epoch 99/200\n",
      "6500/6500 [==============================] - 0s 54us/step - loss: 0.2088 - acc: 0.9303 - val_loss: 0.8074 - val_acc: 0.7610\n",
      "Epoch 100/200\n",
      "6500/6500 [==============================] - 0s 62us/step - loss: 0.2144 - acc: 0.9303 - val_loss: 0.8055 - val_acc: 0.7650\n",
      "Epoch 101/200\n",
      "6500/6500 [==============================] - 0s 71us/step - loss: 0.2225 - acc: 0.9249 - val_loss: 0.8045 - val_acc: 0.7600\n",
      "Epoch 102/200\n",
      "6500/6500 [==============================] - 0s 52us/step - loss: 0.2258 - acc: 0.9205 - val_loss: 0.8039 - val_acc: 0.7590\n",
      "Epoch 103/200\n",
      "6500/6500 [==============================] - 0s 57us/step - loss: 0.2152 - acc: 0.9294 - val_loss: 0.8065 - val_acc: 0.7650\n",
      "Epoch 104/200\n",
      "6500/6500 [==============================] - 0s 71us/step - loss: 0.2166 - acc: 0.9289 - val_loss: 0.8081 - val_acc: 0.7570\n",
      "Epoch 105/200\n",
      "6500/6500 [==============================] - 0s 55us/step - loss: 0.2138 - acc: 0.9320 - val_loss: 0.8109 - val_acc: 0.7630\n",
      "Epoch 106/200\n",
      "6500/6500 [==============================] - 0s 57us/step - loss: 0.2077 - acc: 0.9309 - val_loss: 0.8143 - val_acc: 0.7550\n",
      "Epoch 107/200\n",
      "6500/6500 [==============================] - 1s 78us/step - loss: 0.2141 - acc: 0.9294 - val_loss: 0.8177 - val_acc: 0.7580\n",
      "Epoch 108/200\n",
      "6500/6500 [==============================] - 0s 59us/step - loss: 0.2239 - acc: 0.9285 - val_loss: 0.8158 - val_acc: 0.7620\n",
      "Epoch 109/200\n",
      "6500/6500 [==============================] - 0s 54us/step - loss: 0.2057 - acc: 0.9288 - val_loss: 0.8211 - val_acc: 0.7620\n",
      "Epoch 110/200\n",
      "6500/6500 [==============================] - 0s 56us/step - loss: 0.2158 - acc: 0.9317 - val_loss: 0.8098 - val_acc: 0.7590\n",
      "Epoch 111/200\n",
      "6500/6500 [==============================] - 0s 61us/step - loss: 0.2080 - acc: 0.9320 - val_loss: 0.8062 - val_acc: 0.7580\n",
      "Epoch 112/200\n",
      "6500/6500 [==============================] - 0s 74us/step - loss: 0.2031 - acc: 0.9352 - val_loss: 0.8144 - val_acc: 0.7640\n",
      "Epoch 113/200\n",
      "6500/6500 [==============================] - 0s 51us/step - loss: 0.2043 - acc: 0.9326 - val_loss: 0.8315 - val_acc: 0.7650\n",
      "Epoch 114/200\n",
      "6500/6500 [==============================] - 0s 54us/step - loss: 0.2072 - acc: 0.9340 - val_loss: 0.8241 - val_acc: 0.7660\n",
      "Epoch 115/200\n",
      "6500/6500 [==============================] - 0s 54us/step - loss: 0.2099 - acc: 0.9309 - val_loss: 0.8214 - val_acc: 0.7590\n",
      "Epoch 116/200\n",
      "6500/6500 [==============================] - 0s 60us/step - loss: 0.2214 - acc: 0.9254 - val_loss: 0.8074 - val_acc: 0.7630\n",
      "Epoch 117/200\n",
      "6500/6500 [==============================] - 0s 59us/step - loss: 0.2064 - acc: 0.9289 - val_loss: 0.8239 - val_acc: 0.7650\n",
      "Epoch 118/200\n",
      "6500/6500 [==============================] - 0s 54us/step - loss: 0.2097 - acc: 0.9346 - val_loss: 0.8187 - val_acc: 0.7660\n",
      "Epoch 119/200\n",
      "6500/6500 [==============================] - 0s 55us/step - loss: 0.2058 - acc: 0.9323 - val_loss: 0.8293 - val_acc: 0.7600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/200\n",
      "6500/6500 [==============================] - 0s 51us/step - loss: 0.2055 - acc: 0.9286 - val_loss: 0.8240 - val_acc: 0.7610\n",
      "Epoch 121/200\n",
      "6500/6500 [==============================] - 0s 57us/step - loss: 0.2129 - acc: 0.9309 - val_loss: 0.8340 - val_acc: 0.7640\n",
      "Epoch 122/200\n",
      "6500/6500 [==============================] - 0s 52us/step - loss: 0.1971 - acc: 0.9308 - val_loss: 0.8185 - val_acc: 0.7600\n",
      "Epoch 123/200\n",
      "6500/6500 [==============================] - 0s 58us/step - loss: 0.2126 - acc: 0.9323 - val_loss: 0.8146 - val_acc: 0.7650\n",
      "Epoch 124/200\n",
      "6500/6500 [==============================] - 0s 66us/step - loss: 0.1992 - acc: 0.9369 - val_loss: 0.8249 - val_acc: 0.7680\n",
      "Epoch 125/200\n",
      "6500/6500 [==============================] - 0s 56us/step - loss: 0.2025 - acc: 0.9325 - val_loss: 0.8340 - val_acc: 0.7660\n",
      "Epoch 126/200\n",
      "6500/6500 [==============================] - 0s 69us/step - loss: 0.2020 - acc: 0.9323 - val_loss: 0.8369 - val_acc: 0.7650\n",
      "Epoch 127/200\n",
      "6500/6500 [==============================] - 0s 56us/step - loss: 0.2023 - acc: 0.9360 - val_loss: 0.8185 - val_acc: 0.7670\n",
      "Epoch 128/200\n",
      "6500/6500 [==============================] - 0s 57us/step - loss: 0.2025 - acc: 0.9314 - val_loss: 0.8121 - val_acc: 0.7670\n",
      "Epoch 129/200\n",
      "6500/6500 [==============================] - 0s 65us/step - loss: 0.2087 - acc: 0.9323 - val_loss: 0.8112 - val_acc: 0.7650\n",
      "Epoch 130/200\n",
      "6500/6500 [==============================] - 0s 62us/step - loss: 0.2090 - acc: 0.9328 - val_loss: 0.8228 - val_acc: 0.7710\n",
      "Epoch 131/200\n",
      "6500/6500 [==============================] - 0s 56us/step - loss: 0.2053 - acc: 0.9325 - val_loss: 0.8160 - val_acc: 0.7660\n",
      "Epoch 132/200\n",
      "6500/6500 [==============================] - 0s 53us/step - loss: 0.1924 - acc: 0.9368 - val_loss: 0.8098 - val_acc: 0.7690\n",
      "Epoch 133/200\n",
      "6500/6500 [==============================] - 0s 58us/step - loss: 0.2016 - acc: 0.9348 - val_loss: 0.8264 - val_acc: 0.7700\n",
      "Epoch 134/200\n",
      "6500/6500 [==============================] - 0s 57us/step - loss: 0.1925 - acc: 0.9360 - val_loss: 0.8270 - val_acc: 0.7620\n",
      "Epoch 135/200\n",
      "6500/6500 [==============================] - 0s 52us/step - loss: 0.1891 - acc: 0.9374 - val_loss: 0.8346 - val_acc: 0.7670\n",
      "Epoch 136/200\n",
      "6500/6500 [==============================] - 0s 56us/step - loss: 0.1800 - acc: 0.9397 - val_loss: 0.8590 - val_acc: 0.7600\n",
      "Epoch 137/200\n",
      "6500/6500 [==============================] - 0s 51us/step - loss: 0.1885 - acc: 0.9388 - val_loss: 0.8424 - val_acc: 0.7660\n",
      "Epoch 138/200\n",
      "6500/6500 [==============================] - 0s 75us/step - loss: 0.1892 - acc: 0.9366 - val_loss: 0.8326 - val_acc: 0.7630\n",
      "Epoch 139/200\n",
      "6500/6500 [==============================] - 0s 60us/step - loss: 0.1842 - acc: 0.9383 - val_loss: 0.8401 - val_acc: 0.7680\n",
      "Epoch 140/200\n",
      "6500/6500 [==============================] - 0s 53us/step - loss: 0.1837 - acc: 0.9418 - val_loss: 0.8474 - val_acc: 0.7670\n",
      "Epoch 141/200\n",
      "6500/6500 [==============================] - 0s 56us/step - loss: 0.1937 - acc: 0.9343 - val_loss: 0.8512 - val_acc: 0.7690\n",
      "Epoch 142/200\n",
      "6500/6500 [==============================] - 0s 51us/step - loss: 0.1962 - acc: 0.9328 - val_loss: 0.8528 - val_acc: 0.7660\n",
      "Epoch 143/200\n",
      "6500/6500 [==============================] - 0s 57us/step - loss: 0.1947 - acc: 0.9355 - val_loss: 0.8395 - val_acc: 0.7670\n",
      "Epoch 144/200\n",
      "6500/6500 [==============================] - 0s 53us/step - loss: 0.1884 - acc: 0.9351 - val_loss: 0.8352 - val_acc: 0.7760\n",
      "Epoch 145/200\n",
      "6500/6500 [==============================] - 0s 55us/step - loss: 0.2023 - acc: 0.9348 - val_loss: 0.8413 - val_acc: 0.7640\n",
      "Epoch 146/200\n",
      "6500/6500 [==============================] - 0s 57us/step - loss: 0.1838 - acc: 0.9406 - val_loss: 0.8394 - val_acc: 0.7700\n",
      "Epoch 147/200\n",
      "6500/6500 [==============================] - 0s 57us/step - loss: 0.1878 - acc: 0.9349 - val_loss: 0.8443 - val_acc: 0.7600\n",
      "Epoch 148/200\n",
      "6500/6500 [==============================] - 0s 58us/step - loss: 0.1856 - acc: 0.9414 - val_loss: 0.8338 - val_acc: 0.7600\n",
      "Epoch 149/200\n",
      "6500/6500 [==============================] - 0s 54us/step - loss: 0.1873 - acc: 0.9392 - val_loss: 0.8457 - val_acc: 0.7590\n",
      "Epoch 150/200\n",
      "6500/6500 [==============================] - 0s 73us/step - loss: 0.1888 - acc: 0.9369 - val_loss: 0.8421 - val_acc: 0.7670\n",
      "Epoch 151/200\n",
      "6500/6500 [==============================] - 0s 52us/step - loss: 0.1875 - acc: 0.9391 - val_loss: 0.8300 - val_acc: 0.7640\n",
      "Epoch 152/200\n",
      "6500/6500 [==============================] - 0s 55us/step - loss: 0.1910 - acc: 0.9375 - val_loss: 0.8374 - val_acc: 0.7600\n",
      "Epoch 153/200\n",
      "6500/6500 [==============================] - 0s 76us/step - loss: 0.1905 - acc: 0.9360 - val_loss: 0.8437 - val_acc: 0.7630\n",
      "Epoch 154/200\n",
      "6500/6500 [==============================] - 0s 71us/step - loss: 0.1953 - acc: 0.9322 - val_loss: 0.8517 - val_acc: 0.7620\n",
      "Epoch 155/200\n",
      "6500/6500 [==============================] - 1s 84us/step - loss: 0.1831 - acc: 0.9398 - val_loss: 0.8562 - val_acc: 0.7600\n",
      "Epoch 156/200\n",
      "6500/6500 [==============================] - 1s 91us/step - loss: 0.1850 - acc: 0.9395 - val_loss: 0.8626 - val_acc: 0.7690\n",
      "Epoch 157/200\n",
      "6500/6500 [==============================] - 0s 64us/step - loss: 0.1947 - acc: 0.9338 - val_loss: 0.8575 - val_acc: 0.7630\n",
      "Epoch 158/200\n",
      "6500/6500 [==============================] - 0s 51us/step - loss: 0.1798 - acc: 0.9412 - val_loss: 0.8713 - val_acc: 0.7690\n",
      "Epoch 159/200\n",
      "6500/6500 [==============================] - 0s 50us/step - loss: 0.1862 - acc: 0.9389 - val_loss: 0.8546 - val_acc: 0.7620\n",
      "Epoch 160/200\n",
      "6500/6500 [==============================] - 0s 52us/step - loss: 0.1774 - acc: 0.9397 - val_loss: 0.8511 - val_acc: 0.7620\n",
      "Epoch 161/200\n",
      "6500/6500 [==============================] - 0s 50us/step - loss: 0.1871 - acc: 0.9371 - val_loss: 0.8354 - val_acc: 0.7580\n",
      "Epoch 162/200\n",
      "6500/6500 [==============================] - 0s 50us/step - loss: 0.1945 - acc: 0.9375 - val_loss: 0.8417 - val_acc: 0.7680\n",
      "Epoch 163/200\n",
      "6500/6500 [==============================] - 0s 54us/step - loss: 0.1882 - acc: 0.9386 - val_loss: 0.8510 - val_acc: 0.7670\n",
      "Epoch 164/200\n",
      "6500/6500 [==============================] - 1s 81us/step - loss: 0.1762 - acc: 0.9449 - val_loss: 0.8598 - val_acc: 0.7640\n",
      "Epoch 165/200\n",
      "6500/6500 [==============================] - 0s 56us/step - loss: 0.1929 - acc: 0.9371 - val_loss: 0.8419 - val_acc: 0.7610\n",
      "Epoch 166/200\n",
      "6500/6500 [==============================] - 0s 54us/step - loss: 0.1824 - acc: 0.9392 - val_loss: 0.8510 - val_acc: 0.7610\n",
      "Epoch 167/200\n",
      "6500/6500 [==============================] - 0s 54us/step - loss: 0.1969 - acc: 0.9337 - val_loss: 0.8389 - val_acc: 0.7620\n",
      "Epoch 168/200\n",
      "6500/6500 [==============================] - 0s 51us/step - loss: 0.1917 - acc: 0.9395 - val_loss: 0.8574 - val_acc: 0.7600\n",
      "Epoch 169/200\n",
      "6500/6500 [==============================] - 0s 48us/step - loss: 0.1836 - acc: 0.9383 - val_loss: 0.8512 - val_acc: 0.7600\n",
      "Epoch 170/200\n",
      "6500/6500 [==============================] - 0s 53us/step - loss: 0.1804 - acc: 0.9414 - val_loss: 0.8465 - val_acc: 0.7720\n",
      "Epoch 171/200\n",
      "6500/6500 [==============================] - 0s 52us/step - loss: 0.1836 - acc: 0.9380 - val_loss: 0.8638 - val_acc: 0.7580\n",
      "Epoch 172/200\n",
      "6500/6500 [==============================] - 0s 55us/step - loss: 0.1886 - acc: 0.9351 - val_loss: 0.8585 - val_acc: 0.7620\n",
      "Epoch 173/200\n",
      "6500/6500 [==============================] - 0s 50us/step - loss: 0.1759 - acc: 0.9412 - val_loss: 0.8638 - val_acc: 0.7650\n",
      "Epoch 174/200\n",
      "6500/6500 [==============================] - 0s 55us/step - loss: 0.1789 - acc: 0.9414 - val_loss: 0.8669 - val_acc: 0.7630\n",
      "Epoch 175/200\n",
      "6500/6500 [==============================] - 0s 49us/step - loss: 0.1847 - acc: 0.9366 - val_loss: 0.8796 - val_acc: 0.7650\n",
      "Epoch 176/200\n",
      "6500/6500 [==============================] - 0s 63us/step - loss: 0.1867 - acc: 0.9388 - val_loss: 0.8695 - val_acc: 0.7620\n",
      "Epoch 177/200\n",
      "6500/6500 [==============================] - 1s 77us/step - loss: 0.1861 - acc: 0.9368 - val_loss: 0.8565 - val_acc: 0.7540\n",
      "Epoch 178/200\n",
      "6500/6500 [==============================] - 0s 53us/step - loss: 0.1790 - acc: 0.9405 - val_loss: 0.8392 - val_acc: 0.7610\n",
      "Epoch 179/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6500/6500 [==============================] - 0s 68us/step - loss: 0.1780 - acc: 0.9395 - val_loss: 0.8710 - val_acc: 0.7550\n",
      "Epoch 180/200\n",
      "6500/6500 [==============================] - 0s 58us/step - loss: 0.1679 - acc: 0.9466 - val_loss: 0.8702 - val_acc: 0.7580\n",
      "Epoch 181/200\n",
      "6500/6500 [==============================] - 0s 48us/step - loss: 0.1915 - acc: 0.9385 - val_loss: 0.8712 - val_acc: 0.7600\n",
      "Epoch 182/200\n",
      "6500/6500 [==============================] - 0s 67us/step - loss: 0.1755 - acc: 0.9400 - val_loss: 0.8730 - val_acc: 0.7580\n",
      "Epoch 183/200\n",
      "6500/6500 [==============================] - 0s 60us/step - loss: 0.1913 - acc: 0.9348 - val_loss: 0.8761 - val_acc: 0.7610\n",
      "Epoch 184/200\n",
      "6500/6500 [==============================] - 0s 56us/step - loss: 0.1893 - acc: 0.9412 - val_loss: 0.8694 - val_acc: 0.7580\n",
      "Epoch 185/200\n",
      "6500/6500 [==============================] - 0s 53us/step - loss: 0.1823 - acc: 0.9377 - val_loss: 0.8584 - val_acc: 0.7610\n",
      "Epoch 186/200\n",
      "6500/6500 [==============================] - 0s 55us/step - loss: 0.1860 - acc: 0.9366 - val_loss: 0.8446 - val_acc: 0.7680\n",
      "Epoch 187/200\n",
      "6500/6500 [==============================] - 0s 55us/step - loss: 0.1631 - acc: 0.9474 - val_loss: 0.8635 - val_acc: 0.7640\n",
      "Epoch 188/200\n",
      "6500/6500 [==============================] - 0s 57us/step - loss: 0.1725 - acc: 0.9440 - val_loss: 0.8487 - val_acc: 0.7560\n",
      "Epoch 189/200\n",
      "6500/6500 [==============================] - 0s 60us/step - loss: 0.1799 - acc: 0.9368 - val_loss: 0.8523 - val_acc: 0.7630\n",
      "Epoch 190/200\n",
      "6500/6500 [==============================] - 0s 73us/step - loss: 0.1704 - acc: 0.9448 - val_loss: 0.8524 - val_acc: 0.7620\n",
      "Epoch 191/200\n",
      "6500/6500 [==============================] - 0s 67us/step - loss: 0.1832 - acc: 0.9408 - val_loss: 0.8351 - val_acc: 0.7640\n",
      "Epoch 192/200\n",
      "6500/6500 [==============================] - 0s 59us/step - loss: 0.1767 - acc: 0.9435 - val_loss: 0.8399 - val_acc: 0.7580\n",
      "Epoch 193/200\n",
      "6500/6500 [==============================] - 0s 57us/step - loss: 0.1850 - acc: 0.9395 - val_loss: 0.8448 - val_acc: 0.7680\n",
      "Epoch 194/200\n",
      "6500/6500 [==============================] - 0s 52us/step - loss: 0.1766 - acc: 0.9408 - val_loss: 0.8457 - val_acc: 0.7640\n",
      "Epoch 195/200\n",
      "6500/6500 [==============================] - 0s 59us/step - loss: 0.1743 - acc: 0.9400 - val_loss: 0.8494 - val_acc: 0.7710\n",
      "Epoch 196/200\n",
      "6500/6500 [==============================] - 0s 52us/step - loss: 0.1714 - acc: 0.9423 - val_loss: 0.8568 - val_acc: 0.7640\n",
      "Epoch 197/200\n",
      "6500/6500 [==============================] - 0s 54us/step - loss: 0.1845 - acc: 0.9409 - val_loss: 0.8474 - val_acc: 0.7620\n",
      "Epoch 198/200\n",
      "6500/6500 [==============================] - 0s 51us/step - loss: 0.1766 - acc: 0.9423 - val_loss: 0.8528 - val_acc: 0.7560\n",
      "Epoch 199/200\n",
      "6500/6500 [==============================] - 0s 55us/step - loss: 0.1745 - acc: 0.9437 - val_loss: 0.8635 - val_acc: 0.7660\n",
      "Epoch 200/200\n",
      "6500/6500 [==============================] - 0s 54us/step - loss: 0.1621 - acc: 0.9472 - val_loss: 0.8843 - val_acc: 0.7670\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dropout\n",
    "\n",
    "random.seed(123)\n",
    "model = Sequential()\n",
    "model.add(Dropout(0.3, input_shape=(2000,)))\n",
    "model.add(Dense(50, activation='relu')) #2 hidden layers\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(25, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer=adam,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "dropout_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=200,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6500/6500 [==============================] - 0s 56us/step\n",
      "2500/2500 [==============================] - 0s 49us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.007336052244896284, 0.9990769230769231]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.816141248703003, 0.7748]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see here that the validation performance has improved again! the variance did become higher again compared to L1-regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigger Data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture, one of the solutions to high variance was just getting more data. We actually *have* more data, but took a subset of 10,000 units before. Let's now quadruple our data set, and see what happens. Note that we are really just lucky here, and getting more data isn't always possible, but this is a useful exercise in order to understand the power of big data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Bank_complaints.csv')\n",
    "random.seed(123)\n",
    "df = df.sample(40000)\n",
    "df.index = range(40000)\n",
    "product = df[\"Product\"]\n",
    "complaints = df[\"Consumer complaint narrative\"]\n",
    "\n",
    "#one-hot encoding of the complaints\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(complaints)\n",
    "sequences = tokenizer.texts_to_sequences(complaints)\n",
    "one_hot_results= tokenizer.texts_to_matrix(complaints, mode='binary')\n",
    "word_index = tokenizer.word_index\n",
    "np.shape(one_hot_results)\n",
    "\n",
    "#one-hot encoding of products\n",
    "le = LabelEncoder()\n",
    "le.fit(product)\n",
    "list(le.classes_)\n",
    "product_cat = le.transform(product) \n",
    "product_onehot = to_categorical(product_cat)\n",
    "\n",
    "# train test split\n",
    "test_index = random.sample(range(1,40000), 4000)\n",
    "test = one_hot_results[test_index]\n",
    "train = np.delete(one_hot_results, test_index, 0)\n",
    "label_test = product_onehot[test_index]\n",
    "label_train = np.delete(product_onehot, test_index, 0)\n",
    "\n",
    "#Validation set\n",
    "random.seed(123)\n",
    "val = train[:3000]\n",
    "train_final = train[3000:]\n",
    "label_val = label_train[:3000]\n",
    "label_train_final = label_train[3000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33000 samples, validate on 3000 samples\n",
      "Epoch 1/120\n",
      "33000/33000 [==============================] - 1s 44us/step - loss: 0.7156 - acc: 0.7410 - val_loss: 0.5288 - val_acc: 0.8090\n",
      "Epoch 2/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.4154 - acc: 0.8549 - val_loss: 0.5118 - val_acc: 0.8207\n",
      "Epoch 3/120\n",
      "33000/33000 [==============================] - 1s 27us/step - loss: 0.3527 - acc: 0.8752 - val_loss: 0.5348 - val_acc: 0.8180\n",
      "Epoch 4/120\n",
      "33000/33000 [==============================] - 1s 26us/step - loss: 0.3103 - acc: 0.8922 - val_loss: 0.5605 - val_acc: 0.8173\n",
      "Epoch 5/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.2710 - acc: 0.9058 - val_loss: 0.5819 - val_acc: 0.8130\n",
      "Epoch 6/120\n",
      "33000/33000 [==============================] - 1s 34us/step - loss: 0.2362 - acc: 0.9202 - val_loss: 0.6187 - val_acc: 0.8087\n",
      "Epoch 7/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.2044 - acc: 0.9341 - val_loss: 0.6460 - val_acc: 0.8110\n",
      "Epoch 8/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.1734 - acc: 0.9486 - val_loss: 0.6861 - val_acc: 0.8027\n",
      "Epoch 9/120\n",
      "33000/33000 [==============================] - 1s 28us/step - loss: 0.1465 - acc: 0.9575 - val_loss: 0.7250 - val_acc: 0.8027\n",
      "Epoch 10/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.1235 - acc: 0.9674 - val_loss: 0.7724 - val_acc: 0.7983\n",
      "Epoch 11/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.1034 - acc: 0.9744 - val_loss: 0.8053 - val_acc: 0.7990\n",
      "Epoch 12/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.0854 - acc: 0.9816 - val_loss: 0.8617 - val_acc: 0.7950\n",
      "Epoch 13/120\n",
      "33000/33000 [==============================] - 1s 28us/step - loss: 0.0713 - acc: 0.9859 - val_loss: 0.9075 - val_acc: 0.7977\n",
      "Epoch 14/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.0585 - acc: 0.9895 - val_loss: 0.9591 - val_acc: 0.7933\n",
      "Epoch 15/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.0489 - acc: 0.9921 - val_loss: 1.0037 - val_acc: 0.7950\n",
      "Epoch 16/120\n",
      "33000/33000 [==============================] - 1s 36us/step - loss: 0.0407 - acc: 0.9942 - val_loss: 1.0617 - val_acc: 0.7907\n",
      "Epoch 17/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.0346 - acc: 0.9955 - val_loss: 1.1034 - val_acc: 0.7910\n",
      "Epoch 18/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.0297 - acc: 0.9961 - val_loss: 1.1361 - val_acc: 0.7943\n",
      "Epoch 19/120\n",
      "33000/33000 [==============================] - 1s 28us/step - loss: 0.0249 - acc: 0.9970 - val_loss: 1.1966 - val_acc: 0.7897\n",
      "Epoch 20/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.0212 - acc: 0.9976 - val_loss: 1.2284 - val_acc: 0.7877\n",
      "Epoch 21/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.0186 - acc: 0.9979 - val_loss: 1.2572 - val_acc: 0.7877\n",
      "Epoch 22/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.0164 - acc: 0.9982 - val_loss: 1.2986 - val_acc: 0.7863\n",
      "Epoch 23/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.0151 - acc: 0.9983 - val_loss: 1.3242 - val_acc: 0.7873\n",
      "Epoch 24/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.0131 - acc: 0.9985 - val_loss: 1.3632 - val_acc: 0.7890\n",
      "Epoch 25/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.0128 - acc: 0.9985 - val_loss: 1.3786 - val_acc: 0.7833\n",
      "Epoch 26/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.0115 - acc: 0.9987 - val_loss: 1.4065 - val_acc: 0.7900\n",
      "Epoch 27/120\n",
      "33000/33000 [==============================] - 1s 28us/step - loss: 0.0114 - acc: 0.9987 - val_loss: 1.4332 - val_acc: 0.7870\n",
      "Epoch 28/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.0104 - acc: 0.9988 - val_loss: 1.4510 - val_acc: 0.7840\n",
      "Epoch 29/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.0094 - acc: 0.9988 - val_loss: 1.4715 - val_acc: 0.7837\n",
      "Epoch 30/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.0091 - acc: 0.9989 - val_loss: 1.5048 - val_acc: 0.7850\n",
      "Epoch 31/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.0091 - acc: 0.9988 - val_loss: 1.5065 - val_acc: 0.7840\n",
      "Epoch 32/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.0080 - acc: 0.9989 - val_loss: 1.5322 - val_acc: 0.7823\n",
      "Epoch 33/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.0088 - acc: 0.9989 - val_loss: 1.5507 - val_acc: 0.7823\n",
      "Epoch 34/120\n",
      "33000/33000 [==============================] - 1s 28us/step - loss: 0.0087 - acc: 0.9989 - val_loss: 1.5729 - val_acc: 0.7810\n",
      "Epoch 35/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.0078 - acc: 0.9990 - val_loss: 1.5652 - val_acc: 0.7837\n",
      "Epoch 36/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.0081 - acc: 0.9989 - val_loss: 1.5604 - val_acc: 0.7853\n",
      "Epoch 37/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.0197 - acc: 0.9952 - val_loss: 1.6229 - val_acc: 0.7787\n",
      "Epoch 38/120\n",
      "33000/33000 [==============================] - 1s 34us/step - loss: 0.0725 - acc: 0.9775 - val_loss: 1.5411 - val_acc: 0.7763\n",
      "Epoch 39/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.0381 - acc: 0.9887 - val_loss: 1.5879 - val_acc: 0.7777\n",
      "Epoch 40/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.0139 - acc: 0.9977 - val_loss: 1.6119 - val_acc: 0.7800\n",
      "Epoch 41/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.0081 - acc: 0.9990 - val_loss: 1.6243 - val_acc: 0.7807\n",
      "Epoch 42/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.0073 - acc: 0.9991 - val_loss: 1.6509 - val_acc: 0.7813\n",
      "Epoch 43/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.0071 - acc: 0.9991 - val_loss: 1.6601 - val_acc: 0.7787\n",
      "Epoch 44/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.0058 - acc: 0.9992 - val_loss: 1.6807 - val_acc: 0.7823\n",
      "Epoch 45/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.0066 - acc: 0.9992 - val_loss: 1.6907 - val_acc: 0.7800\n",
      "Epoch 46/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.0063 - acc: 0.9992 - val_loss: 1.6935 - val_acc: 0.7807\n",
      "Epoch 47/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.0063 - acc: 0.9991 - val_loss: 1.6965 - val_acc: 0.7780\n",
      "Epoch 48/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.0068 - acc: 0.9990 - val_loss: 1.7033 - val_acc: 0.7833\n",
      "Epoch 49/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.0059 - acc: 0.9991 - val_loss: 1.7280 - val_acc: 0.7807\n",
      "Epoch 50/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.0064 - acc: 0.9989 - val_loss: 1.7317 - val_acc: 0.7787\n",
      "Epoch 51/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.0067 - acc: 0.9989 - val_loss: 1.7455 - val_acc: 0.7803\n",
      "Epoch 52/120\n",
      "33000/33000 [==============================] - 1s 34us/step - loss: 0.0062 - acc: 0.9990 - val_loss: 1.7586 - val_acc: 0.7833\n",
      "Epoch 53/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.0059 - acc: 0.9991 - val_loss: 1.7573 - val_acc: 0.7823\n",
      "Epoch 54/120\n",
      "33000/33000 [==============================] - 1s 27us/step - loss: 0.0059 - acc: 0.9990 - val_loss: 1.7705 - val_acc: 0.7803\n",
      "Epoch 55/120\n",
      "33000/33000 [==============================] - 1s 34us/step - loss: 0.0060 - acc: 0.9992 - val_loss: 1.7785 - val_acc: 0.7817\n",
      "Epoch 56/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.0062 - acc: 0.9991 - val_loss: 1.7757 - val_acc: 0.7803\n",
      "Epoch 57/120\n",
      "33000/33000 [==============================] - 1s 28us/step - loss: 0.0059 - acc: 0.9991 - val_loss: 1.7905 - val_acc: 0.7787\n",
      "Epoch 58/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.0054 - acc: 0.9992 - val_loss: 1.7906 - val_acc: 0.7783\n",
      "Epoch 59/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.0060 - acc: 0.9991 - val_loss: 1.8060 - val_acc: 0.7793\n",
      "Epoch 60/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.0060 - acc: 0.9991 - val_loss: 1.7897 - val_acc: 0.7807\n",
      "Epoch 61/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.0064 - acc: 0.9990 - val_loss: 1.8275 - val_acc: 0.7793\n",
      "Epoch 62/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.0058 - acc: 0.9990 - val_loss: 1.8243 - val_acc: 0.7823\n",
      "Epoch 63/120\n",
      "33000/33000 [==============================] - 1s 27us/step - loss: 0.0069 - acc: 0.9989 - val_loss: 1.8333 - val_acc: 0.7810\n",
      "Epoch 64/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.0066 - acc: 0.9990 - val_loss: 1.8493 - val_acc: 0.7817\n",
      "Epoch 65/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.0060 - acc: 0.9991 - val_loss: 1.8783 - val_acc: 0.7780\n",
      "Epoch 66/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.0142 - acc: 0.9969 - val_loss: 1.9317 - val_acc: 0.7730\n",
      "Epoch 67/120\n",
      "33000/33000 [==============================] - 1s 28us/step - loss: 0.0403 - acc: 0.9873 - val_loss: 1.8478 - val_acc: 0.7820\n",
      "Epoch 68/120\n",
      "33000/33000 [==============================] - 1s 28us/step - loss: 0.0301 - acc: 0.9908 - val_loss: 1.8343 - val_acc: 0.7830\n",
      "Epoch 69/120\n",
      "33000/33000 [==============================] - 1s 28us/step - loss: 0.0108 - acc: 0.9976 - val_loss: 1.8304 - val_acc: 0.7820\n",
      "Epoch 70/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.0059 - acc: 0.9992 - val_loss: 1.8422 - val_acc: 0.7847\n",
      "Epoch 71/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.0052 - acc: 0.9992 - val_loss: 1.8488 - val_acc: 0.7820\n",
      "Epoch 72/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.0053 - acc: 0.9991 - val_loss: 1.8711 - val_acc: 0.7847\n",
      "Epoch 73/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.0050 - acc: 0.9993 - val_loss: 1.8776 - val_acc: 0.7837\n",
      "Epoch 74/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.0054 - acc: 0.9992 - val_loss: 1.8913 - val_acc: 0.7817\n",
      "Epoch 75/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.0050 - acc: 0.9992 - val_loss: 1.8801 - val_acc: 0.7833\n",
      "Epoch 76/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.0050 - acc: 0.9991 - val_loss: 1.8971 - val_acc: 0.7827\n",
      "Epoch 77/120\n",
      "33000/33000 [==============================] - 1s 34us/step - loss: 0.0053 - acc: 0.9991 - val_loss: 1.8882 - val_acc: 0.7837\n",
      "Epoch 78/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.0053 - acc: 0.9992 - val_loss: 1.8972 - val_acc: 0.7837\n",
      "Epoch 79/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.0053 - acc: 0.9992 - val_loss: 1.9086 - val_acc: 0.7853\n",
      "Epoch 80/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.0057 - acc: 0.9991 - val_loss: 1.9170 - val_acc: 0.7837\n",
      "Epoch 81/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.0051 - acc: 0.9992 - val_loss: 1.9182 - val_acc: 0.7833\n",
      "Epoch 82/120\n",
      "33000/33000 [==============================] - 1s 28us/step - loss: 0.0048 - acc: 0.9992 - val_loss: 1.9273 - val_acc: 0.7857\n",
      "Epoch 83/120\n",
      "33000/33000 [==============================] - 1s 26us/step - loss: 0.0054 - acc: 0.9992 - val_loss: 1.9165 - val_acc: 0.7857\n",
      "Epoch 84/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.0049 - acc: 0.9992 - val_loss: 1.9280 - val_acc: 0.7863\n",
      "Epoch 85/120\n",
      "33000/33000 [==============================] - 1s 35us/step - loss: 0.0052 - acc: 0.9992 - val_loss: 1.9199 - val_acc: 0.7837\n",
      "Epoch 86/120\n",
      "33000/33000 [==============================] - 1s 28us/step - loss: 0.0053 - acc: 0.9991 - val_loss: 1.9478 - val_acc: 0.7843\n",
      "Epoch 87/120\n",
      "33000/33000 [==============================] - 1s 28us/step - loss: 0.0049 - acc: 0.9992 - val_loss: 1.9596 - val_acc: 0.7793\n",
      "Epoch 88/120\n",
      "33000/33000 [==============================] - 1s 28us/step - loss: 0.0055 - acc: 0.9990 - val_loss: 1.9676 - val_acc: 0.7850\n",
      "Epoch 89/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.0060 - acc: 0.9991 - val_loss: 1.9284 - val_acc: 0.7830\n",
      "Epoch 90/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.0051 - acc: 0.9993 - val_loss: 1.9565 - val_acc: 0.7837\n",
      "Epoch 91/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.0058 - acc: 0.9991 - val_loss: 1.9716 - val_acc: 0.7820\n",
      "Epoch 92/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.0050 - acc: 0.9992 - val_loss: 1.9545 - val_acc: 0.7833\n",
      "Epoch 93/120\n",
      "33000/33000 [==============================] - 1s 28us/step - loss: 0.0060 - acc: 0.9990 - val_loss: 1.9551 - val_acc: 0.7857\n",
      "Epoch 94/120\n",
      "33000/33000 [==============================] - 1s 28us/step - loss: 0.0058 - acc: 0.9990 - val_loss: 1.9771 - val_acc: 0.7807\n",
      "Epoch 95/120\n",
      "33000/33000 [==============================] - 1s 35us/step - loss: 0.0188 - acc: 0.9948 - val_loss: 1.9515 - val_acc: 0.7710\n",
      "Epoch 96/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.0464 - acc: 0.9864 - val_loss: 1.9367 - val_acc: 0.7747\n",
      "Epoch 97/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.0220 - acc: 0.9940 - val_loss: 1.8781 - val_acc: 0.7737\n",
      "Epoch 98/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.0083 - acc: 0.9986 - val_loss: 1.8989 - val_acc: 0.7763\n",
      "Epoch 99/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.0048 - acc: 0.9994 - val_loss: 1.9179 - val_acc: 0.7760\n",
      "Epoch 100/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.0048 - acc: 0.9993 - val_loss: 1.9325 - val_acc: 0.7767\n",
      "Epoch 101/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.0049 - acc: 0.9993 - val_loss: 1.9424 - val_acc: 0.7777\n",
      "Epoch 102/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.0045 - acc: 0.9993 - val_loss: 1.9581 - val_acc: 0.7773\n",
      "Epoch 103/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.0041 - acc: 0.9993 - val_loss: 1.9691 - val_acc: 0.7767\n",
      "Epoch 104/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.0046 - acc: 0.9993 - val_loss: 1.9679 - val_acc: 0.7770\n",
      "Epoch 105/120\n",
      "33000/33000 [==============================] - 1s 34us/step - loss: 0.0047 - acc: 0.9993 - val_loss: 1.9779 - val_acc: 0.7787\n",
      "Epoch 106/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.0041 - acc: 0.9992 - val_loss: 1.9850 - val_acc: 0.7753\n",
      "Epoch 107/120\n",
      "33000/33000 [==============================] - 1s 28us/step - loss: 0.0045 - acc: 0.9992 - val_loss: 1.9901 - val_acc: 0.7763\n",
      "Epoch 108/120\n",
      "33000/33000 [==============================] - 1s 28us/step - loss: 0.0045 - acc: 0.9993 - val_loss: 1.9982 - val_acc: 0.7727\n",
      "Epoch 109/120\n",
      "33000/33000 [==============================] - 1s 34us/step - loss: 0.0044 - acc: 0.9993 - val_loss: 1.9936 - val_acc: 0.7780\n",
      "Epoch 110/120\n",
      "33000/33000 [==============================] - 1s 34us/step - loss: 0.0048 - acc: 0.9991 - val_loss: 2.0094 - val_acc: 0.7760\n",
      "Epoch 111/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.0043 - acc: 0.9994 - val_loss: 2.0035 - val_acc: 0.7773\n",
      "Epoch 112/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.0052 - acc: 0.9992 - val_loss: 1.9959 - val_acc: 0.7783\n",
      "Epoch 113/120\n",
      "33000/33000 [==============================] - 1s 28us/step - loss: 0.0046 - acc: 0.9993 - val_loss: 2.0273 - val_acc: 0.7737\n",
      "Epoch 114/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.0049 - acc: 0.9992 - val_loss: 2.0046 - val_acc: 0.7763\n",
      "Epoch 115/120\n",
      "33000/33000 [==============================] - 1s 35us/step - loss: 0.0045 - acc: 0.9993 - val_loss: 2.0077 - val_acc: 0.7783\n",
      "Epoch 116/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.0047 - acc: 0.9992 - val_loss: 2.0297 - val_acc: 0.7753\n",
      "Epoch 117/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.0056 - acc: 0.9992 - val_loss: 2.0461 - val_acc: 0.7743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/120\n",
      "33000/33000 [==============================] - 1s 28us/step - loss: 0.0053 - acc: 0.9991 - val_loss: 2.0269 - val_acc: 0.7813\n",
      "Epoch 119/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.0058 - acc: 0.9991 - val_loss: 2.0335 - val_acc: 0.7777\n",
      "Epoch 120/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.0058 - acc: 0.9992 - val_loss: 2.0366 - val_acc: 0.7763\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = Sequential()\n",
    "model.add(Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(Dense(25, activation='relu'))\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer=adam,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "moredata_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33000/33000 [==============================] - 1s 43us/step\n",
      "4000/4000 [==============================] - 0s 47us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "results_test = model.evaluate(test, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.00540631229686176, 0.9992121212121212]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.0731838654875756, 0.77775]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the same amount of epochs, we were able to get a fairly similar validation accuracy of 89.67 (compared to 88.55 in obtained in the first model in this lab). Our test set accuracy went up from 75.8 to a staggering 80.225% though, without any other regularization technique. You can still consider early stopping, L1, L2 and dropout here. It's clear that having more data has a strong impact on model performance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "* https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/Consumer_complaints.ipynb\n",
    "* https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/\n",
    "* https://catalog.data.gov/dataset/consumer-complaint-database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary  \n",
    "\n",
    "In this lesson, we not only built an initial deep-learning model, we then used a validation set to tune our model using various types of regularization. From here, we'll continue to describe more practice and theory regarding tuning and optimizing deep-learning networks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
